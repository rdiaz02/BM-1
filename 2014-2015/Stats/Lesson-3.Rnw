\documentclass[a4paper,11pt]{article}
<<echo=FALSE,results='hide',error=FALSE>>=
require(knitr, quietly = TRUE)
opts_knit$set(concordance = TRUE)
##opts_knit$set(stop_on_error = 2L)
require(car, quietly = TRUE)
## require(lme4, quietly = TRUE)
@ 

<<packages,echo=FALSE,results='hide',message=FALSE>>=
## zz <- tempfile()
## sink(file = zz)
## require(Rcmdr, quietly = TRUE)
## unlink(zz)
## invisible(capture.output(library(Rcmdr, quietly=TRUE)))
suppressMessages(library(Rcmdr, quietly = TRUE, warn.conflicts = FALSE))
require(doBy, quietly = TRUE)
require(multcomp, quietly = TRUE)
require(abind, quietly = TRUE)
require(e1071, quietly = TRUE)
require(effects, quietly = TRUE)
require(lattice, quietly = TRUE)
## require(RcmdrPlugin.HH, quietly = TRUE)
@ 

\usepackage{amsmath}
\usepackage{ amssymb }
\usepackage[authoryear,round,sort]{natbib}
\usepackage{threeparttable}
\usepackage{array}
\usepackage{hyperref} %% not if using BiocStyle
%%ditto
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=23mm,bmargin=26mm,lmargin=28mm,rmargin=28mm}
\usepackage{url}
\usepackage{xcolor}
%\definecolor{light-gray}{gray}{0.72}
%% \newcommand{\cyan}[1]{{\textcolor {cyan} {#1}}}
%% \newcommand{\blu}[1]{{\textcolor {blue} {#1}}}
%% \newcommand{\Burl}[1]{\blu{\url{#1}}}
\newcommand{\red}[1]{{\textcolor {red} {#1}}}
\newcommand{\Burl}[1]{{\textcolor{blue}{\url{#1}}}}

%\usepackage{tikz}
%\usetikzlibrary{arrows,shapes,positioning}

\usepackage[latin1]{inputenc}
%\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}


\usepackage{gitinfo}

%\usepackage{datetime}
%\newdateformat{mydate}{\THEDAY-\monthname[\THEMONTH]-\THEYEAR}


\title{Lesson 3. ANOVA and linear models}
  
\author{Ramon Diaz-Uriarte\\
Dept. Biochemistry, Universidad Aut\'onoma de Madrid \\ 
Instituto de Investigaciones Biom\'edicas ``Alberto Sols'' (UAM-CSIC)\\
Madrid, Spain\\
{\small \texttt{ramon.diaz@iib.uam.es}} \\
{\small \texttt{rdiaz02@gmail.com}} \\
{\small \Burl{http://ligarto.org/rdiaz}} \\
}


\date{\gitAuthorDate\ {\footnotesize (Rev: \gitAbbrevHash)}}
\begin{document}
\maketitle

\tableofcontents

\clearpage

\section{Introduction}

Linear models and their extensions (which include logistic regression, but
also survival analysis, many classification problems, non-linear models,
dealing with dependent data, etc, etc) are one fundamental topic in
statistics. Here, we will only scratch the surface. But you should come
away from this lesson understanding that these methods are extremely
powerful and flexible and they can be used to address a huge variety of
different research questions. You would spend your time wisely if you at
least took a look at some of the references we provide at the end. 




\section{Comparing more than two groups}

<<create_mit, echo=FALSE, results='hide'>>=
set.seed(789)
n1 <- 11
n2 <- 12
n3 <- 23
m <- c(0, 0, 1.3)
activ <- rnorm(n1 + n2 + n3) + rep(m, c(n1, n2, n3))
activ <- activ - min(activ) + 0.2
mit <- data.frame(activ = round(activ, 3),
                  training = rep(c(1, 2, 3), c(n1, n2, n3)),
                  id = 1:(n1 + n2 +n3))
write.table(mit, file = "MIT.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(list = ls())
@ 


<<>>=
dmit <- read.table("MIT.txt", header = TRUE)
@ 

\subsection{Recoding variables}

Import MIT.txt. These are data about mitochondrial activity related to
three different training regimes. Whoever coded the data, however, used a
number for ``training'', which is misleading, because this is really a
categorical variable. 

The first thing we must do, then, is fix that. Let's recode that, by going
to ``Data'', ``Manage variables ...'', ``Convert numerical variables to
factors''. We will want to label 1 as ``Morning'', 2 as ``Lunch'', and 3
as ``Afternoon'', which are the times at which exercise was conducted and
makes everything much more clear and explicit. Use a new variable (e.g.,
ftraining).

After recoding, you should see this command. Note the ``factor'' function call:
<<>>=
dmit$ftraining <- factor(dmit$training, labels=c('Morning','Lunch','Afternoon'))
@ 


As usual, make sure to look at the data set.

If were to not recode the factor (or use training as it was originally) it
would be a disaster. Look at the output in section \ref{nofactor}.




\subsection{A few plots}
As in Lesson 2, you are advised to also plot the data. For instance, this
will do:

<<>>=
Boxplot(activ~ftraining, data=dmit, id.method="y")
@ 


(the ``2'' is the identifier ---row name in this case--- of a point that
has been flagged as a potential outlier).


\subsection{An ANOVA}

We want to see if time of exercise makes any difference. Conducting three
t-tests is not the best way to go here: our global null hypothesis is
$\mu_{Morning} = \mu_{Lunch} = \mu_{Afternoon}$ and that is what ANOVA
will allow us to test directly.

Find you way around the menu and do a One-way ANOVA (``Statistics'',
``Means''). You'll see this in the R Script window:

<<results='hide'>>=
AnovaModel.1 <- aov(activ ~ ftraining, data=dmit)
summary(AnovaModel.1)
numSummary(dmit$activ , groups=dmit$ftraining, statistics=c("mean", "sd"))
@ 


Can you interpret the output? Look first at the output right after

<<>>=
summary(AnovaModel.1)
@ 

We will cover this in more detail in class, in case you do not remember
ANOVAs. Things to notice:
\begin{itemize}
\item The two rows; one of them is the effect you are interested in (ftraining)
\item The ``Df'' column: those are the degrees of freedom (three groups -
  1 for ``ftraining'').
\item The two columns Sum Sq (Sum of Squares) and Mean Sq (Mean
  Squares). Sum of Squares is a quantity related to the variance. Mean
  Squares is obtained from the ratio of Sum Sq over Df. Then, we use Mean
  Sq to compare how much variance there is between groups related to the
  variance within groups: the F value is the ratio of Mean Sq of ftraining
  over Mean Sq of the residuals. The larger that F value, the more
  evidence there is of groups being different.
\item There is a p-value associated with that F value. In this case it is
  very small.
\end{itemize}


By the way, notice how we created a ``Model'' which, by default, is called
``AnovaModel.1''. But we could have named it differently.


\subsection{So which means are different?}

That small p-value leads us to reject the null hypothesis $\mu_{Morning} =
\mu_{Lunch} = \mu_{Afternoon}$. So there is strong evidence that all three
means are not equal. But which one(s) is(are) different from the other(s)?


If you look at the second piece of output, that from

<<>>=
numSummary(dmit$activ , groups=dmit$ftraining, statistics=c("mean", "sd"))
@ 

you can get an idea: it seems that Morning and Lunch are very similar to
each other, but Afternoon is very different. This agrees with the
impression we got from the boxplot. But we would like a more formal
procedure: we are going to compare all pairs of means and we will take
into account that we are carrying out multiple comparisons.

Before continuing, though, note that tests of pairs of means when the
ANOVA is not significant are rarely justified.




\textbf{Comparing all pairs of means} is done using the ANOVA model, so
the results are not identical to comparing using t-tests (briefly: the
estimate of the variance might be slightly different, and probably
better).

\textbf{Multiple testing corrections} are needed because we are now
conducting three separate tests (in general, if there are $K$ groups and
if you compare all pairs of means you carry out $\frac{K (K-1)}{2}$
tests). Here, we will control the family-wise error rate (the probability
of falsely rejecting one or more tests over the family of tests performed
---three in our case). You will see more about multiple testing in the
final lesson.



To do this, carry out the ANOVA again, but now make sure to click on
``Pairwise comparisons of means''. To be much more explicit, I will also
name the model as ``AnovaMIT'' (entering that in ``Enter name for model'')


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.80\paperwidth,keepaspectratio]{anova-comps.png}
    \caption{\label{pairwise} ANOVA, asking for pairwise comparisons of
      means and naming the model.}
  \end{center}
\end{figure}


A whole bunch of new commands are added to the R Script (I skip the one
that asks for the means and sd of groups but commenting it out; I also add
two comments to the last two commands)

<<fig.width=5,fig.height=5>>=
AnovaMIT <- aov(activ ~ ftraining, data=dmit)
summary(AnovaMIT)

## The next I comment out
## numSummary(dmit$activ , groups=dmit$ftraining, statistics=c("mean", "sd"))
.Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
summary(.Pairs) # pairwise tests
confint(.Pairs) # confidence intervals
cld(.Pairs) # compact letter display
old.oma <- par(oma=c(0,5,0,0))
plot(confint(.Pairs))
par(old.oma) ## restore graphics windows settings
remove(.Pairs) ## remove the object that stored the multiple comp. output
@ 


\subsubsection{The plot of pairwise differences}
Look carefully at the plot: for each difference (for each
\textbf{contrast}), it shows the estimate and a 95\% confidence interval
around it. The plot title says ``95\% family-wise confidence interval'',
and that indicates that multiple testing correction has been used.

Given how far two of the contrasts are from 0.0, it seems those are highly
significant differences. Let's go to the numerical output.





\subsubsection{The numerical output}

This

<<>>=
.Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
summary(.Pairs)
@ 

explicitly shows that we are using Tukey's method and it shows the
p-values of each contrast (each comparison), and it makes it clear that we
are being reported adjusted p values.

There is strong evidence of a difference between Afternoon and the other
two levels, but no evidence of differences between Lunch and
Morning. %% Incidentally, note that the whole process is done using ``Tests
%% for the general linear hypothesis'' and how the contrasts are explicitly
%% shown: this is a very general and powerful procedure (see section
%% \ref{othercontrasts}).



The values that are plotted are generated here:

<<>>=
confint(.Pairs) 
@ 

Note how the usage of Tukey's procedure is again explicit.



\subsubsection{And can I plot the means with s.e from the model?}

Sure. Go to ``Models'', ``Graphs'', ``Effects plots'':

<<>>=
## trellis.device(theme="col.whitebg")
plot(allEffects(AnovaMIT), ask=FALSE)
@ 


That shows the estimates for each group and a 95\% confidence interval
(again, based on the whole ANOVA model). But from that figure it is not
easy to tell which pairs differ, especially taking multiple comparisons
into account.


\subsubsection{This is a mess. What figures do I use?}
That is up to you :-). But this can work: present both the original means
and the plot with the contrasts. You can just copy and paste code
judiciously. Note that I modify slightly the title of the first figure, so
that the usage of Tukey contrasts is explicit and I modify the title of
the second, so we use ``Training'' in the name, not ``ftraining'':


<<out.width='7cm', out.height='7cm', fig.show='hold'>>=
.Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
tmp <- cld(.Pairs) ## silent assignment 
old.oma <- par(oma=c(0,5,0,0))
plot(confint(.Pairs), 
     main = "95% family-wise confidence interval using Tukey contrasts")
par(old.oma) 
plot(allEffects(AnovaMIT), ask=FALSE, main = "Training: effect plot")
@ 



%% An alternative is to use the MMC plots provided by package ``RcmdrPlugin.HH'':

%% <<>>=
%% @ 





\subsubsection{Side note: Interpreting confidence intervals}
If this is not obvious to you, ask it in class: a figure that shows an
estimate (e.g., a mean) and a 95\% confidence interval, where the interval
goes from, say, 1 to 2, \textbf{should not} be interpreted as saying that
there is a 95\% probability that the mean is between 1 and 2. That is not
the correct interpretation of a confidence interval.

Make sure you understand this!!!









\subsubsection{Multiple comparisons, other contrasts,
  etc} \label{othercontrasts} There is a wide literature on methods for
adjusting for multiple comparisons in ANOVA and linear models. And
sometimes a distinction is made between pre-planned and post-hoc
comparisons. Tukey's approach is a widely accepted one (though there are
others) and the distinction between pre-planned and post-hoc does not
arise when researchers directly want to do all possible pairs right from
the beginning. However, many of these issues can become important if you
know, from the start, that some comparisons do not matter to you, and/or
there are many groups. As well, we could be interested in other types of
contrasts, for instance, that the mean of groups 2 and 3 is different from
the mean of group 4. Etc, etc. We will not get into this. 




\subsection{One way ANOVA: summary of steps}

\begin{enumerate}
\item Enter the data.
\item Recode the factor (the dependent variable), if needed.
\item Run the model.
\item Assess model diagnostics.
\item Carry out comparisons between pairs of means with appropriate
  adjustment for multiple comparisons.
\end{enumerate}




\section{Two-way ANOVA}

The following data come from an experiment about the effects of three
diets and two cholesterol-controlling drugs in the reduction of
cholesterol levels. As usual, read the data and look at them. Since the
author used names for the two factors, Diet and Drug, we do not need to
transform them. Call the data ``dcholest''.

<<create_tooth, echo=FALSE, results='hide'>>=

set.seed(45)
n <- c(4, 7, 9, 5, 7, 8)
m <- c(2.1, 1, 2, -.2, 4, 5)
y <- round(unlist(mapply(function(x, y) rnorm(x, y), n, m)), 3)
Drug <- c(rep("A", sum(n[1:3])),
          rep("B", sum(n[4:6])))
Diet <- rep(rep(c("HF", "M1", "M2"), 2), n)
chol <- data.frame(y = y,
                   Drug = Drug,
                   Diet = Diet)
rm(y, n, m, Drug, Diet)
## lm1 <- lm(y ~ Drug * Diet, data = chol)
## lm0 <- lm(y ~ Drug + Diet, data = chol)
## anova(lm1)
## Anova(lm1)
## anova(lm0)
## anova(lm(y ~ Diet + Drug, data = chol))
## Anova(lm0)
write.table(chol, file = "Cholesterol.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(chol)
@ 

<<>>=
dcholest <- read.table("Cholesterol.txt", header = TRUE)
@ 


\subsection{Fitting a two-way ANOVA}

As usual, go to ``Statistics'', ``Means'', and then ``Multiway
ANOVA''. Call it ``cholanova''

Look at the output, which I comment below
<<>>=
## This fits the model. Pay attention to the "*"
cholestanova <- (lm(y ~ Diet*Drug, data=dcholest))
## This shows the ANOVA table. Notice the "Type II"
Anova(cholestanova)

## Now we are show the 3 by 2 table of means, standard deviations, and number 
## of observations
tapply(dcholest$y, list(Diet=dcholest$Diet, Drug=dcholest$Drug), 
       mean, na.rm=TRUE) # means
tapply(dcholest$y, list(Diet=dcholest$Diet, Drug=dcholest$Drug), 
       sd, na.rm=TRUE) # std. deviations
tapply(dcholest$y, list(Diet=dcholest$Diet, Drug=dcholest$Drug), 
       function(x) sum(!is.na(x))) # counts
@ 


\subsection{Interactions}

Now, go to the ``Models'' menu and do a ``Effects plot''

<<>>=
trellis.device(theme="col.whitebg")
plot(allEffects(cholestanova), ask=FALSE)
@ 


What do you see? Do you understand what an interaction is? Do you see it
in the plot? Basically, an interaction means that the effect of one
variable depends on the effect of the other. In this case, so even if Drug
B leads to a larger change in cholesterol, its effects depend on the Diet:
it has that effect on Diets M1 and M2, but it does not on the high fat
diet (HF). This has practical consequences: is Drug B a better drug? It
depends on the diet of the patient. 

Interaction is also called ``non-additivity'' because the model deviates
from a simple model like
\[ y = Drug + Diet\]

as the effect of Drug depends on the value of Diet (or the other way
around). The phenomenon of interaction should be familiar to you: it is
very common in life in general, and in biology you might have previously
seen at as epistasis in genetics.



You can also see interaction plots using this directly from R:

<<>>=
with(dcholest, interaction.plot(Drug, Diet, y, type = "b"))
@ 

or, much nicer, using this from package ``HH'' (I will first load it)

<<>>=
library(HH)
interaction2wt(y ~ Diet*Drug, data=dcholest)
@ 

Notice how this figure displays both main effects and interactions. So
even if the main effect of Drug B is to lead to a larger change in
cholesterol (as you can see in the upper right panel), Drug B actually
leads to much smaller change in cholesterol if given to patients with diet
HF (as seen in the bottom right panel). In fact, under Drug A, it seems
that diet HF is actually slightly better than diet M1 (as seen in the
bottom right panel or in the effects plot).


Given these results, it makes no sense to report any global main effects
and, in fact, we would rarely be interested in interpreting the
significance (or not) of the Diet or Drug term, by themselves, in the
presence of interactions.



\subsection{The order of factors}

Let's pretend there are no interactions. We can do that by creating a data
set without the ``HF'' subjects. Go to ``Active data set'', ``Subset
active data set''. 



\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.60\paperwidth,keepaspectratio]{subset-chol.png}
    \caption{\label{subset} Subset a data set. Note the ``Subset expression'.}
  \end{center}
\end{figure}


<<>>=
dcholest2 <- subset(dcholest, subset=Diet != "HF")
@ 

Now do a two-way ANOVA:

<<>>=
cholest2anova <- (lm(y ~ Diet*Drug, data=dcholest2))
Anova(cholest2anova)
@ 

So no evidence whatsoever of interactions. We can go and refit the model
without the interaction, and that we do in ``Statistics'', ``Fit models'',
``Linear model''. We will actually fit two models, which differ only by
the order in which we give Diet and Drug in the formula (we showed this
already in Lesson 2) and we will call them lm1 and lm2

<<>>=
lm1 <- lm(y ~ Diet + Drug, data=dcholest2)
lm2 <- lm(y ~ Drug + Diet, data=dcholest2)
@ 


Now, go to ``Models'', ``Hypothesis tests'' and do ``ANOVA table (Type I
sums of squares)'' (if you do all of this via the GUI and mouse, remember
to change the model clicking on ``Models'', ``Select active model'' or via
the box that says ``Model:''):

<<>>=
anova(lm1)
anova(lm2)
@ 

As you can see, the F statistic and the p-value are different!!! What
gives here?

Now, use Type II sums of squares:

<<>>=
Anova(lm1)
Anova(lm2)
@ 

Nothing changes between those two. But if you look carefully, the F value
(and p-value) of the Type II Sums of Squares ANOVA table are the same as
those for the term that enters last in the Type I (those produced via
``anova'', without a capital ``A'').


This sounds crazy, irrelevant, and a huge wast of time. But it ain't. This
is an \textbf{extremely common phenomenon} when data are not perfectly
balanced (with categorical data) or there are correlations (with
continuous covariates, as in regression). What is happening? 

\begin{itemize}
\item Type II sums of squares (as well as t-statistics from a linear
  model) always show what that term contributes, \textbf{given all the
    rest} are already in the model. In other words, given all the rest
  have already been taken into account.
  
\item Type I (or sequential) sums of squares do not. They are sequential.
\end{itemize}

Biologically, this makes a lot of sense. Think about it. And this, of
course, affects models with two, three, \ldots factors.


\subsection{ANOVA/linear models with more than two factors}
We will present no examples, but life is filled with them. Think about
cholesterol: to the experiment with drug and diet add a third factor:
an exercise program. Or maybe a fourth factor too: a stress reduction
program. Or maybe \ldots.



\subsection{Multiple comparisons of means}
Can they be done? Yes. You can use the ``glht'' or the TukeyHDS functions
directly. Or you fit the model using ``aov'' (not ``lm''), and then use
the ``RcmdrPlugin.HH'', and ask for the MMC plot. We will not pursue this
any further here (among other questions you should ask yourself, at what
levels of one variable will you be comparing the other? How does lack of
balance affect the contrasts? Do you really want
all possible contrasts?) A good place to start reading on these issues is
chapter 14 (and section 5.3.2) of Everitt and Hothorn's ``A handbook of
statistical analysis using R, 2nd ed''.

%% For instance, this would work. But this is not necessarily what you might
%% really want:

%% <<>>=
%% cholaov <- aov(y ~ Diet * Drug, data = dchol)
%% chol_tukey <- TukeyHSD()
%% @ 



\section{Simple linear regression}

This is another form of a linear model. But now, the independent variable
is continuous. So we will fit a line:

\[Y = \alpha + \beta X + \epsilon\]
where $Y$ is, as usual, the dependent variable, $X$ the independent,
$\beta$ is the slope and $\alpha$ the intercept (this is just the equation
for a line). The simple linear regression procedure will estimate $\alpha$
and $\beta$, finding values that produce a \textbf{best fitting line}
(note: it is a line, not an arbitrary curve).

Let's use a data set from the base package, ``women''. Go to ``Data'',
``Data in packages'' and select ``women'' (it is in package ``datasets'')


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.80\paperwidth,keepaspectratio]{women-data.png}
    \caption{\label{women} Selecting the ``women'' dataset.}
  \end{center}
\end{figure}

Get some help about the active data (look for it in the menu). Now go to
``Statistics'', ``Fit models'', ``Linear regression''. Think about the
model we want. Most likely, we want to model weight as a function of
height. Let's do it (call the model ``r1'')

<<>>=
r1 <- lm(weight~height, data=women)
summary(r1)
@ 


The row of the output that says ``(Intercept)'' gives you the estimate of
the intercept. The t-statistic (under ``t value'') is testing that the
intercept is zero. And it is not. But tests about the intercept are rarely
interesting (except for cases with a natural 0). The second line is more
interesting: that is the slope, how much weight increases per unit
increase in height (to interpret this we need to know the units!). And the
t-statistic tests if the slope is 0, so it is testing whether weight
changes with height. It certainly does!


\subsection{And how does it look like}
Eh!!! We should probably have plotted the data as the first thing. A
couple of plots will be good here. First, in ``Graphs'', ``Scatterplot''

Work you way around the menu until you find the ``Confidence interval
plot'' (you might want to unclick the ``Show spread''):

<<>>=
scatterplot(weight~height, reg.line=lm, smooth=TRUE, 
            spread=FALSE, id.method='mahal', id.n = 2, boxplots='xy', 
            span=0.5, data=women)
@ 

(and this figure suggests that the relation is not really linear, which
should not be surprising in this case).

The second plot we can get from ``Models'', ``Confidence interval plot'',
which will show something like

<<>>=
ci.plot(r1)
@ 

(and this shows confidence and prediction intervals for the linear model).



\section{Diagnostics}

Wait!!! Does our model make sense? This is a model, so we can, and should,
check some of its basic assumptions.




Do not recode factor


Do not use the overlap in CI as a p-value


Doing t-tests via ANOVA (which adds diagnostics)



\section{Linear regression}

\section{Experimental design matters}

Split-plot

Mixed effects and dependencies



\section{Appendix}
\subsection{What if we did not recode training?}\label{nofactor}
Suppose we had not recoded training but we had fitted a linear model. This
would have happened:

<<>>=
lmMITnofactor <- lm(activ ~ training, data=dmit)
summary(lmMITnofactor)
Anova(lmMITnofactor, type="II")
@ 

See how the degrees of freedom make no sense.




\section{Additional reading}

Many books have been devoted to linear models, ANOVA, et al. And in R (not
necessarily through R Commander) there is a wide variety of procedures
implemented. To begin with, look at the great book by Fox and Weisberg
``An R companion to applied regression''. Take also a look at chapters 6
and 7 of Dalgaard's ``Introductory statistics with R'' and chapters 8 and
9 of Kabakoff's ``R in action''. This should get you going.



\end{document}

%% remember to use bibexport to keep just the minimal bib needed
%% bibexport -o extracted.bib OncoSimulR.aux
%% rm OncoSimulR.bib
%% mv extracted.bib OncoSimulR.bib
%% and then turn URL of packages into notes


%%% Local Variables:
%%% TeX-master: t
%%% ispell-local-dictionary: "en_US"
%%% coding: iso-8859-15
%%% End:



