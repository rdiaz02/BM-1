%% ;;; -*- mode: Rnw; -*-

%% RDU
%% Material from Luis del Peso Ovalle
%% Original file was .lyx. Exported as Rnw.
%% - .png figures: not having original pngs, I screen capture from the PDF
%%                 and remove original path
%% - csv files: remove path, leave them in this same directory
%% Then, major reformating of structure, and major modifications of content.

\synctex=1
\documentclass[a4paper,11pt]{article}

\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=28mm,bmargin=28mm,lmargin=30mm,rmargin=30mm}


\usepackage[para]{threeparttable}
\def\TPTtagStyle{\textit} % table footnote markers in italisc
\usepackage{rotating} %% sidewaystable
\usepackage{setspace} %% setstretch
\usepackage{graphics}
\usepackage{amssymb,amsfonts,amsmath,amsbsy}

\usepackage{enumitem}

\usepackage{url}
\usepackage{nameref}
\usepackage{array}
\usepackage{graphicx}
\usepackage{epstopdf}

% \usepackage[numbers,sort,compress]{natbib}
\usepackage{natbib}


% separate consecutive citet entries with comma
\makeatletter
\newcommand\bibstyle@comma{\bibpunct(),a,,}
\newcommand\bibstyle@semicolon{\bibpunct();a,,}
\makeatother
\usepackage{etoolbox}
\pretocmd\citet{\citestyle{comma}}\relax\relax
\pretocmd\Citet{\citestyle{comma}}\relax\relax
\pretocmd\citep{\citestyle{semicolon}}\relax\relax
\pretocmd\Citep{\citestyle{semicolon}}\relax\relax
\pretocmd\citealp{\citestyle{semicolon}}\relax\relax
\pretocmd\Citealp{\citestyle{semicolon}}\relax\relax

\usepackage{nameref}
\usepackage{hyperref}
\hypersetup{
  colorlinks = true,
  citecolor=  black,
  linkcolor = blue,
  %% urlcolor = magenta,
  filecolor = cyan %% controls color of external ref, if used
}
\usepackage{xurl} %% url does not break all URLs in margins

\newcommand*{\qpref}[1]{\hyperref[{#1}]{\textit{``\nameref*{#1}'', section \ref*{#1}}}}
\newcommand*{\qref}[1]{\hyperref[{#1}]{\textit{``\nameref*{#1}'' (section \ref*{#1})}}}

% \usepackage{doi}
% %% Next works with doi package, but the first part is colored differently
%% \renewcommand{\doitext}{DOI: https://doi.org/}
%% So use this instead, modified from
%% https://tex.stackexchange.com/a/482395
\newcommand*{\doi}{}
\makeatletter
\newcommand{\doi@}[1]{\href{https://doi.org/#1}{\textcolor{black}{DOI: } https://doi.org/#1}}
\DeclareRobustCommand{\doi}{\hyper@normalise\doi@}
\makeatother

\usepackage[dvipsnames,table]{xcolor}
%\usepackage{color}
\newcommand{\cyan}[1]{{\textcolor {cyan} {#1}}}
\newcommand{\blu}[1]{{\textcolor {blue} {#1}}}
\newcommand{\Burl}[1]{\blu{\url{#1}}}
\newcommand{\red}[1]{{\textcolor {red} {#1}}}
\newcommand{\green}[1]{{\textcolor {green} {#1}}}
\newcommand{\mg}[1]{{\textcolor {magenta} {#1}}}
\newcommand{\og}[1]{{\textcolor {PineGreen} {#1}}}
\newcommand{\myverb}[1]{{\footnotesize\texttt {\textbf{#1}}}}

\newcommand*{\FIXME}[1]{\textcolor {red} {FIXME: #1}}



\usepackage{pdflscape} %% Landscape
\usepackage{authblk} % author affiliations
\usepackage[iso,english]{isodate}

\usepackage[english]{babel}

%%%%%%%% fonts and special chars
\usepackage[utf8]{inputenc} % allows usage of spanish special characters
% %% txfonts for Times or similar
% %% txfonts breaks amsmath.
% %% http://www.texfaq.org/FAQ-alreadydef
% Simplest, using newest newtxt
\usepackage[]{newtx}


%% Maybe useful for teaching material
\newcommand{\Rnl}{\ +\qquad\ }
\newcommand{\Emph}[1]{\emph{\mg{#1}}}
\usepackage[begintext=\textquotedblleft,endtext=\textquotedblright]{quoting}
\newcommand{\activities}{{\vspace*{10pt}\LARGE \textcolor {red} {Activities:\ }}}
\newcommand{\R}{R}
\newcommand{\flspecific}[1]{{\textit{#1}}}


%% Copyright stuff
\usepackage[copyright]{ccicons}

%% Sow revision
\usepackage{gitinfo}

%% Be generous with interpar. spacing!
\setlength{\parskip}{0.35em}

%% remove? There is a multirow below.
\usepackage{multirow}

%% with nobreak below, try to prevent breaks in R output
\usepackage{fancyvrb}

<<setup,include=FALSE,cache=FALSE>>=
require(knitr)
opts_knit$set(concordance = TRUE)
opts_knit$set(stop_on_error = 2L)
opts_chunk$set(echo = TRUE, nobreak = TRUE)

## https://stackoverflow.com/a/75546561
## %% FIXME
## %% Not quite. Will debug some other time

## default_output_hook <- knit_hooks$get('output')
## knit_hooks$set(
##   output = function(x, options) {
##     if(is.null(options$nobreak)) {
##       default_output_hook(x, options)
##     } else {
##       paste0("\\begin{Verbatim}[samepage=true]\n", x, "\n\\end{Verbatim}"
##              )
##     }
##   }
## )

library(Rcmdr)
@


\begin{document}
\title{BM-2, Lesson 4: Basic analysis of categorical data}

\author{Luis del Peso Ovalle (until 2020), Ramon Diaz-Uriarte (starting 2024)\\
  Dept. Biochemistry, Universidad Aut\'onoma de Madrid \\
  Instituto de Investigaciones Biom\'edicas Sols-Morreale, IIBM, (CSIC-UAM)\\
  Madrid \\
  Spain{\footnote{r.diaz@uam.es, \Burl{https://ligarto.org/rdiaz}  }}
}

\date{\gitAuthorDate\ {\footnotesize (Rev: \gitAbbrevHash)}}

\maketitle

\tableofcontents

\clearpage


\section*{License and copyright}\label{license}
This work is Copyright, \copyright, 2020, Luis del Peso Ovalle, \copyright, 2024, Ramon Diaz-Uriarte, and is licensed under a \textbf{Creative Commons } Attribution-ShareAlike 4.0 International License: \Burl{http://creativecommons.org/licenses/by-sa/4.0/}.

\centerline \ccbysa



All the original files for the document are available (again, under a Creative Commons license) from \Burl{https://github.com/rdiaz02/BM-1}. (Note that in the github repo you will not see the PDF, or R files, nor many of the data files, since those are derived from the Rnw file). This file is called \texttt{Lesson-4.Rnw}.
\vspace*{10pt}

Please, \textbf{respect the copyright and license}. This material is provided freely. If you use it, we only ask that you use it according to the (very permissive) terms of the license: acknowledging the author and redistributing copies and derivatives under the same license. If you have any doubts, ask me.



\section*{Document history}
This document was originally prepared, in 2020, by Luis del Peso Ovalle. Starting September 2024, it was heavily modified by Ramon Diaz-Uriarte.

\clearpage



% \documentclass[twoside,english]{extarticle}
% \synctex=1
% \usepackage{lmodern}
% \usepackage{helvet}
% \renewcommand{\ttdefault}{cmtt}
% \renewcommand{\familydefault}{\rmdefault}
% \usepackage[T1]{fontenc}
% \usepackage{textcomp}
% % \usepackage[latin9]{inputenc}
% \usepackage[a4paper]{geometry}
% \geometry{verbose,tmargin=1.5cm,bmargin=1.5cm,lmargin=2cm,rmargin=2cm,headheight=1.5cm,headsep=0.5cm,footskip=0.5cm}
% \usepackage{fancyhdr}
% \pagestyle{fancy}
% \usepackage{babel}
% \usepackage{array}
% \usepackage{verbatim}
% \usepackage{cprotect}
% \usepackage{url}
% \usepackage{enumitem}
% \usepackage{multirow}
% \usepackage{amsmath}
% \usepackage{amsthm}
% \usepackage{graphicx}
% \usepackage{natbib}
% \usepackage[pdfusetitle,
% bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
% breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
% {hyperref}

% \makeatletter

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
% %% Special footnote code from the package 'stblftnt.sty'
% %% Author: Robin Fairbairns -- Last revised Dec 13 1996
% \let\SF@@footnote\footnote
% \def\footnote{\ifx\protect\@typeset@protect
%   \expandafter\SF@@footnote
% \else
%   \expandafter\SF@gobble@opt
% \fi
% }
%   \expandafter\def\csname SF@gobble@opt \endcsname{\@ifnextchar[%]
%   \SF@gobble@twobracket
%   \@gobble
% }
%   \edef\SF@gobble@opt{\noexpand\protect
%   \expandafter\noexpand\csname SF@gobble@opt \endcsname}
%   \def\SF@gobble@twobracket[#1]#2{}
% %%   Because html converters don't know tabularnewline
%   \providecommand{\tabularnewline}{\\}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   Textclass specific LaTeX commands.
%   \numberwithin{equation}{section}
%   \numberwithin{figure}{section}
%   \newcommand{\lyxaddress}[1]{
%   \par {\raggedright #1
%   \vspace{1.4em}
%   \noindent\par}
% }
%   \newlist{casenv}{enumerate}{4}
%   \setlist[casenv]{leftmargin=*,align=left,widest={iiii}}
%   \setlist[casenv,1]{label={{\itshape\ \casename} \arabic*.},ref=\arabic*}
%   \setlist[casenv,2]{label={{\itshape\ \casename} \roman*.},ref=\roman*}
%   \setlist[casenv,3]{label={{\itshape\ \casename\ \alph*.}},ref=\alph*}
%   \setlist[casenv,4]{label={{\itshape\ \casename} \arabic*.},ref=\arabic*}
%   \theoremstyle{plain}
%   \newtheorem{thm}{\protect\theoremname}
%   \theoremstyle{definition}
%   \newtheorem{xca}[thm]{\protect\exercisename}

%   \makeatother

%   \providecommand{\casename}{Case}
%   \providecommand{\exercisename}{Exercise}
%   \providecommand{\theoremname}{Theorem}

%   \begin{document}
%   \title{BM1, Applied Statistics, Lesson 4: Basic analysis of categorical data
%   and contingency tables}
%   \author{Luis del Peso Ovalle}
%   \date{2020-02-18}
%   \maketitle

%   \lyxaddress{Dept. Biochemistry, Universidad Autonoma de Madrid}

%   \lyxaddress{Instituto de Investigaciones Biomedicas \textquotedblleft Alberto
%   Sols\textquotedblright{} (UAM-CSIC)}

%   \lyxaddress{Madrid, Spain}


%   \lyxaddress{Modifications by Ramon Diaz-Uriarte, 2024-10.}



\section{Basic analysis of categorical data and contingency tables}

In this chapter we will learn about:
\begin{itemize}
\item Making inferences about one proportion and comparing two proportions, including obtaining confidence intervals and conducting hypothesis tests
\item The difference between testing goodness of fit and testing independence
\item Contingency tables
\item Statistical test for categorical variables (Chi square test and Fisher's exact test)
\item Graphical visualization of categorical data
\end{itemize}

We will also learn how to generate contingency tables and perform relevant test in R Commander.
% \begin{quote}
% Disclaimer: This document is not, nor does the author pretend it to
% be, a substitute for the reference books cited in the syllabus. It
% is intended as a mere guide for the course contents. Moreover, it
% may (and surely does) contain errors.
% \end{quote}


\section{Study Cases}

Study the following  cases. In which way are they different to the other cases we have analyzed so far?

\begin{itemize}

\item This uses a data set described in \citet[section 8.1 of][]{Dalgaard2008}: 39 of 215 randomly chosen patients are observed to have asthma. We want to test the null hypothesis that the probability of asthma is 0.15.




\item To explore the physiological functions of endothelin-2 (EDN2), a research group generated gene-targeted mouse models. The offspring of the matting between Et2+/- animals resulted in 40 Et2 +/+ , 72 Et2 +/\textendash{} , and 42 Et2 \textendash /\textendash{} mice\footnote{Data from J Clin Invest. 2013;123(6):2643\textendash 2653.}. Does the inactivation of this gene affect viability? In other words, are these offspring numbers significantly different from the expected Mendelian ratio 1:2:1?

\item Gene expression profiling experiments demonstrate that exposure of
cells to hypoxia results in changes in the expression of several hundred
genes (table \ref{tab:Frequency-of-HRE}). The Hypoxia Inducible Factor
(HIF) is a transcription factor that binds the motif RCGTG (Hypoxia
Response Element, HRE) and plays a key role in the transcriptional
effects of hypoxia. Table \ref{tab:Frequency-of-HRE}, shows the number
of hypoxia-responsive genes that have HRE motifs in their promoter
regions. Does HIF/HRE show any preference for up- or downregulation?
\begin{table}[h]
\caption{\protect\label{tab:Frequency-of-HRE}Frequency of HRE in the promoters
of hypoxia-responsive genes}

\centering{}%
\begin{tabular}{|c|c|c|}
\hline
 & HRE+ & HRE-\tabularnewline
\hline
\hline
Upregulated & 145 & 113\tabularnewline
\hline
Non regulated & 3766 & 7459\tabularnewline
\hline
Downregulated & 69 & 121\tabularnewline
\hline
\end{tabular}
\end{table}
\end{itemize}

\section{Files we will use}
\begin{itemize}
\item mice\_genotypes.csv
\item Hyp\_effect.csv
\item RefSeq\_All\_Chr.csv
\item Fiber.csv.
  \item We will also use a few additional datasets that we will create on the fly (ashtma, peptic ulcer).
\end{itemize}

\section{Categorical data}

The cases described above have in common that they refer to categorical
attributes (variables) in a nominal scale. That is, they both deal
with categorical\protect\footnote{recall that a categorical variable, also referred as nominal, is one
that simply allows you to assign observations to each of the levels
(categories) of the variable} data.

In the mice example concerning viability, we are recording the genotypes of the offspring
and thus we can only assign each individual to one of the categories
(+/+, +/- and -/-). Unlike other type of variables we can not calculate
mean, median or rank of the genotype. In fact, the only thing we can
do is count the frequencies of cases (mice in this case) in each of
the categories of the variable. In other words, we tally up categorical
variables to generate the frequency of cases in each of the possible
levels of the variable. In case \#1 above, we would register the number
of +/+, +/- and -/- mice. Thus, in these cases we would model the
data on the frequency of cases in each possible level\protect\footnote{For this reason, oftentimes the analysis of categorical data is referred
  to as frequency analysis}.

In the gene expression example, we are measuring two nominal variables
from a set of genes: effect of hypoxia on their transcripcion (levels:
upregulation, no effect, downregulation\footnote{A minor point, though: it could be argued that ``upregulation'', ``no effect'', and ``downregulation'' are really three ordered levels of transcription. In other words, even if they are not numerical values, they are ordered; this would be an example of an ordinal categorical variable. We will not pursue this here (though we mention this again at the end).}) and the presence of HRE in
their promoters (levels: present/+ or absent/-). Table \ref{tab:Frequency-of-HRE},
reports the frequency of genes in each of the possible categories.


In summary, categorical data represent the distribution of samples
into several mutually exclusive categories (also called levels) and
this usually involves counting how many cases are in each qualitative category/level. Sometimes, though, we have categorical data that are even simpler, as in the next section.


\section{A single proportion: confidence intervals and hypothesis tests}\label{sec:one-prop-conf}

We will use the data set described in \citet[section 8.1 of][]{Dalgaard2008}: 39 of 215 randomly chosen patients are observed to have asthma. We want to test the null hypothesis that the probability of asthma is 0.15.

We can do the test typing in R very easily, but to make it available for R Commander, we create a data set as follows (this is just an object that has ``YES'' repeated 39 types and ``NO'' repeated 215 - 39 times).

<<asthma-data>>=
ashtma <- data.frame(X = c(rep("A", 39), rep("NO", 215 - 39)))
@


Type the above and select the data in R Commander. Now, find the ``Statistics \(\rightarrow\) Proportions'' and look at the options. Carefully look at what it says about ``Proportion'' (if we had entered the data differently, it might have been comparing the proportion of no asthma with 0.15, and that is not what we want).

\subsection{Normal approximation}
\label{sec:normal-approximation}

Just use the menu, and select the normal approximation.

Doing it directly by hand is probably much faster than entering the data and going to the menu:

<<>>=
prop.test(x = 39, n = 215, p = 0.15, correct = FALSE)
@

We are given a p-value and a confidence interval. This relies on a normal approximation to the binomial. This approximation is justified if the expected number of successes (in our example, cases with asthma) and failures are both larger than 10. This is the case for us (\(0.15\ *\ 215 = 32.25 \) and \(0.85\ *\ 215 = 182.75\)).

We used the argument \texttt{correct = FALSE} to prevent R from using a continuity correction. The reasons can be found, for example, in \citet[section 1.6.2, p.~18]{agresti2018}, \citet[note 1.5, p.~25]{agresti2012}, or \citet[section 1.2.3, p.~35]{bilder2024}. In what follows, we will always  avoid the continuity correction. By the way, that is what R Commander does by default.


You should be able to interpret the p-value and the confidence interval.

\subsection{Binomial test}
\label{sec:exact-binomial-test}

We can also do a test that obtains the exact probability from the binomial distribution. It calculates the probabilities for all the possible values of X that are as extreme as, or more extreme than, the observed 39, and adds up all those values (as you can see, this is just straight from the definition of the p-value).

You can obtain it from the menu or directly as:

<<>>=
binom.test(x = 39, n = 215, p = 0.15)
@

Again, you should be able to interpret the p-value and the confidence interval. And the conclusions are the same as from above.

Do not be mislead by the ``exact'': the confidence interval is, actually, not well defined. (We will also see that the ``exact'' adjective can be misleading with Fisher's exact test, but for other reasons).

% This is Dalgaard's 8.5 exercise
% <<>>=
% pvt <- function(p) binom.test(x = 3, n = 15, p = p)$p.value
% ps <- seq(0, 1, by = 0.001)
% pvs <- sapply(ps, pvt)
% plot(pvs ~ ps, xlab = "Null p", ylab = "p-value")
% @



\section{Two proportions: confidence intervals and hypothesis tests}
\label{sec:two-prop-conf}


We will now use the data in \citealp[Exercise 8.3, p.~154 of][]{Dalgaard2008}, originally from p.~72 of Campbell and Machin, 1993, \textit{Medical Statistics. A Commonsense Approach, 2nd ed.}, John Wiley \& Sons, Chichester. It examines the effects of two drugs on peptic ulcers.

There are several ways to enter these data and get the confidence interval and tests output. We could create a data frame, for example this way (it is OK if you do not understand how I am creating the data):


<<>>=
ex3 <- expand.grid(Drug = c("Piren", "Trithi"),
                   Outcome = c("Healed", "NotHealed"))
ex3$cases <- c(23, 18, 7, 13)
Exercise3 <- ex3[rep(1:4, ex3$cases), c("Drug", "Outcome")]

@

Now, select ``Exercise3'' as the active data, and do a two-sample proportions test using the normal approximation using the menus.

<<>>=
local({  .Table <- xtabs(~Drug+Outcome, data=Exercise3)
  cat("\nPercentage table:\n")
  print(rowPercents(.Table))
  prop.test(.Table, alternative='two.sided', conf.level=.95, correct=FALSE)
})

@



We can also quickly do it by entering the successes and total cases (i.e., healed + non healed) in the call to the function \texttt{prop.test}:

<<>>=
prop.test(x = c(23, 18), n = c(30, 31), correct = FALSE)
@

Again, you should be able to interpret all of the output.


For these quick tests, instead of typing the command, you can directly enter the data via R Commander; see below (section \ref{sec:enter_tabular}).



There is an additional way to compare those proportions: turning the problem into one of asking whether there is  independence of Outcome from Drug, or of independence of rows and columns. We leave it here, and will return to it later (section \ref{sec:contingency-tables}):

<<>>=

tableEx3 <- xtabs( ~ Drug + Outcome, data = Exercise3)
chisq.test(tableEx3, correct = FALSE)

## same as
chisq.test(matrix(c(23, 18, 7, 13), nrow = 2), correct = FALSE)
@



\section{Goodness of fit: Comparing observed and expected frequencies}\label{sec:goodness-fit}

Let us return to the mouse example:  To explore the physiological functions of endothelin-2 (EDN2), a research group generated gene-targeted mouse models. The offspring of the matting between Et2+/- animals resulted in 40 Et2 +/+ , 72 Et2 +/\textendash{} , and 42 Et2 \textendash /\textendash{} mice\footnote{Data from J Clin Invest. 2013;123(6):2643\textendash 2653.}.  The question was: does the inactivation of this gene affect viability?

If the gene has no effect on
embryonic viability, we would expect the typical Mendelian ratio of
genotypes. So, the question can be rephrased as: are the observed
genotype frequencies significantly different from the expected Mendelian
ratio 1:2:1 (0.25:0.5:0.25)? The observed ratios are (40/154):(72/154):(42/152)
or 0.26:0.47:0.27, which are quite similar to the expected ratios
but certainly not the same. However, we should not be surprised as
we would expect some variation due to chance.

To answer our original question, we will compare observed and expected frequencies.  This is exactly what the Pearson's% \protect\footnote{there are other chi-squared test for categorical data, but this is
% the more popular by far.}\protect
 chi-squared test\footnote{Do not mistake it for the Pearson correlation coefficient, which indicates
the extent to which two numerical variables are linearly related.}, also known as the chi-squared goodness-of-fit test,
does.



\subsection{Chi-squared statistic}\label{sec:chi1}

So, as usual, we start with the null hypothesis, \(H_0\) that there is no effect
of gene deletion on viability and thus the frequencies of genotypes
are Mendelian so that any departure from the expected is due to random variation.

The test statistic we will use is chi-square($\chi^{2}$), calculated
according to the formula: $\chi^{2}=\sum_{category}\frac{\left(observed_{i}-expected_{i}\right)^{2}}{expected_{i}}$.

Note that, the closer the observed and expected frequencies, the smaller
the value of the chi-square statistic. Thus, increasing values of
this statistic would support the alternative hypothesis (the observed
ratios do not fit the expected values)\protect\footnote{Residuals are the difference between the observed and expected values.
The magnitude of each residual indicates how much each of the observed
frequencies differs from what is expected. The residuals are typically
standardized (by dividing by the square of the expected frequencies)
to enable individual residuals to be compared relative to one another.
Large values of residuals indicate large deviations from what is expected
and thus also indicate large contribution to the overall association.
On addition, the sign (+ or \textminus ) of the residual indicates
whether the frequencies were higher or lower than expected, but note we square the deviations. Pearson's
residuals are calculated as $\frac{observed-expected}{\sqrt{expected}}$}.


% \begin{comment}
% Explain how residuals indicate the direction of change.
% \end{comment}


\subsection{Pearson's Chi-square test}

Now, all we need to do is assess how likely it is, under \(H_0\), that we find a value of the statistic as large as, or larger than, the value we found.

We do this by using the chi-square($\chi^{2}$) distribution (see figure \ref{fig:Chi-squared-probability-distribu}). Note that, in contrast to what we did with the two-tailed t-test, we only look at the right tail of the distribution. Why do you think this is the case? (This is also what we did with the F-statistic, by the way).

% Once we have an statistic that reflects the agreement between the
% two distributions, we just need to set a threshold of how large this
% value can be before we reject the null hypothesis. Fortunately, the
% distribution of chi-square($\chi^{2}$) values under the null hypothesis
% is known (see figure \ref{fig:Chi-squared-probability-distribu})
% and, from this distribution, we can calculate how likely a chi-square($\chi^{2}$)
% value like the one we got is compatible with the null hypothesis (how
% likely is a chi-squared with that value is observed under the null
% hypothesis).

\begin{figure}[h]
\caption{\protect\label{fig:Chi-squared-probability-distribu}Chi-squared probability
distribution}

<<chi_square_prob_distr,echo=F,fig.width=4, fig.height=4, fig.show='hold'>>=
x<-seq(from=0,to=50,by=0.1)
color=c("black","red","blue","green","magenta")
DF=c(1,2,3,5,10)
plot(x,dchisq(x,df=DF[1]),col=color[1],type="l",lwd=2,frame.plot=F,xlab="chi-square value",xlim=c(0,25),ylim=c(0,0.5),ylab="Probability")
for (idx in 2:5) {
  points(x,dchisq(x,df=DF[idx]),col=color[idx],type="l",lwd=2)
}
legend("topright",legend=paste(DF,"df"),fill=color)
@
\end{figure}

% \begin{comment}
% Although the chi-square distribution could be calculated empirically
% by a manual or computer simulation (e.g. fill a bag with balls labelled
% ``+/+'', ``+/-'' or ``-/-'' so that their proportions are 1:2:1.
% Then, take a random sample and compute the $\chi^{2}$ value. Repeat
% many times and plot the $\chi^{2}$ values obtained in each iteration),
% the distributions of statistics such as the chi-square distribution
% are derived analytically, that is the formula (function) that it follows
% is derived from first principles.
% \end{comment}


The shape of the chi-square($\chi^{2}$) distribution
depends on the degrees of freedom\footnote{Recall that degrees of freedom is a measure of how many values are
free to vary when determining independent estimates of parameters.
When calculating frequencies in categorical data, the degrees of freedom
correspond to the total number of categories minus one. For example
in case 1, once we know the frequencies of two genotypes the value
of the third is calculated from these two, i.e. only two categories
out of three are \textquotedblleft free to vary\textquotedblright .
Thus degrees of freedom= 3 categories -1 = 2.}.

% \begin{comment}
% Note that as the degrees of freedom increase the distribution of expected
% chi-square values under the null hypothesis contain increasingly large
% values. My interpretation is that, since the chi-squares statistics
% is a sum across levels and for each level we always expect some deviation
% due to chance, the larger the number of levels the higher the value
% of chi-square expected by chance.
% \end{comment}
% \begin{comment}
% From wikipedia: In statistics, the number of degrees of freedom is
% the number of values in the final calculation of a statistic that
% are free to vary. The number of independent ways by which a dynamic
% system can move without violating any constraint imposed on it, is
% called degree of freedom. In other words, the degree of freedom can
% be defined as the minimum number of independent coordinates that can
% specify the position of the system completely. Mathematically, degrees
% of freedom is the number of dimensions of the domain of a random vector,
% or essentially the number of 'free' components (how many components
% need to be known before the vector is fully determined).
% \end{comment}

So, to determine the probability of getting a chi-square value equal
or higher than ours under the null hypothesis, we have to compare
it with the appropriate chi-square($\chi^{2}$) distribution according
to the degrees of freedom in our case.  The chi-square distribution can take values in \([0, +\infty]\)  (why is it bounded below by zero?) and it is not symmetrical.
% Moreover, the peak of the distribution is not at zero as one would
% intuitively expect. In fact, the peak approach to zero for small df
% but departs from zero for increasingly larger df. The reason is that
% the more categories/levels there are (df=\#categories-1), the more
% likely the observed and expected frequencies will differ just by chance.


% \begin{comment}
% As the chi-square statistic is a sum of residuals across categories,
% the larger the number of categories the larger the value of chi-square.
% In each category there is a random chance to have higher/lower number
% than expected, since they are squared, in each category there is a
% random chance of having a number larger than expected. As the number
% of categories increases the sum of these deviations increases. Note
% that residuals (observed-expected) are normalized to the expected
% values not to the number of categories.
% \end{comment}



% Thus, chi-square values far below the peak of the distribution would
% imply than the agreement between observed and expected is even better
% than what you would expected by chance. For this reason, chi-square
% tests are typically one-tailed test focusing on the right-hand tail
% (the discrepancy is too-large to be observed under the null hypothesis).
% Nevertheless, it is also possible to perform two-tail tests and focus
% on the left-hand tail. Note that, in this case, we are asking whether
% the discrepancy between observed and expected values are smaller that
% you would expect just by chance. If the one-tailed test focusing on
% the left hand of the distribution is significant the interpretation
% is that the data is \textquotedblleft too good to be true\textquotedblright{}
% pointing to dishonesty on the part of the researcher\footnote{Indeed the extraordinary conformity of Gregor Mendel\textquoteright s
% pea experiments have been subjected to such skepticism. Also, it is
% suspected that he looked at many phenotypical characteristics, several
% of which did not follow a typical Mendelian pattern (e.g. genes in
% sex chromosomes, and other) so he just ignored those features and
% focus those that adhere to his law...This debate, known as the \textquotedblleft Mendel-Fisher
% controversy\textquotedbl{} have lasted for over a hundred years and
% it involves Fisher, Pearson and other statisticians and it is still
% alive as shown by the very recent article in science by Radick \citet{Radick2015}.} or some flaw in the data or experimental design.




\subsubsection{Pearson's chi-squared test in R}

Getting back to our problem (case \#1), to determine if the offspring
of the mating between EDN2+/- animals is significantly different from
expected values, we just have to perform a (goodness-of-fit) chi-squared
test. Firstly, we import data in file \textquotedblleft mice\_genotypes.csv\textquotedblright{}
into R. As always, it is a good idea to take a look at the data and,
to this end, you can generate a pie or bar plot (figure \ref{fig:Representing-categorical-data}).
As we mentioned earlier (Lesson 1), these are the typical representations
for categorical data. In fact, because it only contains categorical
data, these are the only options available in R commander for this
data set. (More on visualization later: section \ref{sec:visualization}).

% \begin{comment}
% Note that in this case the bar plot does not contain any error bar
% because it represent the absolute (or relative) numbers of cases in
% each level, there is no uncertainty about its value.
% \end{comment}

\begin{figure}[h]
\caption{\protect\label{fig:Representing-categorical-data}Representing categorical
data (I)}

<<GrphCatDataI,fig.width=10,fig.height=4,echo=-1>>=
mice <- data.frame(ID = paste("id",
                             formatC(1:154, digits = 0,
                                    width = 4, flag = 0),
                            sep = ""),
                 Genotype = sample(c(rep("wt/wt", 40),
                                     rep("wt/mut", 72),
                                     rep("mut/mut", 42))))
par(mfrow=c(1,2))
pie(table(mice$Genotype))
barplot(table(mice$Genotype),xlab="Genotype",ylab="Frequency")
@
\end{figure}

Wait!!! Do you think the pie chart is a good type of plot? Why or why not?

We can ask for the frequency counts by selecting \emph{Summaries
\textendash > Frequency} distributions under the \emph{Statistics}
menu (figure \ref{fig:Chi-square-test-in}, follow up to step 2 and
click \textquotedblleft OK\textquotedblright{} button). In the \textquotedblleft Output\textquotedblright{}
window you'll get the absolute and relative number of cases for each
level of the categorical variable \textquotedblleft Genotype\textquotedblright .

Back to testing. Within this same menu we can perform a chi-square goodness of fit
test by selecting the box for this test (see figure \ref{fig:Chi-square-test-in}
step 3). Importantly, when we ask for the test, a pop-up window prompt
us to indicate the expected frequencies (figure \ref{fig:Chi-square-test-in}
step 4). By default the program will assign equal frequencies to each
category. In our case, we should, enter 0.25:0.5:0.25 corresponding
to the expected frequencies under the null hypothesis (i.e. Mendelian
ratio).

\begin{figure}[h]
\centering{}\includegraphics[width=10cm]{pasted14-rdu.png}
\caption{\protect\label{fig:Chi-square-test-in}Chi-square test in R (I)}
\end{figure}


Note that we did not have to explicitly indicate the degrees of freedom
as the program calculates it from the number of categories (see output
from the test). Now, the test returns a p-value of 0.7. Because the
resulting probability it is not small, we cannot rule out the null
hypothesis and hence we conclude that we do not have enough evidence
supporting that the ratios are not Mendelian. Note that this is not
the same as concluding that ratios are Mendelian. This is the code:


<<Rcode_for_chisquare,tidy=TRUE,echo=-1>>=
mice<-data.frame(ID=paste("id",formatC(1:154,digits=0,width=4,flag=0),sep=""),Genotype=sample(c(rep("wt/wt",40),rep("wt/mut",72),rep("mut/mut",42))))
table(mice$Genotype)
chisq.test(table(mice$Genotype),p=c(0.25,0.5,0.25), correct = FALSE)
@

You should be able to interpret the output.

\section{Contingency tables: testing independence between rows and columns}\label{sec:contingency-tables}

Contingency tables allow us to organize and visualize categorical data.write of a table with
the levels of one variable in one dimension across the levels of the
other variable in the second dimension. In fact, table \ref{tab:Frequency-of-HRE}
is a contingency table that summarizes the observed data. Some of the simplest contingency tables are 2x2 tables: 2 rows and 2 columns. For example, what we had in section \ref{sec:two-prop-conf} and to this we go next.



\subsection{2x2 contingency tables}\label{sec:2x2}

We used these data in  \ref{sec:two-prop-conf}. Let us see it again

<<>>=
xtabs(~ Drug + Outcome, data = Exercise3)
@

That is a contingency table. Above, we tested for differences in proportions between those treated with different drugs. But that there are no differences in proportions is the same as saying that the probability of healing (or not healing) does not depend on the drug given. In other words, that there is independence between Outcome and Drug. Or put it this way: we want to test whether the frequencies of Healed vs.\ Non-healed change across levels of Drug (i.e., if the frequencies of Healed and non healed are different between the drugs). So we now phrase our null hypothesis as saying that Outcome and Drug are independent.


Under \(H_0\), i.e., if there is independence between Outcome and Drug, then, for example, the probability of both being healded and having taken Piren is the product of each. And therefore the expected number of people who took Piren and are Healed should be the product of those two probabilities times the total sample size. In other words, under \(H_0\), the expected number of people who are Healed and took ``Piren'' should be \( P(Piren) P(Healed) n\), where \(n\) is the total sample size (in our case 61). We can write it as: the expected number of people in the ``Healed and Piren'' cell should be \(\bar{\mu}_{Healed,Piren} = n  \bar{\pi}_{Piren} \bar{\pi}_{Healed} \)

We wrote the above for one of the cells, but for any table, where we index rows with \(i\) and columns with \(j\), we can write this more generally as

\(\bar{\mu}_{ij} = n \bar{\pi}_{i+} \bar{\pi}_{+j}\).


Now, \(\bar{\pi}_{Piren}\), or \(\bar{\pi}_{i+}\), for \(i = Piren\), is the marginal probability of Piren: the sum of the entries in the row that says Piren divided by the total. So \(\frac{23 + 7}{61}\). And similarly for \(\bar{\pi}_{Healed}\), or \(\bar{\pi}_{+j}\).

Therefore, in our 2x2 table we can compute the four \(\bar{\mu}_{ij}\), the four expected numbers in each cell under \(H_0\).

And we are done. We now compute the \(\chi^{2}\) statistic as \(\sum \frac{(n_{ij} - \bar{\mu}_{ij})^2}{\bar{mu}_{ij}}\).


This happens to be the same formula we saw in section \ref{sec:chi1}. In both cases we are computing the squared of the deviations between the observed and the expected, dividing them by the expected, and adding all of them.

In section \ref{sec:chi1} those were deviations with respect to a single variable. Here they are deviations where the expected and observed are cross-classified according to two variables. (And this could be extended to three, four, \ldots).


Now, all the terms in here will make sense:
<<>>=

tableEx3 <- xtabs( ~ Drug + Outcome, data = Exercise3)
chisq.test(tableEx3, correct = FALSE)

## same as
chisq.test(matrix(c(23, 18, 7, 13), nrow = 2), correct = FALSE)
@


By the way, notice the similarities between what we have done and examining interactions in a two-way ANOVA.


\subsection{\textit{r\ x\ c} or \textit{I\ x \ J} contingency tables}\label{sec:rc-tables}

(The \textit{r\ x\ c} and  \textit{I\ x \ J} are two common ways of referring to the same thing: 2-dimensional contingency tables with \(r\) or \(I\) rows and \(c\) or \(J\) columns.)

These just extends what we saw in the previous section: instead of a 2x2 table, we have a two-dimensional table but rows and/or columns can have more than two categories or types. So we still have \(\chi^{2} = \sum \frac{(n_{ij} - \bar{\mu}_{ij})^2}{\bar{mu}_{ij}}\).


In other words,  we want to test if the distribution
of frequencies of each level of the categorical variable \(I\) differ
across the levels of the other categorical variable \(J\) or if frequencies
of \(I\) are the same regardless the levels of \(J\).


For example, let us look at the gene gene expression experiment:


<<>>=
Hx <- read.table("Hyp_effect.csv", head = TRUE, sep = ",")
## Reorder the factor levels
Hx$Effect <- factor(Hx$Effect,
                    levels = c("Upregulated", "Non_regulated", "Downregulated"))
Hx$HRE <- factor(Hx$HRE,
                 levels = c("HRE+", "HRE-"))

(Hx_tab <- xtabs(~ Effect + HRE, data = Hx))
chisq.test(Hx_tab, correct = FALSE)
@

% (You can do all, or most, of the above from R Commander:
% \begin{figure}[h]
% \centering{}\includegraphics[width=12cm]{pasted15-rdu.png}
% \caption{\protect\label{fig:Chi-square-test-in-1}Chi-square test in R (II)}
% \end{figure}
% )


You should be able to interpret the output from above. But ... what about degrees of freedom: \(df = (I - 1) (J - 1)\), where \(I\) is the number of categories in the first variable, and \(J\) in the second. So this example \((2 - 1)\ (3 - 1)\).



Another, less extreme example. This comes from Table 3.2, p.~179 of \citet{bilder2024} and refers to a study about bloating severity after eating fiber-enriched crackers\footnote{The book provides full references for the origin of these data; the description is achived at \Burl{https://web.archive.org/web/20000620172010/http:
    //lib.stat.cmu.edu/DASL/Stories/HighFiberDietPlan.html}}. The data are in file \texttt{Fiber.csv}, which I downloaded from the book's website, \Burl{http://www.chrisbilder.com/categorical/programs_and_data.html}.

Note that we have a slightly different format for the data. Using the same code as in p.~180 of \citet{bilder2024}, with minor modifications, we read the data and do a few transformations to the factors, and then create our final table (the code can be downloaded from their web page, \Burl{http://www.chrisbilder.com/categorical/programs_and_data.html})
<<>>=
diet <- read.csv(file = "Fiber.csv")

# Match order given in table
diet$fiber <- factor(x = diet$fiber,
                     levels = c("none", "bran", "gum", "both"))
diet$bloat <- factor(x = diet$bloat,
                     levels = c("none", "low", "medium", "high"))

diet.table <- xtabs(formula = count ~ fiber + bloat, data = diet)
diet.table
@

Now, do a chi-square test:

<<>>=
chisq.test(diet.table, correct = FALSE)
@

You should be able to interpret it, and understand the number of degrees of freedom.

\subsection{Higher-order tables}
\label{sec:higher-order-tables}

The same logic applies to tables of more than two dimensions, say Drug x  Outcome x Exercise.  Now, we can have dependencies between pairs of variables, or among the three. For example, each drug might have a different effect on outcome, and exercise might affect this difference. We will not pursue this here. Arguably, though, except for very specific problems (the Mantel-Haenzsel test mentioned below being one of them), it is probably a better idea to directly approach this using models (logistic, logit, and loglinear, for binomial, multinomial, and Poisson response variables, respectively); this is just mentioned in section \ref{sec:glms}.

% \subsection{Contingency tables}

% To analyze the interaction between categorical variables, we use
% \emph{contingency tables}. This is just a way to organize and visualize
% this type of data. To construct them we just write of a table with
% the levels of one variable in one dimension across the levels of the
% other variable in the second dimension. In fact, table \ref{tab:Frequency-of-HRE}
% is a contingency table that summarizes the observed data. The organization
% of data in this way allow us to compute the expected frequencies under
% the null hypothesis that both variables are independent (i.e. there
% is no association between the presence of the HRE an the regulation
% of gene expression by hypoxia). We can compute a table of the expected
% frequencies by completing each cell according to the formula $expected_{rc}=\frac{margin_{r}margin_{c}}{total}$
% where \emph{margin\protect\textsubscript{r}} is the sum of frequencies
% in row \emph{r} (total row \emph{r}), \emph{margin\protect\textsubscript{c}}
% is the sum of frequencies in column \emph{c} (total column \emph{r}),
% and \emph{total} is the sum of frequencies in all cells (total number
% of observations). The only thing this formula does is to compute how
% the total counts in a given level of variable X would distribute across
% the different levels of Y if they are independent.

% Although we will only deal with two variables in this class, you
% could construct multi-way contingency tables for more than two categorical
% variables.



% \section{Chi-square test for heterogeneity}

% Now, let focus on the second case above in which we wanted to determine
% whether HIF binding was preferentially associated induction/repression
% of gene expression or had no preference. As we mentioned before, we
% are dealing also with categorical data, but there are two important
% differences with the previous case. The first one is that we have
% two categorical variables (presence of HRE and effect of hypoxia on
% gene expression) instead of one. Secondly, in the previous case we
% wanted to compare the observed frequencies to those expected under
% a specific null hypothesis (Mendelian frequencies). How do we calculate
% the expected frequencies in this second case? In other words, what
% is the null hypothesis in this case? The key to the answer is that,
% in this case, we are actually testing whether there is an association
% between the two variables we are measuring (note that we are asking
% whether the presence of an HRE is associated with gene expression).
% In this scenario our null hypothesis is that there is no association
% between these variables. That is, we want to test if the distribution
% of frequencies of each level of the categorical variable X differ
% across the levels of the other categorical variable (Y) or if frequencies
% of X are the same regardless the levels of Y. In fact, this is analogous
% to the interactions in factorial ANOVA.


% \begin{comment}
% to make it easier to understand use the following example. We are
% interested in studying weather the color of the clothes a person wears
% depend on their gender. We sample a group of 100 people (the class)
% in which are 60 women and 40 men. And found that 20 of women wear
% colorful (yellow, pink, red,...) raincoats whereas 40 wear dark (black,
% grey, blue) raincoats. Among men, the numbers are 5 (colorful) and
% 35 (dark). What numbers would we expect if the two variables were
% independent (null hypothesis)? One way to think about it is: everyone
% in the class leave their raincoats in the hangers, then randomly assign
% raincoats to each person. What would be the distribution of colors
% among men and women in the class? use a contingency table with gender
% in the rows and colors in the columns to compute the expected write
% down the marginals and distribute colors (column marginals) between
% male and female according to the proportion between the row marginals.
% The expected numbers under the null hypothesis (variables are independent)
% could be obtained using a thought experiment: All people in the class
% leave their raincoat in the hangers, then a randomly picked raincoat
% is assigned to each person in the class, then count the number of
% males/females wearing colorful/dark rainbows. Repeat the experiment
% n times. Since 1/4 of the raincoats are colorful, a quarter of the
% women and a quarter of men will receive a colorful coat on average,
% that is 15 women and 10 men.
% \end{comment}


% \subsection{Chi-squared test for heterogeneity\protect\footnote{also known as chi-squared test of independence to differentiate it
% from the goodness of fit describe before} in R}

% Read the data in file \textquotedblleft Hyp\_effect.csv\textquotedblright{}
% into R and take a look at the raw data (\textquotedblleft View data
% set\textquotedblright{} button). As you will see there are 11673 genes
% whose response to hypoxia has been determined and categorized as \textquotedblleft UP\textquotedblright{}
% (upregulated or induced), \textquotedblleft DOWN\textquotedblright{}
% (downregulated or repressed) or \textquotedblleft NO\textquotedblright{}
% (expression not altered by hypoxia). You can get these numbers from
% \emph{Statistics->Summaries->Frequency distributions}. Also, the presence
% of the HIF binding site has been determined in the promoter of each
% gene and recorded as \textquotedblleft YES\textquotedblright{} (present)
% or \textquotedblleft NO\textquotedblright{} (not present). We could
% also explore the data using bar and pie graphs as we did above for
% case \#1. Moreover, using R we can represent both variables simultaneously
% (see figure \ref{fig:Chi-square-test-in-1}), but this function is
% not implemented in R commander. Finally, we can generate a contingency
% table from the Statistics menu: Statistics->Contingency tables->Two-way
% table (see figure \ref{fig:Chi-square-test-in-1} and follow it skipping
% step 3a) and also perform a chi-square test on this table (see figure
% \ref{fig:Chi-square-test-in-1}).

% \begin{figure}[h]
% \caption{\protect\label{fig:Chi-square-test-in-1}Chi-square test in R (II)}

% \centering{}\includegraphics[width=12cm]{pasted15-rdu.png}
% \end{figure}


% Because the resulting probability is small, you reject the hypothesis
% that presence of HRE and gene expression are independent.

% Note that, to calculate the degrees of freedom, for contingency tables
% we multiply the number of categories (minus 1) of each variable. For
% example, in a case with two variables with 2 and 3 categories respectively
% (as in the case \#2), df=(2-1){*}(3-1)=2.

% \begin{comment}
% Finally, it should be noted that the degrees of freedom are not based
% on the number of observations as with a Student's t or F-distribution.
% For example, if testing for a fair, six-sided die, there would be
% five degrees of freedom because there are six categories/parameters
% (each number). The number of times the die is rolled will have absolutely
% no effect on the number of degrees of freedom.

% The degrees of freedom in the case of contingency tables can be visualized
% as the marginal occupying the calls in the contingency adjacent to
% them, the remaining empty cells are the df.
% \end{comment}



\section{Looking at residuals}\label{sec:residuals}


So we have very strong evidence of deviations from the null in the Hx data and some evidence in the diet data. This might tell us there are deviations from independence, but we would like to examine what is behind these deviations. What cells are deviating from the expected or how strong is the association. Moreover, or large sample sizes it
is easy to get a very small p-value even when the difference between
the observed and expected values is small\protect\footnote{Note that this is true for any other statistical test, not just those
for categorical variables}. As usual, a statistically significant p-value does not necessarily
indicates a significant difference from the practical (or biological)
point of view.


We can look at the residuals, the deviations from the expected, to understand what is happening (R commander makes this available under ``Components of chi-square'').
Residuals sum up to the value of the \(\chi^2\) for the test.

\citet[p.~39]{agresti2018}, \citet[p.~81]{agresti2012}, and \citet[p.~183]{bilder2024} use standardized residuals (or standardized Pearson residuals), that we can obtain using \texttt{stdres} instead of \texttt{residuals}.  Cells with larger expected frequencies tend to have larger deviations, so we want to scale (standardize) to account for this. Standardized residuals are also asymptotically standard normal under \(H_0\); thus, a cell with a standardized residual with an absolute value larger than around 2 (when few cells in the table) or 3 (when the table has many cells) indicates lack of fit of \(H_0\) in the given cell\footnote{Why do we require larger deviations for more cells? To prevent from finding one large residual just by chance.}.

% (so standardized residuals larger than about 2 or 3 in absolute value indicate deviation from \(H_0\) in that cell \citealp[p.~81]{agresti2012}).

<<res_hx>>=
chisq.test(Hx_tab, correct = FALSE)$residuals^2
chisq.test(Hx_tab, correct = FALSE)$stdres

chisq.test(diet.table, correct = FALSE)$residuals^2
chisq.test(diet.table, correct = FALSE)$stdres
@

%% the diet one colSums sum to 0 because of the design: all rowSums are the same.
%% Agresti, Intro, explains this in some detail for the 2xJ table, why over the smaller dimension they sum to 0. See p. 40

For large (meaning large numbers of rows and/or columns) patterns in residuals might not always be discernible and thus might not always help us much. Section \ref{sec:OR} mentions another approach.



\section{Strength of association: odds ratios and relative risks}\label{sec:OR}

% The statistical test we have seen indicate when there is a statistically
% significant relationship between two variables, but it does not indicate
% how strong the association is. Moreover, for large sample sizes it
% is easy to get a very small p-value even when the difference between
% the observed and expected values is small\protect\footnote{Note that this is true for any other statistical test, not just those
% for categorical variables}. In words, a statistically significant p-value does not necessarily
% indicates a significant difference from the practical (or biological)
% point of view. For all this reasons, it is not advisable to draw conclusions
% just based on the p-value and we should also

We continue with the theme above (section \ref{sec:residuals}).  So there are deviations from independence: can we make sense of it? The odds ratio and relative risk are other measures of association.

% Here, we try to look at  the
% \emph{effect size}, that is the difference between the null and actual
% value of the parameter of interest. In the case of chi-square test
% we can use an \emph{odds ratio} or the \emph{risk ratio} to quantify
% the strength of the association.

Suppose this table shows number of patients with different outcomes (recoverd or non-recovered) under two treatments (drug and control):

\begin{tabular}{|c|c|c|c|}
\hline
 &  & \multicolumn{2}{c|}{Outcome}\tabularnewline
\hline
\hline
 &  & Recovered & Non-recovered\tabularnewline
\hline
\multirow{2}{*}{Group} & Drug & a & b\tabularnewline
\cline{2-4}
 & Control & c & d\tabularnewline
\hline
\end{tabular}


In 2x2 tables, the relative risk is the ratio or the risk ratio is \(\pi_1 / \pi_2\), where \(\hat{\pi}_1 = \frac{a}{a + b}\), \(\hat{\pi}_2 = \frac{c}{c + d}\). So we are comparing how much larger the probability of recovery is for the patients in the Drug group relative to those in the control group. This is a sensible thing to do. Can you think why?


% Rate = proportion in group with condition present ($Rate_{a}=\frac{a}{a+b}$;$Rate_{b}=\frac{c}{c+d}$)

% Then risk ratio is: $Rr=\frac{Rate_{a}}{Rate_{b}}$

Another statistic is the odds ratio. The odds is defined as \(\frac{\pi}{1 - \pi}\). The odds ratio is then \(\frac{\text{odds}_1}{\text{odds}_2}\). In the table above, the estimate of the odds ratio is thus: \(\hat{\theta} = \frac{a/b}{c/d} = \frac{a\ d}{b\ c}\)


It is possible to obtain estimates and confidence intervals for the odds ratio (for example, see package \texttt{epitools}). When examining odds ratio, often a relevant null hypothesis is that the odds ratio is 1.

Odds ratios and relative risks are discussed in \citet[pp.~39 and ff]{bilder2024} and \citet[pp.~29 and ff]{agresti2018} (and it is possible to small sample confidence intervals for them; e.g., \citealp[p.~50]{agresti2018}).



% Odds=probability \emph{of success}/(1-probability \emph{of success}).

% ($Odds_{1}=\frac{P_{a}}{(1-P_{a})}=\frac{a/\left(a+b\right)}{1-(a/\left(a+b\right))}=\frac{a}{b}$;$Odds_{2}=\frac{c}{d}$)

% Then the odds Ratio is: $OR=\frac{Odds_{1}}{Odds_{2}}$ and the log
% OR is: $LOR=\ln\left(OR\right)$


% \subsection{More on entering categorical data in R commander}

% Lets explore the strength of associations in further detail. Imagine
% that you study the association between the presence of a given SNP
% and cancer development and, after studying a large number of cases
% you get the following results:

% \begin{tabular}{|c|c|c|}
% \hline
%  & Cancer & Healthy\tabularnewline
% \hline
% \hline
% SNP present & 549 & 49788\tabularnewline
% \hline
% SNP absent & 461 & 51898\tabularnewline
% \hline
% \end{tabular}

% As we have seen above, R can compute the contingency table from raw
% data. However, since even very large raw data files can be summarized
% in a few numbers, sometimes it is just easier to type the contingency
% table directly into R commander. In addition, in many cases, we will
% just get the tabulated rather than raw data. For these reasons, R
% commander includes a specific menu to facilitate this task. In order
% to get familiar with it, we are going to enter the data corresponding
% to case \#2 as provided in table \ref{tab:Frequency-of-HRE}.

% To enter these values in a tabular form go to Statistics->Contingency
% tables->Enter and analyze... select the appropriate number of rows
% and columns and enter the values. Finally select the test(s) you want
% to run (see figure \ref{fig:Entering-contingency-tables}).

% \begin{figure}[h!]
% \caption{\protect\label{fig:Entering-contingency-tables}Entering contingency
% tables}

% \centering{}\includegraphics[width=12cm]{pasted9.png}
% \end{figure}


% Please take a look at the OR in the output of the Fisher's exact
% test. Also compute by hand the risk of developing cancer in the population
% with and without the SNP (compute the risk ratio). How large is the
% ratio?



\section{Assumptions of the chi-square test, minimal expected counts, Fisher's exact test and all that}

This should not come as a surprise: we assume all \emph{observations} are independent\footnote{Do not mistake it for independence of \emph{variables}}. For example the genotype of a mouse does not affect the genotype of other mice or the presence/absence of HRE in a given gene has no effect on other genes. Similarly we do not have the same individual measured multiple times, or sets of related/paired individuals, etc. In case they are not independent, other procedures should be used (for example, the McNemar's test for two related groups; see \ref{sec:Other-test-for}).


Much more contentious are these two related issues:
\begin{enumerate}
\item What are the sample size and/or expected counts requirements for us to apply the chi-square test? In other words, are there sample sizes or expected counts such that we should consider other procedures, because the chi-square is not a good idea (low power or high type I error or whatever)?
\item And if we have small sample sizes or expected counts, what should we do?
\end{enumerate}

There are no simple answers, or at least there are strong disagreements about the answers (simple or otherwise).


\subsection{Sample sizes and expected counts for us to trust the chi-square}
\label{sec:sample-sizes-expect}

Why be concerned about sample sizes and expected counts? Briefly, because the use of the chi-square distribution involves some asymptotic arguments.

A common rule used to be that no more than 20\% of the expected (\textbf{expected, not observed!}) frequencies should be less than five; another is 5 or more in all cells of a 2-by-2 table, and 5 or  more in 80\% of cells in larger tables, but no cells with zero expected  count.

It turns out that these rules might be overly restrictive and the details can depend strongly on the size of the table (numbers of rows and columns) and the total sample size.



\subsection{What can we do if expected counts are too small or \ldots}

The answer is often ``use Fisher's exact test''. That, however, might not be the best idea according to some.

\subsubsection{Fisher's exact test}
\label{sec:fishers-exact-test}

It can be used for 2x2 tables and for larger rxc tables. Conditioning on the observed row and column margins, we compute the two-tailed p-value as the sum of the probabilities of all the tables that have a probability as extreme as, or more extreme than, the one of the table we observed. The probability of each table is obtained, in the 2x2 case, directly from a hypergeometric distribution\footnote{For one-tailed tests on 2x2 tables we can just focus on the number of cases in the first cell, i.e., the number of observations in the cell in the first row and column, \(n_{11}\). For fixed row and column margins, given \(n_{11}\) the rest of the entries are known. Then, we can just count how many tables there are with \(n_{11}\) as extreme or more extreme than the one we observed.} For larger tables (e.g., 3x4) similar approaches exist, using the multivariate hypergeometric distribution\footnote{For details, see, for example, \citet[section 16.5.2]{agresti2012}, \citet[section 2.4.1.4]{tang2023a}, \citet[section 6.2.1, p.~4.4.6]{bilder2024}.} and they are implemented in R in function \texttt{fisher.test}.


To take the mystery out of this procedure, this is the classical lady testing tea example. I copy a couple of figures from \citet{agresti2018} (pp.~47 and 48):


\begin{figure}[h]
  \centering{}\includegraphics[width=10cm]{fisher-1-agresti.png}\\
  \centering{}\includegraphics[width=10cm]{fisher-2-agresti.png}
    \caption{\label{fig:fisher-agresti} Tables 2.7 and 2.8 from Agresti, 2018, \textit{An introduction to categorical data analysis, 3rd ed}, Wiley.}
\end{figure}


A one-sided P-value, had we observed 3 cases in \(n_{11}\) would be \(0.014 + 0.229\). A two-tailed test would be \(0.014 + 0.229 + 0.229 + 0.014\).



<<>>=
## One-sided: alternative is more matches
fisher.test(matrix(c(3, 1, 1, 3), nrow = 2), alternative = "greater")

## Two-sided
fisher.test(matrix(c(3, 1, 1, 3), nrow = 2))

@



% It computes the exact probability of getting the observed statistics (and any other more extreme) by exhaustively examining all the possible distributions of counts in the table. The test conditions on both the row and column marginals, and for a 2x2 table this yields a hypergeometric distribution, for example for the counts in the first row and column. The test can be extended to rxc tables.

%% Recall in the 2x2 we often use, for example, counts in n_{1,1}. See Agresti or Tang et al., p. 41, sect. 2.2.1.3

The test can become computationally unfeasible for very large tables (large in terms of dimensions, numbers of rows and columns, and/or sample size). In this case, we can obtain p-values by simulation (in R use argument \texttt{simulate.p.value = TRUE} in larger than 2x2 tables). Instead of examining exhaustively all possibilities, we repeatedly simulate outcomes (samples) under the null hypothesis (null hypothesis and, for Fisher's, fixed row and column margins) using a Monte Carlo procedure; for each such simulation we compute the statistic and then count how often the simulated statistics are larger than the observed one.
Monte Carlo simulation allows us to approximate very closely (more closely the larger the number of simulations) the exact p-value.


Let us do this in the 3x2 table. From the menus, you cannot directly use the \texttt{simulate.p.value} options, so I'll do everything without the menus. Just copy the code.

This is the usual Fisher's exact test (and this you can get from R commander menus). I've added also the chisq.test.
<<>>=
fisher.test(Hx_tab)
chisq.test(Hx_tab, correct = FALSE)
@

Now, Fisher's with simulation of p-values. We use a small number here, the default of 2000 (for real, I'd use a lot more)\footnote{Incidentally, the p-value you are likely to get is 0.0005, actually 0.00049975, which is \(1/(B + 1)\); this is because the pattern we observe is the most extreme of all, and we compute the p-value in the usual way: how many are as extreme as, or more extreme than, what we find. Since we have used 2000, the denominator is 2001. This is technical material that you can skip.}:
<<>>=
fisher.test(Hx_tab, simulate.p.value = TRUE, B = 2000)
@

Let us use a different data set, where things are not as extreme, so that you can see that the p-value with \texttt{simulate.p.value} will often different from run to run (unless you use an infinite number of simulations, and most likely you won't have the patience to do that). We will use again the diet (bloating) example from section \ref{sec:rc-tables}. Now, we do three different tests, but I run the one with simulated p value a few times, to show its variability (you can decrease the variability by increasing B)

<<>>=
chisq.test(diet.table, correct = FALSE)
fisher.test(diet.table)
fisher.test(diet.table, simulate.p.value = TRUE)
fisher.test(diet.table, simulate.p.value = TRUE)
fisher.test(diet.table, simulate.p.value = TRUE)
@

This is an interesting case, because of the small expected cell counts. \citet{bilder2024} discuss it in detail in pp.~180 and ff.\ (and they use a simulation approach, described in p.~182 and section 6.2.2, but a more sophisticated one, that we won't cover here; regardless, the p-value is obtained with the same procedure as with \texttt{simulate.p.value}\footnote{Or, at least, that is my understanding of what they do, as they seem to condition on the margins: in p.~182 they write: ``(..) we can use one call to rmultinom(), where we set each \(\pi_{ij}\) (prob argument) to \(n_{i+} n_{+j}/n^2\) (the estimated expected count under the null hypothesis divided by the sample size). We chose to use the one multinomial model
here (...)''}).


\subsubsection{Issues with Fisher's test}
\label{sec:issues-with-fishers}

The main concerns in the literature are:
\begin{itemize}
\item conservatism
\item the conditioning on row and column margins
\end{itemize}

Conservatism can be diminished using the \texttt{mid P-value}. This is well explained in \citet[section 2.6.3]{agresti2018} and in \citet[pp.~458 and 586 of]{bilder2024}.



\FIXME{about here}

\subsection{Chi square with \texttt{simulate.p.value}?}

This is another alternative: just use your usual \texttt{chisq.test} but with option \texttt{simulate.p.value = TRUE}. No reliance in the asymptotic approximation. There is catch, though: in R, when you use \texttt{simulate.p.value}, the simulation uses the same sampling model as in Fisher's exact test\footnote{This is copied verbatim from the help of \texttt{chisq.test}: ``In the contingency table case, simulation is done by random sampling from the set of all contingency tables with given marginals, and works only if the marginals are strictly positive. (...)  Note that this is not the usual sampling situation assumed for the chi-squared test but rather that for Fisher's exact test.''}

  %% See the help or this discussion https://stats.stackexchange.com/questions/159057/alternatives-for-chi-squared-test-for-independence-for-tables-more-than-2-x-2/159063#159063

There is nothing wrong with this, but this is just Fisher's test, so if you have concerns about the sampling model of Fisher's exact test, this simulation procedure carries the same concerns.

More sophisticated simulation schemes are possible, but that is beyond the scope of this class (and might run into other problems, like generating marginals that are 0, etc). In R, package \texttt{exact2x2}, \Burl{https://cran.r-project.org/web/packages/exact2x2/index.html} might be of interest. See also









% observed frequencies under a hypergeometric distribution (not that this matters for this course, but you might here people refer to this test this way'')

% Note, though, that Fisher's exact test is a test that conditions on the marginals of both rows and columns. The test is valid even if the marginals row or the colum or none of the marginals are fixed, but other tests are sometimes suggested for small samples.


% However, unlike the permutation tests, it does not relies on random
% sampling (unless you select \texttt{simulate.p.values}). Instead







% or when we can use the usual chi-square test

% \begin{itemize}
% % \item Simple random sample \textendash{} The sample data is a random sampling
% % from a fixed distribution or population where every collection of
% % members of the population of the given sample size has an equal probability
% % of selection.
% \item Sample size (whole table) \textendash{} A sample with a sufficiently
% large size is assumed. If a chi squared test is conducted on a sample
% with a smaller size, then the chi squared test will yield an inaccurate
% inference. The researcher, by using chi squared test on small samples,
% might end up committing a Type II error.
% \item Expected cell count \textendash{} Adequate expected cell counts. A typical rule states that no
%   more than 20\% of the expected (\textbf{expected, not observed!}) frequencies should be less than five\footnote{A common rule is 5 or more in all cells of a 2-by-2 table, and 5 or
%     more in 80\% of cells in larger tables, but no cells with zero expected
%     count.}. Because, when expected frequencies are small, the calculated value
%   of chi-square tends to be too large and will therefore indicate a
%   lower than appropriate probability, thereby increasing the risk of
%   Type I error\footnote{To visualize this, imagine that you are trying to estimate if a couple
%     with a single child have the expected ratio of 50\% female:50\%male.
%     Regardless the sex of the child, the observed frequencies (100\%:0\%
%     or 0\%:100\%) will differ dramatically from the expected ones. The
%     small sample size avoids getting an accurate estimation of the frequencies.}. This rule, though, might be too restrictive in some cases, in the sense the chi-square approximation could be OK.
% \end{itemize}

% There are several alternatives (other than increasing sample size!),
% when the ``large enough'' conditions are not met:
% \begin{enumerate}
% % \item Yates\textquoteright{} correction or the continuity correction\footnote{technically it is applicable only in cases with df=1}.
% % It simply consists in subtracting 0.5 from the absolute value\footnote{by using the absolute value we ensure that we always decrease the
% % value of the difference} of the difference between observed and expected counts: $\chi^{2}=\sum_{category}\frac{\left(|observed_{i}-expected_{i}|-0.5\right)^{2}}{expected_{i}}$
% % . Obviously, this result in smaller values of the chi-square($\chi^{2}$)
% % statistic\footnote{Note also that it will have much higher relative impact when frequency
% % counts are small than when they are large} and thus larger p-values.
% \item The probability is not computed by comparing with a distribution such as the $\chi^{2}$ distribution. Instead, we repeatedly simulate outcomes (samples) under the null hypothesis using a Monte Carlo procedure; for each such simulation we compute the statistic and then count how often the simulated statistics are larger than the observed one.

%   Permutation tests (and Monte Carlo procedures) \textbf{are not} exact tests, in contrast to what happens with the usual (see below) Fisher's exact test. Permutation tests (Monte Carlo procedures) are a way to approximate the exact p-value we would obtain if we could exhaustively examine all the possible configurations of the data under the null hypothesis% \footnote{This is in fact doable in some small tables. So one can get an exact test by computing the chi-square statistic for all possible assignments under the null. And this is not necessarily Fisher's test, but one that conditions on whatever we want to condition, such as fixed row totals, for example.}
%   . One should use a sufficiently large number of simulations (the error of the estimated p-value is a decreasing function of the number of simulations).

%   And Fisher's test if we use the \texttt{simulate.p.value = TRUE} is not ``exact'' in this sense either (the p-value is estimated using a Monte Carlo simulation, not by comparing with the exact distribution nor by exhaustively examining all possible cases).

%   But the exactness here is not the relevant factor. The permutation procedure allows us to compute p-values even if the conditions for the use of the $\chi^{2}$ distribution are not met.


  % And how is the sampling done for chisq? For
  % the RxC table it is actually Fisher's test
  %% See the help or this discussion https://stats.stackexchange.com/questions/159057/alternatives-for-chi-squared-test-for-independence-for-tables-more-than-2-x-2/159063#159063




%   Permutation test. This test is unique in that it does not calculates
% an statistic that is then used to compute the probability but instead
% it does calculates the probability empirically by simulating the repeated
% random sampling of an hypothetical population containing the expected
% proportions in each category. Thus, when conducting a good-of-fitness
% test with a sample size and/or expected counts are less than 5 for
% more than 20\% of the cells, the permutation test should be used
% instead.



% \begin{comment}
% by me: this just simulates multiple random samples from the expected
% distribution (or the null), computes the chi-square statistic for
% each of them and counts the number of times the value of the statistic
% is higher that the one observed.

% In R it is done using the chisq.test() function with the argument
% \emph{simulate.p.value=T}. In this case the p-value is derived via
% Monte Carlo simulation. The repeated random sampling from an hypothetical
% population is an example of a more general procedure called Montecarlo
% Simulation Method that uses the properties of the sample, or the expected
% properties of a population, and takes a large number of simulated
% random samples to create a distribution that would apply under the
% null hypothesis.
% \end{comment}


% \item Fisher's Exact test. It can be used for 2x2 contingency tables regardless the sample size.

% However, unlike the permutation tests, it does not relies on random
% sampling (unless you select \texttt{simulate.p.values}). Instead it computes the exact probability of getting the observed statistics (and any other more extreme) by exhaustively examining all the possible distributions of counts in the table\footnote{The test conditions on both the row and column marginals, and for a 2x2 table this yields a hypergeometric distribution, for example for the counts in the first row and column. } observed frequencies under a hypergeometric distribution (not that this matters for this course, but you might here people refer to this test this way'')

% Note, though, that Fisher's exact test is a test that conditions on the marginals of both rows and columns. The test is valid even if the marginals row or the colum or none of the marginals are fixed, but other tests are sometimes suggested for small samples.




% \begin{comment}
% From: \url{http://www.physics.csbsju.edu/stats/exact.html} , from
% \url{http://www.biostathandbook.com/fishers.html}, from \url{http://www.stat.wisc.edu/~st571-1/06-tables-2.pdf}
% (this one is saved in the folder ``Estadistica\_y\_probabilidad''
% under ``Books'' in my computer) and from \citet{McKillup2012}.
% In the exact method, we view the particular contingency table we got
% in our experiment as embedded in a universe of similar tables that
% have the same outcome probabilities as our table (i.e., have the same
% row totals) and the same distribution of treatments (i.e., have the
% same column totals). To compute the exact probability we generate
% all possible variations of the table that have the same column and
% row counts. Then calculate the probability of each table and then
% sum the probability of the given table and every other \textquotedbl unusual\textquotedbl{}
% table. (The \textquotedbl unusual\textquotedbl{} tables are those
% that have probabilities less than or equal to the given table.) If
% the total probability of such unusual tables is \textquotedbl small\textquotedbl{}
% \textendash that is if it is rare to have observed such unlikely tables\textendash{}
% we can reject the null hypothesis. To calculate the probability of
% each table we just sample each category without replacement and compute
% the it. To calculate the probability of each table (i.e. of getting
% the exact number we have in each cell of the table) we use the hypergeometric
% distribution. The hypergeometric distribution is similar to the binomial
% distribution in that it counts the number of successes in a sample
% of size n , but the sample is made without replacement from a finite
% population, and so separate trials are not independent. For example,
% a bucket contains r red balls and w white balls and n balls are sampled
% without replacement. If the random variable X counts the number of
% red balls in the sample, then the probability of X=k under the hypergeometric
% distribution is: $P(X=k)=\frac{\left(\begin{array}{c}
% r\\
% k
% \end{array}\right)\left(\begin{array}{c}
% w\\
% n-w
% \end{array}\right)}{\left(\begin{array}{c}
% r+w\\
% n
% \end{array}\right)}$. Then in the case of a contingency table, we can model it as:

% \begin{tabular}{|c|c|}
% \hline
% X & rt1-X\tabularnewline
% \hline
% ct1-X & ct2-rt1+X\tabularnewline
% \hline
% \end{tabular}

% where rt1 and rt2 are row totals and ct1 and ct2 column totals.

% The probability of this table under the hypergeometric distribution
% is:ion is: $P(X)=\frac{\left(\begin{array}{c}
% rt1\\
% X
% \end{array}\right)\left(\begin{array}{c}
% rt2\\
% ct1-X
% \end{array}\right)}{\left(\begin{array}{c}
% rt1+rt2\\
% ct1
% \end{array}\right)}$.

% See \url{https://en.wikipedia.org/wiki/Hypergeometric_distribution}
% \end{comment}


% Fisher's exact test is more accurate than the chi-square test when
% the expected numbers are small, thus it is advisable to use Fisher's
% exact test rather than chi-square test of independence when the total
% sample size is less than say 1000.
% \end{enumerate}

% \FIXME{explain controversy around conditioning on marginal totals; also exact under the right conditions; and can be used also for large N and more than 2x2}


% \section{Permutation test and Fisher's exact test in R}

% To compute the Fisher's exact test in R commander, the only thing
% we have to do is select ``Fisher's test'' instead of ``chi-square''
% under the ``contingency tables'' menu (see step 3a in figure \ref{fig:Chi-square-test-in-1}).
% Please note that, although the original Fisher's exact test was designed
% for 2x2 contingency tables, in R it has been extended to work with
% larger tables such as the one in our example (3x2).

% Unfortunately, the permutation test can not be accessed from R
% commander file menu. However, it is extremely easy to perform this
% test from the command line. You just need to include the argument
% \emph{simulate.p.value = TRUE} in the chi-square function (see figure
% \ref{fig:Heterogeneity,-permutation-and}).

% \begin{figure}[h]
% \caption{\protect\label{fig:Heterogeneity,-permutation-and}Heterogeneity,
% permutation and Fisher's test in R }

% <<permutation test,echo=-1,fig.width=4,fig.height=4>>=
% Hx<-read.table("Hyp_effect.csv",head=T,sep=",")
% with(xtabs(~Effect+HRE),data=Hx)
% contTable<-with(xtabs(~Effect+HRE),data=Hx)
% #Chi-square test
% chisq.test(contTable, correct = FALSE)
% #Permutation test
% chisq.test(contTable, simulate.p.value = TRUE)
% #Fisher's exact test
% fisher.test(contTable)
% @
% \end{figure}



% \FIXME{explain what is the permutation test, and that we can use it for Fisher's too, though then no longer really exact}


\section{Visualizing multivariate categorical data}\label{sec:visualization}

We can use stacked bar plots to visualize multivariate categorical
data. To this end just go to the bar graph under the graph menu (figure
\ref{fig:Stacked-bar-plot}, step 1). Pick a variable to represent,
e.g. ``HRE'' (figure \ref{fig:Stacked-bar-plot}, step 2a), and
then click on plot by groups (step 2b, figure \ref{fig:Stacked-bar-plot})
to select the ``grouping'' variable. Finally, under options select
you can set other aesthetic details (step 4, figure \ref{fig:Stacked-bar-plot}).

\begin{figure}[h!]
\centering{}\includegraphics[width=12cm]{pasted4.png}
\caption{\protect\label{fig:Stacked-bar-plot}Stacked bar plot}
\end{figure}

<<>>=
with(Exercise3,
     Barplot(Outcome, by = Drug, style="divided",
             legend.pos="above", xlab="Outcome",
             ylab="Frequency", label.bars=TRUE))

@

<<>>=
with(Hx,
     Barplot(HRE, by=Effect, style="divided",
             legend.pos="above", xlab="HRE",
             ylab="Frequency",
             label.bars=TRUE))
@

Interestingly, however, the last plot does not seem to indicate the very strong differences we find. Notice this one, and make sure to think about which one makes most sense to you:

<<>>=
with(Hx,
     Barplot(Effect, by=HRE, style="divided",
             legend.pos="above", xlab="Effect",
             ylab="Frequency", label.bars=TRUE))
@


This uses percentage instead of counts, and maybe it tells the story more clearly? You might want to look at the residuals we discussed in section \ref{sec:residuals} and see if you can relate them to the plots. Moreover, what is the size of the difference that is being detected?



<<>>=
with(Hx, Barplot(HRE, by=Effect, style="divided",
                 legend.pos="above", xlab="HRE",
                 ylab="Percent",
                 scale="percent", label.bars=TRUE))
@

Mosaic plots are a very informative two-dimensional representation
of the categorical data. % (see figure \ref{fig:Visualization-of-multivariate})
 Unfortunately it is not implemented in R commander.

% \begin{comment}
% By me: I guess that the Pearson residuals represented in mosaic plots
% by the mosaic function are the difference between the observed and
% expected values. See footnote above.
% \end{comment}

% \begin{figure}[h!]
% <<label="visualizing-multivariate-categorical-data", tidy=TRUE,fig.width=4,fig.height=4,echo=-2,warning=F,error=F,message=F>>=
% library(vcd)
% mosaic(xtabs(~ Effect + HRE, data = Hx), shade = TRUE)
% @
% \caption{\protect\label{fig:Visualization-of-multivariate}Visualization of
% multivariate categorical data}

% \end{figure}



<<>>=
library(vcd)
mosaic(xtabs(~ Effect + HRE, data = Hx), shade = TRUE)
@


Does this help more? What are the values being represented? The residuals are NOT the standardized residuals but, rather, the \(\chi^2\) residuals.

<<>>=
chisq.test(Hx_tab, correct = FALSE)$residuals
@


We can use the standardized residuls with the code below (this is the same procedure as used by \citealp{agresti2018} in p.~41).

<<>>=
hx_res <- chisq.test(Hx_tab, correct = FALSE)$stdres
mosaic(Hx_tab, residuals = hx_res,
       shade = TRUE,
       residuals_type = "Std\nresiduals",
       labeling = labeling_residuals)
@





\citet{friendly2016} is a book-length treatment of visualization of categorical data using R, and their chapter 5 describes function \texttt{vcd}, that we just used. \citet{xu2010a} also discusses some plots and is, by the way, a nice review of categorical
data analysis in experimental biology.

%% FIXME: check Xu.


\section{Categorical data analysis: all we have left out}\label{sec:Other-test-for}


Be aware that we have only scratched the surface of categorical
data analysis. There are many other questions/issues/procedures that can be relevant for categorical data:
\begin{enumerate}

\item Confidence intervals with small-sample procedures (e.g., Fisher's exact 2x2)
\item

  % \item For simple cases in which the categorical variable has only to possible
% levels (``success'' and ``failure'') and when certain conditions
% are met, the sampling distribution of the estimated proportion is
% nearly normal and inference can be done using parametric test much
% like for numerical data. %


% \begin{comment}
% See ``Data Analysis and statistical Inference'' course notes and
% book ``OpenIntro Statistics''

% This test can be applied when: 1) each sample point can result in
% just two possible outcomes (that is when the categorical variable
% has only two levels). 2) there are at least 10 observations in each
% category level. If these conditions are met, then the sampling distribution
% of the estimated proportion is nearly normal and inference can be
% done much like for numerical data.

% \paragraph{For example (from \protect\url{https://www.cliffsnotes.com/study-guides/statistics/univariate-inferential-tests/test-for-comparing-two-proportions}),
% a public health researcher wants to know how two high schools\textemdash one
% in the inner city and one in the suburbs\textemdash differ in the
% percentage of students who smoke. A random survey of students gives
% the following results: }

% \begin{table}[h]
% \caption{example proportion test}

% \begin{tabular}{|c|c|c|}
% \hline
%  & n & smokers\tabularnewline
% \hline
% \hline
% inner city & 125 & 47\tabularnewline
% \hline
% suburban & 153 & 52\tabularnewline
% \hline
% \end{tabular}
% \end{table}


% \paragraph{{[}note that this can be represented as a 2x2 contingency table with
% variable Smoker (yes,no) versus location (inner-city, suburban){]}
% The proportion of smokers in inner-city school is 47/125=0.376 and
% the proportion of smokers in the suburban school is 52/153=0.340.
% Are these proportions significantly different?. {[}the following by
% me{]} To address this question the tests assumes that the distribution
% of sample proportions follows a normal distribution (similar to the
% central limit theorem for sample means) and thus we can compare the
% two proportions using a z-score (equivalent to a t-test). }

% \paragraph{In the case of comparing the proportion observed in a sample with
% an expected value (e.g. the proportion of males in a course with the
% value of 0.5 expected for the general population), we only have to
% compute the sd the population which is given by the formula: $\sqrt{\frac{p_{0}\left(1-p_{0}\right)}{n}}$
% and then compute the z-score (z statistic): $z=\frac{\hat{p}-p_{o}}{\sqrt{\frac{p_{0}\left(1-p_{0}\right)}{n}}}$
% where $\hat{p}$ is the expected proportion in the general population.
% The the probability can be read from a standard normal table (or distribution).
% E.g. if z>1.96 (or z<-1.96), p<0.05 and if z>2.6 (or z<-2.6), p<0.01.
% In the case of the comparison between proportions in two populations
% we will use $z=\frac{p_{1}-p_{2}}{\sqrt{\frac{p_{1}\left(1-p_{1}\right)}{n_{1}}+\frac{p_{2}\left(1-p_{2}\right)}{n_{2}}}}$}
% \end{comment}



\item The McNemar test (or McNemar Chi-square test) is a variant of the
chi-square test used to compare categorical dependent (paired or matched)
samples. For example, the presence of a disease (categorized as ``present'',
``absent'') in a group of individuals before and after taking an
experimental drug to treat it. Disease in each individual is recorded
before and after treatment. Thus data are not independent (paired
data). Another example: Incidence of a disease in individuals exposed
to a drug using their siblings as matched controls. The exposure to
the drug is recorded for the diseased individual and the healthy sibling.
Again the data are not independent (matched data).


% %
% \begin{comment}
% In R it is implemented in the mcnemar.test() function.

% The following is adapted from:

% \url{http://www.biostat.umn.edu/~susant/Fall11ph6414/Section12_Part2.pdf}

% and

% \url{https://en.wikipedia.org/wiki/McNemar's_test}

% The following are contingency tables for the above examples

% \begin{tabular}{|c|c|c|}
% \hline
%  & after:absent & after:present\tabularnewline
% \hline
% \hline
% before:absent & a & b\tabularnewline
% \hline
% before:present & c & d\tabularnewline
% \hline
% \end{tabular}

% \begin{tabular}{|c|c|c|}
% \hline
%  & sibling:non-exposed & sibling:exposed\tabularnewline
% \hline
% \hline
% patient:non-exposed & a & b\tabularnewline
% \hline
% patient:exposed & c & d\tabularnewline
% \hline
% \end{tabular}

% Note that, in the first case, cells contain the number of individuals
% (e.g. ``c'' is the number of individuals with skin rash prior treatment
% that still suffer it after treatment), whereas in the second case
% cells contains pairs of individuals (e.g. ``c'' records the number
% of pairs in which the diseased individual was exposed to the drug
% and the healthy sibling was not exposed). The ``b'' and ``c''
% are called the \cprotect\emph{discordant} cells because they represent
% pairs with a difference, where as ``a'' and ``d'' are \cprotect\emph{concordant}.

% The null hypothesis in the first case is that the proportion of subjects
% positive for the disease is the same before and after treatment is
% the same. In the second case the proportion individuals exposed to
% the drug is the same for healthy and sick individuals. In any case,
% under the null, we would expect b=c. Thus, the McNemar\textquoteright s
% Chi - square statistic is calculated using the counts in the \textquoteleft b\textquoteright{}
% and \textquoteleft c\textquoteright{} cells of the table: $\chi^{2}=\frac{\left(b-c\right)^{2}}{b+c}$.
% If the null hypothesis is true the McNemar Chi - square statistic
% = 0.

% There are alternatives to the McNemar test for paired categorical
% data: binomial sign test, Cochran's Q test, Liddlell's,...
% \end{comment}


\item The Cochran\textendash Mantel\textendash Haenszel chi-square test
computes the probability of the null hypothesis that two nominal variables
are conditionally independent in each stratum of a third variable.

% \begin{comment}
% It can be performed in R using the mantelhaen.test() function. For
% example, in the dataset \textquotedblleft Arthritis\textquotedblright{}
% from the vcd package, the effect of a treatment (placebo vs treated)
% is recorded in the variable \textquotedblleft Improved\textquotedblright{}
% (levels: none, some, marked). They also recorded the gender of patients
% (Sex: male, female), thus we can the hypothesis that Treatment and
% Improved variables are independent within each level of the variable
% Sex. That is, we are testing whether treated individuals improved
% more than those receiving placebos when controlling for sex (see figure
% {[}fig:Cochran-Mantel-Haenszel-test{]}).

% \begin{figure}[h]
% \caption{\protect\label{fig:Cochran-Mantel-Haenszel-test}Cochran-Mantel-Haenszel
% test}

% <<Cochram,tidy=TRUE>>=
% library (vcd)
% head(Arthritis)
% summary(Arthritis)
% mytable<-xtabs(~Treatment+Improved+Sex,data=Arthritis)
% mantelhaen.test(mytable)
% @
% \end{figure}
% \end{comment}

\item Some categorical variables are actually ordinal, not simply nominal. In this case, it is possible to conduct more powerful tests that account for the ordinal nature of the variable. (One simple case is shown in section 8.3 of \citep{Dalgaard2004}; for a lot more detail, see the excellent books by Alan Agresti, \citep{agresti2012} or the more introductory version as \citep{agresti2018}).


% \item A particularly problematic subtype of categorical data are ordinal
%   variables. See \url{http://jeromyanglim.blogspot.com.es/2009/10/analysing-ordinal-variables.html}.%

%   \FIXME{why problematic?} And mention prop.trend.test.



% \begin{comment}
% see \url{https://onlinecourses.science.psu.edu/stat504/book/export/html/90}This
% type of data can be analyzed as nominal variables, but then information
% about order is lost. There are several way to analyze this type of
% data and exploit the order information. One way is to study their
% linear trend or correlation using (variants?) of the Pearson's or
% Spearman's correlation coefficients. Another way is to use the Mantel-Haenszel
% statistics for testing independence between two ordinal variables.
% Mantel-Haenszel (MH) statistic, M2, applies to both the Pearson and
% Spearman correlation. It tests the null hypothesis of independence
% with ordinal variables (i.e., correlation parameter, rho, is equal
% to zero) versus the two-sided alternative (i.e., correlation parameter,
% rho, is not zero). To compute M2 we need to assign scores to both
% rows and columns. Scores are numerical values that we assign to each
% item in each category of our ordinal variables. There are several
% types of scores that can be assigned. The package vcdExtra includes
% the function CMHtest() to compute this test.

% \end{comment}

\end{enumerate}


\section{(Generalized) Linear models for categorical and count data}
\label{sec:glms}

We spent time covering linear models. Generalized linear models extend linear models for responses that are not necessarily numerical, or that can be numerical but are not at all Gaussian. Briefly, generalized linear models allow us to put, on the right of an expression like ``\verb@y ~ x + u + ...@'' \  a \(Y\) that can be a proportion, for example.

A common example is logistic regression, where we model, on the left of the ``~'', the probability of an event. So we can model the probability of response (healed vs.\ non-healed) as a function of a bunch of variables. In the simplest cases, this will reduce to a contingency table, but generalized linear models allow us to model much more complex dependencies. We can have additive effects, interactions, etc. Moreover, they provide estimates of effects and the same principled ways of addressing modeling questions we say for linear models.  Generalized linear models can also be used to model categorical data that fall into more than two categories (multinomial data) or counts (Poisson data). Etc, etc.



\section{Summary}
\begin{enumerate}
\item There are two common questions for categorical data:

\begin{enumerate}
\item does the data fits a given/expected distribution of proportions (goodness
of fit)
\item are two categorical variables independent? (heterogeneity)
\end{enumerate}
\item Chi-square test can used to analyze both goodness of fit and independence,
provided certain restrictions are met.
\item For small sample sizes Fisher's exact test is an alternative to the
chi-square test for independence.
\item Pie and bar graphs are appropriate to represent a single categorical
variable, whereas stacked bar plots and mosaic plots are used to represent
multivariate categorical data.
\end{enumerate}

\section{Exercises}
\begin{enumerate}
\item A common assumption in DNA sequence analysis is that the frequency
of all four nucleotides is the same (0.25\% each). However, the alignment
of several thousand exon-intron boundaries, reveals that the first
base of the intron (5' splice site) tends to be a G. After recording
the number of each base at this position in your alignment, which
test would you apply to determine whether the nucleotide distribution
at this position is significantly different from a random alignment
of DNA fragments?


\item You suspect that the presence of a disease is associated to a certain
variation (represented by the genotype ``-''). To test this hypothesis,
you look at the genotype of group of individuals suffering from the
disease and healthy controls. The results are in the following table.
Compute by hand what would be the expected distribution if the variation
does not affects the presence of the disease. Is the the mutation
associated to the disease? compute the chi-square statistic by hand.

\begin{tabular}{|c|c|c|}
\hline
 & Sick & Healthy\tabularnewline
\hline
\hline
+/+ & 10 & 125\tabularnewline
\hline
+/- & 9 & 60\tabularnewline
\hline
-/- & 6 & 25\tabularnewline
\hline
\end{tabular}


\item You want to know if there is a bias in the gene content for the +
and - DNA strands of mammals. Since the human genome is well characterized,
you decide to use it as a representative sample of the mammalian genomes
to address this question. The file RefSeq\_All\_Chr.csv contains information
about all reference transcripts in the human genome . Import this
table into R commander (note that it is a tab-separated text file)
and process it to investigate if there is evidence for a bias in gene
content. What does the p-value inform about the bias? read references
\citet{Nuzzo2014} and \citet{Halsey2015} about p-values and effect
size.

\end{enumerate}


\section{Recommended reading}
\label{sec:recommended-reading}

Much of what we have covered here is discussed in \citet{Dalgaard2008} and \citet{diez2019}. The first is available through UAM's electronic library, and the second is available in several formats, from their web page.

Beyond that, the first place I always go to for issues related to categorical data analysis is \texttt{agresti2012} and \citep{agresti2018}. \citet{bilder2024} is also a great book. For generalized linear models with R, \citet{fox2018} and \citet{faraway_extending_2016} are very good.


%\bibliographystyle{jss2}
\bibliographystyle{natbib_paren_date_urlbst}

\bibliography{refs}



\section{Appendix: Entering tabular data and doing analyses from here}\label{sec:enter_tabular}

You can enter the tabular data directly in R Commander under ``Statistics \(\rightarrow\) Contingency tables'', and that would directly give you a contingency table and allow you to perform several tests.

Imagine
that you study the association between the presence of a given SNP
and cancer development and, after studying a large number of cases
you get the following results:

\begin{tabular}{|c|c|c|}
\hline
 & Cancer & Healthy\tabularnewline
\hline
\hline
SNP present & 549 & 49788\tabularnewline
\hline
SNP absent & 461 & 51898\tabularnewline
\hline
\end{tabular}

You can enter the tabulated values directly, and do a test. Go to Statistics->Contingency
tables->Enter and analyze... select the appropriate number of rows
and columns and enter the values. Finally select the test(s) you want
to run (see figure \ref{fig:Entering-contingency-tables}).

\begin{figure}[h!]
\centering{}\includegraphics[width=12cm]{pasted9.png}
\caption{\protect\label{fig:Entering-contingency-tables}Entering contingency
tables}
\end{figure}


\end{document}




A bunch of great links, to store properly
https://stats.stackexchange.com/questions/100976/n-1-pearsons-chi-square-in-r/439226#439226


https://stats.stackexchange.com/questions/14226/given-the-power-of-computers-these-days-is-there-ever-a-reason-to-do-a-chi-squa

https://stats.stackexchange.com/questions/159057/alternatives-for-chi-squared-test-for-independence-for-tables-more-than-2-x-2/159063#159063


https://stats.stackexchange.com/questions/275828/applying-monte-carlo-simulation-for-fisher-test

https://stats.stackexchange.com/questions/441139/what-does-the-assumption-of-the-fisher-test-that-the-row-and-column-totals-shou
