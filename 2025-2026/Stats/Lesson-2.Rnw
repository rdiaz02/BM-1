%\VignetteEngine{knitr::knitr}
\documentclass[a4paper,11pt]{article}
\synctex=1
<<echo=FALSE,results='hide',error=FALSE>>=
suppressMessages(require(SparseM, quietly = TRUE)) ## gives warning
require(knitr, quietly = TRUE)
library(patchSynctex)
opts_knit$set(concordance = TRUE)
opts_knit$set(stop_on_error = 2L)
require(car, quietly = TRUE)
options(width = 65)
@

%% Packages needed: knitr, BiocStyle needs to be 1.2.0 or above
<<packages,echo=FALSE,results='hide',message=FALSE>>=
## suppressMessages(require(SparseM, quietly = TRUE)) ## gives warning
suppressMessages(require(BiocStyle, quietly = TRUE))
suppressMessages(library(Rcmdr, quietly = TRUE, warn.conflicts = FALSE))
suppressMessages(library(RcmdrPlugin.HH, quietly = TRUE, warn.conflicts = FALSE))
suppressMessages(library(RcmdrPlugin.KMggplot2, quietly = TRUE, warn.conflicts = FALSE))
suppressMessages(require(ggplot2, quietly = TRUE))


## suppressMessages(require(doBy, quietly = TRUE))
## suppressMessages(require(RcmdrPlugin.plotByGroup, quietly = TRUE))
@

%% not if using BiocStyle
%% \usepackage[authoryear,round,sort]{natbib}
%% \usepackage{hyperref}
%%\usepackage{geometry}
%%\geometry{verbose,a4paper,tmargin=23mm,bmargin=26mm,lmargin=28mm,rmargin=28mm}

\usepackage[margin=10pt,font=small,labelfont=bf,
labelsep=endash]{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{threeparttable}
\usepackage{array}
\usepackage{url}
\usepackage{xcolor}
%\definecolor{light-gray}{gray}{0.72}
%% \newcommand{\cyan}[1]{{\textcolor {cyan} {#1}}}
%% \newcommand{\blu}[1]{{\textcolor {blue} {#1}}}
%% \newcommand{\Burl}[1]{\blu{\url{#1}}}
\newcommand{\red}[1]{{\textcolor {red} {#1}}}
\newcommand{\Burl}[1]{{\textcolor{blue}{\url{#1}}}}
\usepackage[copyright]{ccicons} %% for the CC license icons
%\usepackage{tikz}
%\usetikzlibrary{arrows,shapes,positioning}

\newcommand*{\myhref}[1]{\href{#1}{#1}}
\newcommand*{\qref}[1]{\hyperref[{#1}]{\textit{``\nameref*{#1}'' (section \ref*{#1})}}}


\usepackage{wasysym} %% smileys
\usepackage[latin1]{inputenc}
%\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\newcommand*{\externalf}{{\textcolor{blue}{[See other file:] }}}
\usepackage{gitinfo}

%\usepackage{datetime}
%\newdateformat{mydate}{\THEDAY-\monthname[\THEMONTH]-\THEYEAR}

<<style-knitr, eval=TRUE, echo=FALSE, results="asis">>=
BiocStyle::latex()
@

%% %% Modify margins
\geometry{verbose,a4paper,tmargin=23mm,bmargin=23mm,lmargin=32mm,rmargin=40mm}


\bioctitle[Lesson 2: Comparing two groups (and one group)]{BM-1, Applied
  Statistics, Lesson 2: Comparing two groups (and one group).}

\author{Ramon Diaz-Uriarte\\
  Dept. Biochemistry, Universidad Aut\'onoma de Madrid \\
  Instituto de Investigaciones Biom\'edicas ``Alberto Sols'' (UAM-CSIC)\\
  Madrid, Spain{\footnote{ramon.diaz@iib.uam.es, rdiaz02@gmail.com}} \\
%% {\footnote{rdiaz02@gmail.com}} \\
{\small \Burl{http://ligarto.org/rdiaz}} \\
 }


%\title{Lesson 2. Comparing two groups (and one group)}

%% \author{Ramon Diaz-Uriarte\\
%% Dept. Biochemistry, Universidad Aut\'onoma de Madrid \\
%% Instituto de Investigaciones Biom\'edicas ``Alberto Sols'' (UAM-CSIC)\\
%% Madrid, Spain\\
%% {\small \texttt{ramon.diaz@iib.uam.es}} \\
%% {\small \texttt{rdiaz02@gmail.com}} \\
%% {\small \Burl{http://ligarto.org/rdiaz}} \\
%% }


\date{\gitAuthorDate\ {\footnotesize (Rev: \gitAbbrevHash)}}
\begin{document}

\maketitle

\tableofcontents

\clearpage

\section*{Warning: eternally provisional}
This file can get changed often. When asking questions in class or in the
forum, refer to the section numbers AND section names, which change less
than the page numbers.



\section*{License and copyright}\label{license}
This work is Copyright, \copyright, 2014-2025 Ramon Diaz-Uriarte, and is licensed under a \textbf{Creative Commons
} Attribution-ShareAlike 4.0 International License:
\Burl{http://creativecommons.org/licenses/by-sa/4.0/}.

\centerline \ccbysa



All the original files for the document are available (again, under a
Creative Commons license) from
\Burl{https://github.com/rdiaz02/BM-1}. (Note that in the github repo you
will not see the PDF, or R files, nor many of the data files, since those
are derived from the Rnw file).


\clearpage

\section{Introduction}




Someone in your lab has measured the expression of several genes from a
set of patients with and without cancer. Since you are the only one who
took BM-2, you are in charge of looking at the data and answering the
question ``Does the expression of the genes differ between patients with
and without cancer?''.


\subsection{Files we will use}


\begin{itemize}
\item This one
\item \Robject{P53.txt}
\item \Robject{MYC.txt}
\item \Robject{BRCA2.txt}
%%  \item ACRB.txt
\end{itemize}






<<create_p53, echo=FALSE, results='hide'>>=
set.seed(1)
dp53 <- data.frame(p53 = round(rnorm(23, c(rep(2, 13), rep(2.8, 10))), 3),
                   pten = round(c(rlnorm(13, 1), rlnorm(10, 1.35)), 3),
                   brca1 = round(rnorm(23, c(rep(2, 13), rep(5.8, 10))), 3),
                   brca2 = round(c(rep(c(1, 2, 3), length.out = 13),
                       rep(c(2, 3, 4), length.out = 10))),
                   cond = rep(c("Cancer", "NC"), c(13, 10)),
                   id = replicate(23, paste(sample(letters, 10), collapse = "")))
write.table(dp53, file = "P53.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(list = ls())
@


%% <<tests_p53_myc, echo=FALSE, results="hide">>=
%% t.test(p53 ~ cond, data = dp53)
%% t.test(pten ~ cond, data = dp53) ## we catch it
%% t.test(log(pten) ~ cond, data = dp53) ## we don't
%% t.test(log(p53) ~ cond, data = dp53) ## no effect: we don't see it
%% @


\section{Types of data}
We need to get this out of the way, as we will refer to it
frequently. Data can be measured in different scales. From ``less
information to more information'' we can organize scales this way:

\begin{description}
\item[Nominal or categorical scale] We use a scale that simply
  differentiates different classes. For instance we can classify some
  objects around here, ``computer'', ``blackboard'', ``pencil'', and we
  can give numbers to them (1 to computer, 2 to blackboard, etc) but the
  numbers have no meaning per se.

  \textbf{Binary} data are in a nominal scale with only two classes: dead
  or alive (and we can give a 0 or a 1 to either), male or female, etc.


  Lots of biological data are in a nominal scale. For instance, suppose
  you look at the types of repetitive elements in the genome, and give a 1
  to SINEs, a 2 to LINEs, etc. Or you number the aminoacids from 1
  (alanine) to 20 (valine). You can of course count how many are of type 1
  (how many are alanines), etc, but it would make no sense to do averages
  and say ``your average AA composition is 13.5''.



\item[Ordinal scale] The data can be ordered in the sense that you can say
  that something is larger or smaller than something else. For instance,
  you can rank your preference for food as: ``chocolate > jamon serrano >
  toasted crickets > liver''. You might assign the value 1 to chocolate (your most
  preferred food) and a 4 to liver (the least preferred) but differences
  or ratios between those numbers have no meaning.

  Some measurements in biology are of these kind. Name a few?


\item[Interval or ratio scale] You can take differences and ratios, and
  they do have meaning\footnote{Some authors make a distinction between
    ratio and interval scales; I won't. The only difference is that ratio
    scales have a natural zero.}. If a subject has a value of 6 for the
  expression of gene PTEN, another a value of 3, and another a value of 1,
  then the first has six times more RNA of PTEN than the last, and two
  times more than the second.

\end{description}


We will try to be careful. With nominal data we will always try to keep
them as ``things without numbers'', so that we make no mistakes (i.e.,
keep the aminoacids names, not just a bunch of 1 to 20). Ordinal scale are
trickier, and we will need to think about the type of data and the
analyses.





\section{Looking at the data: plots}

We first need to import the data (from ``Data'', ``Import data'', ``from
text file, clipboard, or URL, \ldots''). Make sure you name it sensibly;
for instance, dp53. When you do that, you will see something similar to
(not identical! because I did it suppressing unneeded stuff):

<<>>=
dp53 <- read.table("P53.txt", header = TRUE, stringsAsFactors = TRUE)
@


Notice the \texttt{stringsAsFactors = TRUE}: we want the strings to be
turned into factors, so we ask for it.

The first step ever is to look at the data. In fact, here we can look at
all the original data. So go take a look at the data (``View data set'',
there is a button next to ``dp53''). Resist the temptation to modify it
there. \red{Why?}


\subsection{Plots to do}
For all except the trivially tiniest datasets we want to use graphics.
Make sure you do the following plots (in the menu, under ``Graphs''):
\begin{itemize}
\item Histogram for each gene, using condition (``cond'') as the conditioning or grouping
  variable (``Plot by:'').
\item Boxplot, using condition (``cond'') as the conditioning or grouping
  variable (``Plot by:'').
\item Plot of means (and make sure you get nicer axes labels).
\item Stripchart, and make sure you use ``jitter'', not ``stack'': \red{can
  you tell for which one of the variables this matters a lot?}
\item Density plots (``Density estimates'')
\end{itemize}


To get you going, I show all those for p53.
<<out.width='11cm',out.height='11cm'>>=
with(dp53, Hist(p53, groups=cond, scale="frequency", breaks="Sturges",
                col="darkgray"))
@

<<fig.height=10, fig.width=8,results='hide'>>=
op <- par(mfrow = c(2, 2)) ## to show 2 by 2 on the same figure
Boxplot(p53~cond, data=dp53, id = list(method="y"))
plotMeans(dp53$p53, dp53$cond, error.bars="se",
  xlab="Condition", ylab="P53 expression levels")
stripchart(p53 ~ cond, vertical=TRUE, method="jitter",
  ylab="p53", data=dp53)
densityPlot(p53~cond, data=dp53, adjust=1)
par(op)
@


% %% If loaded silently, as above, will not get in the menuse. So do not do
% %% that for interactive usage
% Now load the ``plotByGroup'' plugin \footnote{You don't have it installed?
%   Install it, then. You should know how to do it by now. The name is
%   ``RcmdrPlugin.plotByGroup''} and you'll find a new menu entry, under
% ``Graphs'', that says ``Plot by group''. Use

Let us now do a couple of plots by group. (For the first two, you'll need to use the command line, since a former plugin, ``RcmdrPlugin.plotByGroup'', is no longer available):



% \begin{itemize}
% \item histogram by group
% \item boxplot by group
% \end{itemize}


<<out.width='10cm',out.height='9cm'>>=
## First, load the lattice package
## Functions histogram and bwplot are from that package
library(lattice)
histogram(~p53 | cond, data=dp53)
@


% You could also use the usual \texttt{hist} function, and overlay them:

% <<histover,out.width='10cm', out.hight='9cm'>>=
% hist(p53[cond == "Cancer"], col = rgb(1,0,0,0.5), data = dp53)
% hist(p53[cond == "NC"], col = rgb(0,0,1, 0.5), add = TRUE)
% @

%% Next: replace by Boxplot
<<out.width='10cm',out.height='9cm'>>=
bwplot(~p53 | cond, data=dp53)
@


The last one we can also obtain from the \textbf{lattice plot ... (HH)} after loading the ``RcmdrPlugin.HH'':



<<>>=
bwplot(p53 ~ cond, data= dp53, type="p",
       par.settings=simpleTheme(pch=16),
       auto.key=list(border=TRUE),
       scales=list(x=list(relation='same', log=FALSE),
                   y=list(relation='same', log=FALSE)))
@


And a violin + boxplot showing the points themselves, using package RcmdrPlugin.KMggplot2 (and we rotate it ... just because we can):

<<fig.show = 'hold'>>=
.df <- data.frame(x = dp53$cond, y = dp53$p53, z = dp53$cond)
.plot <- ggplot(data = .df,
                aes(x = factor(x), y = y, fill = z)) +
  geom_violin(position = position_dodge(width = 0.9)) +
  geom_boxplot(width =  0.2) +
  geom_jitter(colour = "black",
              position =
                position_jitterdodge(jitter.width = 0.25,
                                     jitter.height = 0,
                                     dodge.width = 0.9)) +
  coord_flip() +
  xlab("cond") +
  ylab("p53") +
  labs(fill = "cond") +
  theme_bw(base_size = 14, base_family = "sans")
print(.plot)
rm(.df, .plot)
@



\subsection{What are the plots telling you?}
Now, think about what these plots tell you:
\begin{enumerate}
\item Is the ``boxplot by group'' more or less useful than
  the built-in one?
\item When was the stripchart specially useful? Could you see anything
  strange for brca2 without the stripchart?
\item Eye balling the plots, what variables do you think show differences
  between the two conditions? Wait! Think about at least:
  \begin{enumerate}
  \item Differences in mean/median
  \item Differences in dispersion (variance, IQR, etc)
  \end{enumerate}
\item Similar question again: what genes look like they have differential
  expression between the cancer and non-cancer patients?

\item Are density plots reasonable in these cases?

\end{enumerate}



\subsection{Multiple plots per page again}
If we access R directly we can do something like:

<<fig.width=7, fig.height=5, results='hide', fig.show = 'hold'>>=
op <- par(mfrow=c(1,2))
Boxplot(p53~cond, data=dp53, id.method="y")
plotMeans(dp53$p53, dp53$cond, error.bars="se", xlab="Condition",
          ylab="Mean of P53")
par(op)
@

Note that all I really did was add the \verb@par(mfrow=c(1,2))@ and then
copy the appropriate lines provided by the ``R Script'' window. And I clicked
``Submit''. The \verb@op <- par(mfrow=c(1,2))@ and \verb@par(op)@ are a
minor detail to restore options as they were.%% (It is simpler if you copy, then clear, then paste and modify).

There are two ideas in this section: a) that sometimes we want to present figures
side by side to enhance understanding; b) how to do this in R.


\clearpage
\section{Saving the things we do}

(This material is completely optional, and it has to do with how we can do
certain things in R, produce reports, etc, so we will not spend any time on it. It is here so that you can use it for your own lab reports.)


\subsection{The R script}
Notice we have a complete R script: the whole transcript of all we did.

\begin{itemize}
\item Can you save it?
\item Can you open it and reuse it?
\end{itemize}


Do not underestimate the power of this feature: you have a record of ALL
you did (unless you messed around with ``Edit data set''). This is a
record of your workflow. We want research to be reproducible: this helps
us achieve it (and it is a simple version of what scientific workflows
provide).


\subsection{An HTML report}

This is all very nice, but you might want to intersperse your figures with
comments and notes and have something to hand to your colleagues. Let's
give them something more useful. Go the the ``R Markdown'' tab and
generate the HTML report. That is an HTML file you can put on a web page,
send around, etc.

Edit the Markdown file and:

\begin{itemize}
\item Get rid of useless plots.
\item Add your name and a meaningful title.
  \item Add some reasonable text near the BRCA2 figure. E.g., ``This looks
    weird''.
\end{itemize}


And note that you have, in here, also a record of all of your
commands. But the proper file to carefully save is the R script. \red{Why?}

%% the R Markdown file gets updated whenever you submit something from R
%% script.


\subsection{Saving the figures ``for real''}
The HTML report does not have high-quality figures for publication. You
can obtain those from ``Save graph to file''.



\clearpage
\section{Comparing two groups with a t-test}

Let's start with p53. Compare the mean between the two groups
(``Independent samples t-test''). You will see something like (not
identical, again, because I called it directly using default options)


<<echo=FALSE, results='hide'>>=
options(width = 65) ## so that we do not run over the margin
@

<<>>=
t.test(p53 ~ cond, data = dp53)
@


\begin{itemize}
\item What is this test for?
\item Do you remember what the formula for the t-statistic look like? And
  why should this matter?
\item What are the options given?
  \begin{itemize}
  \item Equal variances? Does it make a difference? Any simple hint of
    whether you are using Welch's test or the equal-variances one?
  \item Alternative hypothesis? One-tailed vs. two-tailed.
  \end{itemize}
\item Do results agree with the figures? What figures?
\item Oh, in fact: can we interpret the results?
\item The output gives a confidence interval: what is that?
\end{itemize}

(We will spend time in class making sure all this is understood if you
have forgotten your stats classes).

Repeat the above with brca1.



\subsection{Ideas that should be clear from the t-test part}
A few ideas that should be clear after this section:

\begin{enumerate}
\item The difference between a sample and a population
\item That (most of the time) we use samples to make inferences about populations
\item What a statistic is; estimators (for example, the sample mean is an
  estimator) are a type of statistic \footnote{Briefly: a statistic is
    number that can be computed from a sample. An estimator is a function
    used to calculate, or estimate, a quantity of a probability
    distribution. The sample mean, with the function $\sum x/N$, is a
    function of the data and is used to estimate the true mean of a
    distribution using the data from a given sample.}
\item What a t-statistic is (a t-statistic is a test statistic used in
  hypothesis testing; a test statistic is a statistic used in hypothesis
  testing).
\item That statistics (and, thus, estimators) have distributions
\item The difference between standard deviation and standard error
\item That sampling introduces variability
\item That some procedures ``ask more from the data'' (e.g., interval data)
\item p-values
\item Null hypothesis
\item Distribution of the statistic under the null hypothesis
\item The logic behind a statistical test
\item The difference between estimation and hypothesis testing
\end{enumerate}


If any one of the above is not clear, please ask it in class. Most of this,
though, is material you have surely been taught before  \smiley{} .  So if any of this is unclear, please ask in class after having looked a the notes from your previous stats classes and, maybe, the Wikipedia.

\subsubsection{What p-values and hypothesis testing are and are not}
\label{sec:what-p-values}

These ideas should be clear:
\begin{itemize}
\item We do not ``confirm the null hypothesis'': we fail to reject (and failing
  to reject can be something trivial to achieve)
\item The p-value is not the probability of the null hypothesis
\item The p-value is not the probability of the alternative hypothesis
\item The p-value can be used as a measure of strength of evidence
  \textbf{against} the null hypothesis. Remember the logic ``either the null is
  false or something as (un)likely as the p-value happened''
\item p-values are computed using models that make some assumptions
\item Using p-values is often a much more sensible idea than saying ``signficant''
\item Hair-splitting over, say, $p = 10^{-13}$ and $p = 10^{-16}$ makes no sense
\item p-values are not the only tool we can use: do not forget confidence intervals!!
\end{itemize}


\subsection{Confidence intervals}

If this is not obvious to you, ask it in class: a figure that shows an
estimate (e.g., a mean) and a 95\% confidence interval, where the interval
goes from, say, 1 to 2, \textbf{should not} be interpreted as saying that
there is a 95\% probability that the mean is between 1 and 2. That is not
the correct interpretation of a confidence interval. Make sure you
understand this!!!


In class we will very briefly cover how the confidence interval is constructed. In this case, it involves kind of turning around the logic of the p-value.

\externalf Please look at file \myhref{Lesson\_2\_Confidence\_intervals\_with\_Rcmdr.pdf}.



\subsection{Assumptions of the t-test}

One key assumption is \textbf{independence} of the data. This is the case
for the t-test but is also the case for many statistical tests. We return
to this below several times because it is a \textbf{crucial} assumption
(e.g., section \ref{pseudorep} and also Lesson 3). Lack of independence is
a serious and pervasive problem (and one form of non-independence is also
referred to as ``pseudoreplication'' \footnote{A major paper, a long while
  ago, in the journal \textit{Ecological Monographs} by Hurlbert dealt
  with this and made ``pseudoreplication'' a well known term.}).


When comparing two means, \textbf{equality of variances} is also
important. But detecting differences in variances is complicated. Two
practical solutions are: Welch's test (the default in R) and using
transformations. However, think about
the meaning of a comparison of means when variances are hugely different:
do you really want to do that?


What about normality? It matters, but not as much, especially as sample
size gets larger. Deviations from normality because of skewness
(asymmetry) can have a large effect. Deviations because of kurtosis (tails
heavier than the normal) have very minor effects. That is why we often say
``data are sufficiently close to normality''. And in the ``sufficiently
close'' we worry mainly about asymmetries. And things tend to get
``sufficiently close'' as sample size grows larger (a consequence of the
central limit theorem); how large is large enough? Oh, it depends on how
far the distribution of the data is from the normal distribution, but
often 10 is large enough, and 50 is generally well large enough (but in
certain cases 100 might not be even close to large enough). (And if you want to
play with the central limit theorem, there is a very nice demo available from R
commander; if we have time, we will play with it. You load it from the plugins,
it is the teaching demos, and once you load it, it will be available as ``central
limit theorem'').






Of course, although this should be obvious: when we talk about symmetry, normality, etc, we are talking about the data distribution of \textbf{each group separately}. Please, read this carefully: \textbf{each group separately}. For example, \textbf{it makes no sense to check if the distribution of the complete set of data, of the two groups together, is normally distributed}.  This should be obvious: if it is not, draw two normal distributions (with different means) by hand, with different means and think about what happens if you put together samples from each one of them and look at the distribution. At any rate, testing for normality is not sensible most of the time ---we return to this below. When we deal with linear models we will examine distribution assumptions, but looking at the residuals.



Finally, \textbf{outliers} can be a serious concern (by the way, outliers, or potential outliers ---under some definition of outlier--- get flagged by function \Rfunction{Boxplots} in R). In general, points very far from the rest of the points will have severe effects on the value computed for the mean (not the median ---this is also related to why nonparametric procedures can be more robust). What to do, however, is not obvious. An outlier might be the consequence of an error in data recording. But it might also be a perfectly valid point and it might actually be the ``interesting stuff''. Sometimes people carry out (and, of course, \textbf{should explicitly report}) analysis with and without the outlier; sometimes the same qualitative conclusions are reached, sometimes not. Please, think carefully about what an outlier is before proceeding with your analysis but do not get into the habit of automatically getting rid of potential outliers. And whatever you do, you should report it.


The book by Rupert Miller ``Beyond Anova'' (Chapman and Hall, 1997)
contains a great discussion of assumptions, consequences of deviations
from them, what to do, etc.


\section{One vs two-tailed (or two-sided) tests}

When comparing means between two groups, A and B, we can think of three scenarios:

\begin{enumerate}
\item We are asking if the means are different. \(H_0: \mu_A = \mu_B\).
\item We are asking if the mean of A is larger than the mean of B. \(H_0: \mu_A \le \mu_B\).
\item We are asking if the mean of A is smaller than the mean of A. \(H_0: \mu_A \ge \mu_B\).
\end{enumerate}


The first scenario, the most common one, is called a two-tailed test. We look (and sum) both tails of the distribution. Deviations from \(H_0\) in both directions (A being larger than B, and A being smaller than B) count against the null.

Now, suppose you find \(\bar{X}_A > \bar{X}_B\) (i.e., the sample mean of group A is larger than the sample mean of group B). You do a two-tailed t-test and find a t-statistic \(t_1\) with a p-value of 0.06. This means that the area under the curve that is larger than \(t_1\) is 0.03, and the area under the curve that is smaller than \(-t_1\) is 0.03. \red{Make sure you understand this!!! Draw a picture of the t-distribution right now and place, in that figure, \(t_1\)! }


(In a culture of p-value worshiping) it could be tempting to reason as follows: ``I got a barely significant p-value. Hummmm... But ... hey, let us do this. I will write that my \(H_0\) is \(\mu_A \le \mu_B\). The p-value will then be 0.03''. Well, this is both \textbf{a technical mistake and a case of scientific misconduct}: you have changed your original question just to achieve a smaller p-value. That renders the p-value meaningless.

Now, what about this case? My true, original \(H_0: \mu_A \ge \mu_B\). The p-value will be \(1 - 0.03 = 0.97\). And you can think: ``Had I done a two-tailed test, I'd have a p-value of 0.06. Now I have one of 0.97!''.


The following code shows the above issues (with very similar p-values to the ones of the example):
<<one-two>>=
set.seed(2)
xA <- rnorm(20, 1)
xB <- rnorm(20, 0.5)

mean(xA)
mean(xB)

## Two-tailed or two-sided
t.test(xA, xB, alternative = "two.sided")

## One-tailed: Ho: mu_A <= mu_B
t.test(xA, xB, alternative = "greater")

## One-tailed: Ho: mu_A >= mu_B
t.test(xA, xB, alternative = "less")

## Note the p-value: 1 - 0.0286
## or 1 - (0.0572/2)
@



This emphasizes:
\begin{itemize}
\item One vs. two-tailed tests are testing different scientific hypothesis. Use the one that is relevant for your scientific question.
\item If your null hypothesis is that  \(H_0: \mu_A \ge \mu_B\) this is what you are saying: very large differences, where \(\bar{X}_A > \bar{X}_B\) are not scientifically relevant. (In other words, the ``Had I done a two-tailed test, I'd have a p-value of 0.06. Now I have one of 0.97!'' should not be seen as a lament or regret; this is just fine, as the difference in means is in the direction that your \(H_0\) predict.)
\end{itemize}

In most cases, when comparing means, the two-sided test is often the scientifically most relevant. But not always. (But in many other cases, the tests are naturally one-sided; for example, in ANOVA, the F-ratio is a one-sided test ---though the null being tested regarding means is one of mean equality: remember to ask this in Lesson 3).





We will say more about ideas related to the sides of tests in section \qref{bioequiv}.


\section{Power of a test}

If there is a true difference in means we would like to detect it. Power refers to our ability to reject the null when it is false. This figure might help; rows refer to the true state of the Universe and columns to your decision.


\begin{table}[h!]
\begin{tabular}{p{6.5cm}|>{\centering}p{2.5cm}|>{\centering}p{2.5cm}|}
  \multicolumn{1}{c}{} & \multicolumn{1}{>{\centering}p{2.5cm}}{Null hypothesis not rejected} &
  \multicolumn{1}{>{\centering}p{2.5cm}}{Null hypothesis rejected}\tabularnewline
  \cline{2-3}
  Means do not differ ($H_0$ \textit{is really true}) & Correct & Type I error\tabularnewline
  \cline{2-3}
  Means differ ($H_0$ \textit{is really false}) & Type II error & Correct\tabularnewline
  \cline{2-3}
\end{tabular}
\end{table}
Power is $1 - Type\ II\ error$. Power is the probability of rejecting the null hypothesis when the null hypothesis is false.



How likely we are to detect a difference that really exists (power)
depends on:

\begin{itemize}
\item The threshold we use for saying ``the means differ'' ($\alpha$ level, or
  Type I error)\footnote{There is, actually, a difference between p-value and $\alpha$ level; the p-value is a function of the data, is something you compute with a given procedure for a given data set; the $\alpha$ level or Type I error rate is a property of the procedure.}.
\item The sample size.
\item The size of the effect (difference in means).
\item The standard deviation.
\end{itemize}





Go play with the ``Teaching demos: Power of a test'' (from the
\CRANpkg{RcmdrPlugin.TeachingDemos}).


\begin{figure}[h!]
 \begin{center}
 \includegraphics[width=0.80\paperwidth,keepaspectratio]{power.png}
 \caption{\label{power} Playing with power, from the
   \CRANpkg{RcmdrPlugin.TeachingDemos}, Power of the test.}
 \end{center}
 \end{figure}



 There are simple, standard ways of figuring out the power. But they
 require you to specify the above values. Not always known (or easy to
 guess). The \CRANpkg{RcmdrPlugin.IPSUR} package provides some extra tools.


\begin{figure}[h!]
 \begin{center}
 \includegraphics[width=0.40\paperwidth,keepaspectratio]{ipsur-power.png}
 \caption{\label{ipsurpower} Entry window for \CRANpkg{RmcdrPlugin.IPSUR}:
   Power for t-test, under ``Statistics'', ``Power (IPSUR)''.}
 \end{center}
 \end{figure}




Power can be computed before hand to:

\begin{itemize}
\item Know if we are likely to find a difference if there is one (given
  our sample size and estimated effect sizes and standard deviations).
\item Figure out if our sample size is OK given the desired power (and
  estimated effect sizes and standard deviations).
\end{itemize}


Please note: \textbf{it makes little sense} to compute the power of a test
after the fact. It tells you nothing valuable\footnote{The paper by Hoenig
and Heisey, ``The abuse of power: the pervasive fallacy of power
calculations for data analysis'', \textit{The American Statistician},
2001, 55: 19--24, explains this in more detail.}



(A slightly more technical note: as is common in many stats intro texts, we have
been mixing ideas that derive from Fisher's null hypothesis testing with ideas
from Neyman and Pearson's alternative hypothesis and acceptance-rejection
framework; we will not get further into this).

\subsection{An analogy: should I take the umbrella?}

If the trade-off between Type I and Type II errors is not clear, remember
  the analogy we used in class with respect to the umbrella (well, the behavior
  of taking or not the umbrella, as a function of meteorological predictions
  ---withe the \textbf{major} caveat that a p-value is \textbf{not} the
  probability of the null, whereas meteorological predictions are often the
  probability of, say, rain).

\subsection{The winner's curse}

When studies have low power, estimates of effects from tests that yield
  ``significant'' results are biased up (i.e., they are larger than they should
  be). In other words, for a given particular phenomenon, with low power studies,
  if you focus only on those papers with significant p-values, the effect
  estimates are too large (in absolute sense). So publication bias together with
  low power yields overestimates of effect sizes. If this is not clear, we will
  explain it in class.

Here is a interesting and recent example where the winner's curse can be
  playing a role: the possible use of botox to relieve depression   \Burl{https://www.science.org/news/2021/06/can-botox-ease-depression-eliminating-frowns-researchers-have-doubts}.

  The winner's curse is a \textbf{serious problem} (related to a bunch of others,
  such as reporting/publication bias and the reproducibility crisis.).


  Oh, and the winner's curse is something that \textbf{can affect you too}; in other words, the winner's curse is not something that only happens to other researchers \smiley{}.


%% \section{Data transformations}\label{transform}

%% (Look at this section briefly if we do not have a lot of time. Many things
%% will be clearer after we play with transformations in Lesson 3.)

%% Let us look at the boxplot for ``pten''. What does it look like? Take the
%% log transformation: create a new variable. Yes, find this out; it is
%% under ``Data'', ``Manage variables'', ``Compute new variable'' and you
%% enter ``log(pten)'' as the ``Expression to compute''; take a look at
%% Figure \ref{ucla} for an example; or you can be much faster and do it by
%% typing code directly in the R Script or the RStudio console:

%% <<>>=
%% dp53$logpten <- log(dp53$pten)
%% @


%% Now, plot the log of pten. And do t-tests of the original and
%% log-transformed variable.

%% <<results='hide', fig.show='hold', fig.width=5, fig.height=5>>=
%% op <- par(mfrow = c(1, 2))
%% Boxplot(pten~cond, data=dp53, id.method="y")
%% Boxplot(logpten~cond, data=dp53, id.method="y")
%% par(op)
%% @


%% Do we expect multiplicative or additive effects? (With a two-group
%% comparisons it isn't always easy to think this, but can be crucial in
%% regression; we will see it in Lesson 3).




%% <<>>=
%% t.test(pten ~ cond, data = dp53)
%% t.test(logpten ~ cond, data = dp53)
%% @


\section{(Bio)equivalence testing and turning things around}\label{bioequiv}
We have set things up so that we \textbf{need strong enough evidence to reject the null} and we \textbf{use p-values of measures of strength of evidence AGAINST the null}. This is often what we want in science (why? think about how many papers with claims that are trivially generated under the null you want to read). But not always. And in many cases, in particular in issues related to public health, we might want to follow a precautionary principle (\Burl{https://en.wikipedia.org/wiki/Precautionary_principle}).

For example, we might want to say ``We will only allow you to dump chlorine in the river if there is strong enough evidence that such action will not cause harm, for instance, will not increase fish mortality''. This is not something you can settle with p-values as we have used them. \red{Why??}

What can we do? We want to turn the process around. You would want a procedure for answering the following question: ``Is there strong enough evidence that, if chlorine has an effect, the effect is no larger than an increase in fish mortality of 1\%?''. This is like inverting the burden of proof: it is as if now we want evidence in favor of a hypothesis that says that things do not differ by more than a given, small, value (i.e., it seems we now want evidence in favor of what is often the null). In other words, we want strong evidence that the true value is inside the equivalence bounds, the bounds that say that ``things are similar, or equivalent'' (we have simplified things here, by just worrying about increases in fish mortality, but often we worry about deviations both up and down).

We can approach this problem as one of finding evidence against the (new null) hypothesis that things differ by more of the specified tolerance, in our case that 1\% increase in fish mortality; in other words, that the true value falls outside the equivalence bounds. If we can reject our new null that the groups differ by more than a given threshold (that the true difference falls outside the equivalence bounds), we have established that they are equivalent.  In some cases it is relatively straightforward to do this (such as with the TOST procedure), in many others it is not.

We will not pursue this any further. But, please, think of other examples of similar cases. Examples involving, for example, human disease and certain environmental changes, or secondary effects of drugs, or that two drugs (say, brand and generic) are for all practical purposes identical.

Oh, and, of course, this section emphasizes another idea: a large p-value is not evidence in favor of the null!!!




Figure \ref{fig-equiv}, from Lakens et al. 2018\footnote{Lakens et al.(2018). Equivalence Testing for Psychological Research: A Tutorial. \textit{Advances in Methods and Practices in Psychological Science}, 1(2): 259--269. \Burl{https://doi.org/10.1177/2515245918770963}}, shows equivalence testing alongside the usual null hypothesis significance testing, as well as other tests. Which of the four cases represented corresponds to the chlorine example in the notes? What case would represent examining if the effects of the brand name drug and a generic are similar?


\begin{figure}[h!]
 \begin{center}
\includegraphics[width=0.80\paperwidth,keepaspectratio]{bioequiv-lakens-2018.png}
 \caption{\label{fig-equiv} Equivalence testing, from Lakens et al 2018. Figure 1 in Lakens et al.(2018). Equivalence Testing for Psychological Research: A Tutorial. \textit{Advances in Methods and Practices in Psychological Science}, 1(2): 259--269. \Burl{https://doi.org/10.1177/2515245918770963}.
}
 \end{center}
 \end{figure}








\section{Bayesian inference}

We will quickly review the following issues (please, make sure to look at your intro probability/stats notes):
\begin{itemize}
\item What is Bayes rule (or Bayes theorem)
\item Using Bayes rule to obtain the posterior probability of a hypothesis given data. Why this is a very attractive idea.
\item The problems of trying to do that: where is the prior for the hypothesis coming from?
\item Why we will not spend more time on this.
\item That Bayes rule is \textbf{NOT} controversial at all: it is \textbf{the procedure to use} in many cases, for example diagnostic testing (probability of having disease X, given that you got a positive test result). But using Bayes rule can be controversial or complicated for the cases we discuss here.
\item That most of the p-values you see in the literature are, in fact, not ``Bayesian p-values'' but rather use frequentist statistics, so you must know how to use and interpret them.
\item (And that we do not have conceptual or philosophical problems with at least many Bayesian approaches, and use them when we think appropriate; it is just that for this class we do not deem it appropriate to go in that direction).
\end{itemize}



\section{Confidence intervals and p-values: a longer discussion}

\externalf We now go over the file \myhref{confidence-intervals-p-values-interpretation.pdf}. %% \texttt{confidence-intervals-p-values-interpretation.pdf}.


% Make sure you remember to look at the notes you took for the file
%   \texttt{confidence-intervals-p-values-interpretation.pdf}.



\clearpage

\section{Paired tests}\label{pairedt}

Load now the data set called \Robject{MYC.txt}. As before, give it a
reasonable name.


<<create_myc, echo=FALSE, results='hide'>>=
set.seed(15)
s <- rnorm(12, 4, 25)
s <- c(s, s)
cond <- rep(c(0, .5), c(12, 12))
y <- rnorm(24) + s + cond
y <- y - min(y) + 0.3
id <- replicate(12, paste(sample(letters, 10), collapse = ""))
id <- c(id, id)
dmyc <- data.frame(myc = round(y, 3),
                   cond = rep(c("Cancer", "NC"), c(12, 12)),
                   id = id)

## t.test(myc ~ cond, data = dmyc)
## ## should this work?? It does but from the help it ain't obvious to me
## t.test(myc ~ cond, paired = TRUE, data = dmyc)
## t.test(myc ~ cond, data = dmyc)
## t.test(myc ~ cond, paired = TRUE, data = dmyc)
## t.test(dmyc$myc[1:12] - dmyc$myc[13:24])
## summary(lm(myc ~ id + cond, data = dmyc))


write.table(dmyc, file = "MYC.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
@

<<>>=
dmyc <- read.table("MYC.txt", header = TRUE, stringsAsFactors = TRUE)
@



What is the active data set now?


Look at the data. Anything interesting?


There are actually two observations per subject, one from tumor tissue and
one from non-tumor tissue. This is a classical setting that demands a
paired t-test. \red{Why?} When answering this question think about the
idea of using each subject as its own control.


How can we check the above? (Why is this doing what I want?)

<<>>=
all(with(dmyc, table(id, cond)) == 1)
@




\subsection{Reshaping the data for a paired t-test}

The purpose of this section is to show you several different ways of reshaping data, but we will skip the details in class, unless you run intro problems. In class we will definitely cover section \ref{datashape}, \ref{reshaping} and read section \ref{reshapefinal}. The rest is left here so that you can refer to it when you need to do it with your own data, but, again, we will \textbf{not} cover it in class: you need to understand the source of the problem but the syntax for the solutions is not important (you have it here, and can refer to it when you need it).

\subsubsection{The shape of the data}\label{datashape} Often, when you know you are only going to do a paired test you can organize your data in a table structure like the one in Table \ref{unstacked}:

\begin{table}[h!]
\begin{tabular}{ccc}
  SubjectID&Tumor&Non-Tumor\\
  \hline
  pepe&23&45\\
  maria&29&56\\
  \ldots&\ldots&\ldots\\
\end{tabular}
\caption{Paired data in a ``unstacked'' or ``wide'' shape/format.}\label{unstacked}
\end{table}


That looks ok, but is really a very limiting format for the data. Think
about what you would do if you had additional ``tumor'' and ``non-tumor''
data per subject, or you had additional covariates for each measure,
etc. That is why we often want to have the data in a ``stacked'' format as
in Table \ref{stacked}:


\begin{table}[h!]
\begin{tabular}{ccc}
  SubjectID&Myc&Condition\\
  \hline
  pepe&23&tumor\\
  pepe&45&nontumor\\
  maria&29&tumor\\
  maria&56&nontumor\\
  \ldots&\ldots&\ldots\\
\end{tabular}
\caption{Paired data in a ``stacked'' shape/format.}\label{stacked}
\end{table}

The structure in Table \ref{stacked} is arguably a much more useful way of
storing the data for most general analyses and we can keep adding
additional information if we need to\footnote{For example, Wickham's ``Tidy data'', in the
  \textit{Journal of Statistical Software}:
  \Burl{http://www.jstatsoft.org/v59/i10}}.

\subsubsection{Reshaping our data}\label{reshaping}

In our case, we cannot do a paired t-test directly with our data because
of the way the data have been given to us. We first need to reshape the
data as R Commander ---in contrast to using directly R--- will not allow
us to carry out a paired t-test on these data as they are. Of course, you
could also do this outside R if you want and then import the data so that
the two values of myc expression for a subject are on the same row but
that is ugly.

So that is what we want: the values of myc of the same subject to be on
the same row. We will do this reshaping in four different ways.

There are many alternative approaches for reshaping the data directly from
the command line\footnote{For instance:
  \Burl{http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/}
  or \Burl{https://rpubs.com/bradleyboehmke/data_wrangling}; these are
  heavily biased towards the ``Hadley's way''.}. And yes, it is unavoidable
that this will get a little bit complicated. But manipulating and
reshaping data must be done with a lot of care. You do not need to
remember the details here, you just need to be aware of the issues.





\subsubsection{Reshaping using R Commander (I)}
\label{sec:reshaping-rcmdr-1}

(Yes, we already saw  this in Lesson 1) We will turn the data from long to wide. You already know how to do it (see Figure \ref{reshape-rcmdr}):

\begin{figure}[h!]
 \begin{center}
 \includegraphics[width=0.40\paperwidth,keepaspectratio]{l2-reshape-rcmdr.png}
 \caption{\label{reshape-rcmdr} Reshaping using R Commander (I).}
 \end{center}
 \end{figure}


 And this is the code produced from R Commander:
<<>>=
dmycWide <- reshapeL2W(dmyc, within="cond", id="id", varying="myc")
@


\subsubsection{Reshaping using R Commander (II)}\label{reshapemenu}

This is a less elegant approach, though in simple cases will work:

\begin{enumerate}
\item Create a data set that contains only the values for observations
  with status ``Cancer''. (``Subset'', under ``Active data set''). Use the
  ``Subset expression'' you want; in this case
  \verb@cond == "Cancer"@. Call this data set cc.

\item Make sure ``id'' is used as row name. Do this using ``Set case
  names'' under ``Active data set''. Can you understand why we are doing
  this?

\item Rename the value of variable ``myc'' to ``myc.c'' (for MYC in the
  cancer patients).

\item Create a data set that contains only the values for observations
  with status ``NC''.  Call this data set, say, nc. (Yes, you need to
  select the original data, ``dmyc'', then ``Subset'', and use as
  expression \texttt{cond == "NC"}).
  \item Again, set ``id'' as case names for this data set.
\item Rename this ``myc'' to ``myc.nc''.

\item Merge nc and cc. Call it ``merged''. Use the option to merge columns!

\item (Why can we trust that it did get it right? Look at the code that
  shows \\
  \verb@merge(cc, nc, all=TRUE, by="row.names")@).
\end{enumerate}

%% Note: this does not work unless we do further work:
%% merge(dmyc[, dmyc$cond == "NC], dmyc[, dmyc$cond == "Cancer"], all = TRUE, by = "row.names")





\subsubsection{Reshaping with \Rfunction{unstack}}\label{reshapestack}

%%  I will use just one here,
%% which might be the easiest to begin with, with data with a structure as
%% simple as ours (I'd probably very rarely use this approach for real, with
%% my own data, but doing it differently would require us to take a longer
%% diversion).
We will first use the R function \Rfunction{unstack}. This is the natural
reverse operation of ``stacking''.


<<>>=
unstack(x = dmyc, form = myc ~  cond)
@

That seems to do it but \ldots how can you be sure each row corresponds to
the same subject? We are counting on it to work, but it might not if the
data had not been ordered by subject id. (See also section \ref{ptr}). So
we will be explicit here, ordering by condition and then id:

<<>>=
dmyc0 <- dmyc[order(dmyc$cond, dmyc$id), ]
dmyc0
@

Now, we unstack:

<<>>=
merged2 <- unstack(dmyc0, form = myc ~ cond)
merged2
@

We could directly use the data, but we are missing the IDs. We will add them:

<<>>=
(merged2$id <- dmyc0$id[1:12])
@

Verify we are now ok comparing merged2 with dmyc.



\subsubsection{Reshaping with \Rfunction{reshape}}
\label{reshapereshape}

This is another built-in function in R, \Rfunction{reshape} (and the one used internally by the \texttt{reshapeL2W} from R Commander). This can be a complicated function to use, but it is extremely powerful. I'll use it
here fully specifying all the  arguments\footnote{In this case, you
  do not need to specify the \texttt{v.names} argument, for example, but
  I'd rather be explicit.}:

<<>>=
(merged3 <- reshape(dmyc, direction = "wide", idvar = "id",
                    timevar = "cond", v.names = "myc"))
@


%% FIXME: or dcast from reshape2?
\subsubsection{Reshaping with  \Rfunction{spread}}\label{spread}

OK, so we will do it in a fourth way. You need to install the
\CRANpkg{tidyr} package. If you don't have it, install it if you
want. This is not a big deal: it is just to show you a fourth way.

<<>>=
library(tidyr)
(merged4 <- spread(dmyc, cond, myc))
@

You can compare with \texttt{dmycWide} and \texttt{merged3} and \texttt{merged2} and \texttt{merged}, and verify they have the same data.


\subsubsection{\Rfunction{dcast} from \CRANpkg{reshape2}?}
\label{sec:rfunct-from-cranpkgr}

As it says, you could try doing the reshaping using \Rfunction{dcast} from
\CRANpkg{reshape2}. Try it if you want to.

%% Probably the preferred way now is tidyr

%% dcast(dmyc, id ~ cond, value.var = "myc")

%% see here for nice summary: http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/


\subsubsection{Reshaping et al: final comments}\label{reshapefinal}

A few comments:
\begin{itemize}
\item This data reshaping is not strictly necessary from doing a paired t-test in R but it is for R Commander (see below ---section \ref{ptr}).
\item We will see below a way to carry out these same analyses with a
  linear model (section \ref{lm-paired}). And no reshaping needed.

\item Data manipulation can be tricky: you \textbf{must} be sure to get it
  right. What would I do? Do it in two different ways, and compare. Among
  those two I'd probably have \Rfunction{reshape} because, even if
  complicated, it allows you to be completely explicit about what is what.
\end{itemize}


%%% On Rcmdr: say something about where to redirect output, in Rcmdr
%%% options?
%% Even if one redirects ouptu to consloe from Rcmdr, when using Rcmdr
%% from both emacs+ess or shell, things like
%%  log(3))
%% do not produce any sensible error message because of new capture
%% procedure, it seems.
%% Using Rstudio, however, allows to send type things in the console
%% directly, and get sensible error messages. Note than if from Emacs or
%% shell, that does not work reliably regardless of whether or not we set
%% Rcmdr to use the R console for outpyt.
%% yes, in Rstudio it seems that hard-wired that Rcmdr send things to the
%% (Rstudio) console.



%% \subsection{Reshaping the data for a paired t-test, second approach}


%% You can do this in a simpler way using the \CRANpkg{RcmdrPlugin.doBy}
%% (BEWARE: you must install and use the version of \CRANpkg{RcmdrPlugin.DoBy}
%% I provided you\footnote{There is a bug in one of the functions in the original
%%   package, and I have fixed it. The version you can get from CRAN will not
%%   work; the version I provide you has the bug fixed ---I've emailed the
%%   author about this, but so far haven't reached him.}).


%% \begin{enumerate}
%% \item Split the dmyc data set according to ``cond'', using the ``Split by''
%%   under the new ``doBy'' menu.
%% \item For each of the new data sets, turn the id column into the rownames
%%   as we did before: under ``Data'', ``Active data set'', ``Set case
%%   names''.
%% \item Merge the two new data sets, again using ``Merge'' and doing it by
%%   columns. Call this m3, for instance.
%% \end{enumerate}

%% Once you have done all of the above, you will have this on the R Script
%% (and the Output)
%% <<>>=
%% SplitData <- splitBy(~ cond, data = dmyc)
%% SplitData.Cancer <- SplitData["Cancer"]
%% SplitData.NC <- SplitData["NC"]
%% SplitData.Cancer <- as.data.frame(SplitData.Cancer)
%% SplitData.NC <- as.data.frame(SplitData.NC)
%% row.names(SplitData.Cancer) <- as.character(SplitData.Cancer$Cancer.id)
%% SplitData.Cancer$Cancer.id <- NULL
%% row.names(SplitData.NC) <- as.character(SplitData.NC$NC.id)
%% SplitData.NC$NC.id <- NULL
%% m3 <- merge(SplitData.NC, SplitData.Cancer, all=TRUE, by="row.names")
%% rownames(m3) <- m3$Row.names
%% m3$Row.names <- NULL
%% @


%% Please, before continuing, make sure you look at the data: ``View data set''.



%% %% There is a third approach, with aggregate and using id as aggregate by,
%% %% and function function(x) {return(x)}


\subsection{The paired t-test}
Just do a paired t-test. What is the result? Compare it with doing a
t-test as if they were two independent samples. The differences are rather
dramatic!


I'll use \texttt{dmycWide}; you can use \texttt{merged3} or \texttt{merged} or \texttt{merged2}
or \texttt{merged4}
if you want. I'll type the expressions directly. You can of course get
this from R Commander (under ``Statistics'', ``Means'').

<<>>=
## Paired; you can get this from R Commander too
t.test(dmycWide$myc.NC, dmycWide$myc.Cancer, alternative='two.sided',
       conf.level=.95,  paired=TRUE)
@


<<>>=
## Two-sample
t.test(dmycWide$myc.NC, dmycWide$myc.Cancer, alternative='two.sided',
       conf.level=.95,  paired=FALSE)
@

The last is of course the same as
<<>>=
t.test(myc~cond, alternative='two.sided', conf.level=.95,
       var.equal=FALSE,  data=dmyc)

@

(Is there a sign difference? Why? What happens if you change the order of
NC and Cancer in the paired test?)


\subsection{The paired t-test again: a single-sample t-test}\label{paired-single}
What is the above test doing?

Create a new variable (e.g., call it ``diff.nc.c'') that is the difference
of myc in the NC and Cancer subjects (``Data'', ``Manage variables'',
``Compute new variable''). You have to fill up (modify to your needs) the
boxes ``New variable name'' and ``Expression to compute''

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.80\paperwidth,keepaspectratio]{compute-diff-var.png}
    \caption{\label{ucla} Creating a new variable that is the difference
      between the two values for a subject.}
  \end{center}
\end{figure}


You will see this in the R Script:
<<>>=
dmycWide$diff.nc.c <- with(dmycWide, myc.NC - myc.Cancer)
@


Now, do a single-sample t-test:
<<>>=
with(dmycWide, (t.test(diff.nc.c, alternative='two.sided', mu=0.0,
                      conf.level=.95)))
@

Compare the t-statistic, the degrees of freedom and the p-value with the
paired t-test we did above. Again: what is it we are doing with the paired
t-test? (Remember this, as this will be crucial when we use Wilcoxon's
test).



\clearpage
\subsection{Plots for paired data}

What do you think of this plot?

<<fig.width=5.5, fig.height=5.5>>=
plotMeans(dmyc$myc, dmyc$cond, error.bars="se", ylab = "MYC",
          xlab = "Condition")
@

% (By the way, this is a plot that you must be sure you know how
% to generate from R commander.)

So what is a reasonable plot for paired data? A boxplot (or stripchart if
few points) of the within-individual (or Intra-subject) differences is a
very good idea. % In fact, I will now create them from scratch directly from R
% (this \textbf{assumes} that the data are ordered the same way by patient:
% the first myc.cancer is the same patient as the same myc.nc, etc. This is
% the case in our data, but need not be; see details in section \ref{ptr})



<<fig.width=5.5, fig.height=5.5, echo=TRUE, fig.show = 'hold'>>=
Boxplot( ~ diff.nc.c, data = dmycWide, xlab = "",
         ylab = "Intra-subject difference (NC - Cancer)", main = "MYC")
@


We can also create a violin plot with the boxplot inside and the points jittered, as we did in Lesson 1, using KMggplot2, with some additional editing of the code by hand (and some more editing to fix a deprecation warning from ggplot2):

<<diff-violin, fig.width=4.5, fig.height=4.5, echo=TRUE, fig.show = 'hold'>>=
library(RcmdrPlugin.KMggplot2)

.df <- data.frame(y = dmycWide$diff.nc.c)
.plot <- ggplot(data = .df, aes(x = factor(1), y = y)) +
  geom_violin() + geom_boxplot(width = 0.2) +
  geom_jitter(colour = "black", width = 0.1, height = 0) +
  scale_x_discrete(breaks = NULL) +
  xlab("") +
  ylab("Intra-subject difference (NC - Cancer)") +
  theme_bw(base_size = 14, base_family = "sans") +
  theme(axis.title.x = element_blank(), axis.text.x = element_blank())
print(.plot)
rm(.df, .plot)
@

In this case, the violin plot is an overkill because we have too few data points. This is just to show it as an example.



% <<fig.width=5, fig.height=5, echo=TRUE>>=
% diffs <- dmyc$myc[1:12] - dmyc$myc[13:24]
% Boxplot( ~ diffs, xlab = "", ylab = "Intra-subject difference", main = "MYC")
% @



\clearpage

A more elaborate plot directly shows both the NC and Cancer data, with a
segment connecting the two observations of a subject. For that, though, we
need to use R code directly. For example this will do (beware: this code
does the job but it is not very efficient or elegant):

<<fig.show='hold'>>=
stripchart(myc ~ cond, vertical=TRUE, data=dmyc)
for(i in unique(dmyc$id))
  segments(x0 = 1, x1 = 2,
           y0 = dmyc$myc[dmyc$cond == "Cancer" & dmyc$id == i],
           y1 = dmyc$myc[dmyc$cond == "NC" & dmyc$id == i],
           col = "red")
@

You can copy the above code in the R Script window and click on
``Submit'', or you can copy it in the RStudio console.

Now, look at these plots carefully, and understand that there are
different sources of variation. In this particular example, there was a
huge inter-subject variation in the expression of MYC; if we can control
for it (e.g., with intra-subject measures) then we can detect what is, in
relative terms, a small effect due to the condition.


(This is a short section, but it is very important. That is the reason I
have added a few plots to beef it up. Seriously.)





\subsection{Choosing between paired and two-sample t-tests}
\textbf{How the data were collected dictates the type of analysis}. This
is a crucial idea. You cannot conduct a two-sample t-test when your data
are paired because your data are NOT independent (see also section
\ref{pseudorep}).

This section is not about the analysis, but about the design. It is about
``should I collect data so that data are paired or not?'' A paired design
controls for subject effects (correlation between the two measures of the
same subject) but if there are none, then we are decreasing the degrees of
freedom (by half): compare the degrees of freedom from the paired and the
non-paired tests on the myc data. In most cases with biological data there
really are subject effects that lead to large correlations of
within-subject measurements. But not always.

This is a major topic (that cannot be done justice to in a
paragraph). Sometimes the type of data are something you have no control
over. But sometimes you do have control. How do you want to spend your
money? Suppose you can sequence 100 exomes. Do you want to do 100
samples, 50 controls and 50 tumors, or do you want to do those same 100
exomes, but from 50 patients, getting tumor and non-tumor tissue? The
answer is not always obvious. Go talk to a statistician.




\section{One-sample t-test}\label{sec:one-sample-t}
We have already used the one-sample t-test (section
\ref{paired-single}). You can of course compare the mean against a null
value of 0, but you can also compare against any other value that might
make sense (``Null hypothesis mu'' in R commander). Issues about
assumptions are as for the two-sample (except, of course, there are no
considerations of differences in variances among groups, since there is
only one group). One-sample tests are probably not as common in many
cases, as we are often interested in comparing two groups. But when you
want to compare one group against a predefined value, the one-sample
t-test is what you want. (I am sure you have studied this before).






\section{Non-parametric procedures}


\subsection{Why and what}\label{non-par-rationale}

Non-parametric procedures refer to methods that do not assume any particular
probability distribution for the data. These methods often proceed by replacing
the original data by their ranks. They offer some protection against deviations
from some assumptions (e.g., deviations from normality).  But some times they
might be testing a slightly different null hypothesis. Whether or not to use them
can sometimes be a contentious issue. Some considerations that come into play
are:

\begin{itemize}
\item Do the data look bad enough that a parametric procedure will lead to
  trouble?
\item What about the relative efficiency of the non parametric procedure?
  (relative efficiency refers to the sample size required for two
  different procedures that have similar type I error rate to achieve the
  same power ---more on power later). When assumptions are met, parametric
  methods do have more power (though not always a lot more). When
  assumptions are not met, nonparametric methods might have more power, or
  the correct Type I error, etc. Note that very, very, very small p-values
  are often not achievable with non-parametric methods (because of the way
  they work), and this is a concern with multiple testing adjustments in
  omics experiments (this will be covered in the next lesson).
\item Is this test flexible enough? In particular, can I accommodate
  additional experimental factors?
\end{itemize}



We focus here on the Wilcoxon test. This is often the way to go when we
have \textbf{ordinal} scale measurements and we want to compare two
independent groups. Note, however, that the Wilcoxon test \textbf{requires
  interval scale data} for the single-sample Wilcoxon and the paired
Wilcoxon (this is something that many websites and some textbooks get
wrong); a simple illustration is shown in section \ref{wpi}.


However, \textbf{\red{the independence assumption}} is as important with
Wilcoxon as with the t-test. Nonparametric tests are not
``assumption-free'' tests! (there is no such thing, in statistics or in life).


% An excellent non-parametrics book is Conover's ``Practical nonparametric
% statistics'', in its 3rd edition (it is so awesome, I have two copies, one
% at home and one at the office ---you can get one from Amazon for about 20
% \texteuro{}). (For more general categorical data analysis, Agresti's
% ``Categorical data analysis'' is a must. It is now also in its third
% edition. There is, by the same author, a smaller ``Introduction to
% categorical data analysis'').



\subsection{Wilcoxon rank-sum test or Mann-Whitney U test: 2 independent samples}

This test applies to data of ordinal and interval scales. The basic logic
is: put all the observations together, rank them, and examine if the sum
of the ranks of one of the groups is larger (or smaller) than the
sum of the ranks from the other\footnote{The actual test statistic reported by R is not this one, but is one that is linearly related to it. Likewise, the equivalent Mann-Whitney U test uses another different statistic, but the test results are identical.}. If this makes sense to you, you should
understand why you can use both ordinal and interval scales.


%% The W statistic is: sum(ranks)[group = A] - n*(n+1)/2; where n is the sample size of the group in question.

Just go do it with R Commander. It is under ``Statistics'',
``Nonparametric tests''. Do it for p53, pten, and brca1. Compare the
results with those from the t test.

For example:
<<>>=
wilcox.test(p53 ~ cond, alternative="two.sided", data=dp53)
@




% Contrary to Conover, not really testing F(x)= G(x)
%% Wikipedia entry much clearer: is a nonparametric test of the null
%% hypothesis that it is equally likely that a randomly selected value
%% from one sample will be less than or greater than a randomly selected
%% value from a second sample.

% <<>>=
% ## Will not reject
% x <- rnorm(10000, mean = 0, sd = 1)
% y <- rnorm(10000, mean = 0, sd = 20)
% wilcox.test(x, y)
% @


% Not about medians or means either

% <<>>=
% ## Will reject, medians the same
% x <- c(rep(10, 1000), 11, rep(12, 1000))
% y <- c(rep(10, 1000), 11, rep(13, 1000))
% summary(x)
% summary(y)
% wilcox.test(x, y)

% ## Will reject, means the same
% x <- seq(from = 10, to = 20, length.out = 1000)
% y <- c(rep(0, 999), 15000)
% summary(x)
% summary(y)
% wilcox.test(x, y)

% @



% <<>>=

% ## Will accept, medians differ
% x <- c(rep(10, 1000), 11, rep(12, 1000))
% y <- c(rep(10, 1000), 10.1, rep(12, 1000))
% summary(x)
% summary(y)
% wilcox.test(x, y)


% ## Will accept, means differ
% x <- c(rep(10, 1000), 1e9, rep(1000, 1000))
% y <- c(rep(10, 1000), -1e9, rep(1000, 1000))
% summary(x)
% summary(y)
% wilcox.test(x, y)
% @

\subsection{What a rank-sum Wilcoxon test is not}

Many people say ``I'll use a Wilcoxon test to compare the means''. Well \ldots the \textbf{Wilcoxon test is not a test of means}. It is often not even a test of medians.  The Wilcoxon test can reject the null even if the medians are the same, and the Wilcoxon test can fail to reject the null even if the medians differ.

How is this possible? Please, make sure you read this:

\Burl{https://www.graphpad.com/guides/prism/latest/statistics/stat_nonparametric_tests_dont_compa.htm}

%% in the directory as  graph-pad-link.png

More details are available here: \Burl{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1120984/} and in the Wikipedia entry for the Mann-Whitney U test (\Burl{https://en.wikipedia.org/wiki/Mann\%E2\%80\%93Whitney_U_test}). Many more details about permutation tests not testing what you might think they are testing are available, for example, from Christensen \& Zabriskie, 2022. When Your Permutation Test is Doomed to Fail. \textit{The American Statistician}, 76. \Burl{https://doi.org/10.1080/00031305.2021.1902856}


A few examples? Sure. Run the following in R:

<<>>=

## Will accept, medians differ
x <- c(rep(10, 1000), 11, rep(200, 1000))
y <- c(rep(10, 1000), 100,  rep(200, 1000))
summary(x)
summary(y)
wilcox.test(x, y)


## Will accept, means differ
x <- c(rep(10, 1000), 1e9, rep(1000, 1000))
y <- c(rep(10, 1000), -1e9, rep(1000, 1000))
summary(x)
summary(y)
wilcox.test(x, y)
@




<<>>=
## Will reject, medians the same
x <- c(rep(10, 1000), 11, rep(12, 1000))
y <- c(rep(10, 1000), 11, rep(13, 1000))
summary(x)
summary(y)
wilcox.test(x, y)

## Will reject, means the same
x <- seq(from = 10, to = 20, length.out = 1000)
y <- c(rep(0, 999), 15000)
summary(x)
summary(y)
wilcox.test(x, y)

@



<<>>=
## Will not reject so
## contrary to Conover, not really testing F(x)= G(x)
x <- rnorm(10000, mean = 0, sd = 1)
y <- rnorm(10000, mean = 0, sd = 20)
wilcox.test(x, y)
@

<<fig.height=5, fig.width=5>>=
stripchart(c(x, y) ~ c(rep("x", 10000), rep("y", 10000)), vertical = TRUE,
           method = "jitter", ylab = "Value")
@


What is the Wilcoxon test testing? As the Wikipedia entry says ``the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater than X.''


Summary: do not use a Wilcoxon expecting it to test for mean differences. And below we will emphasize another related message: do not use a Wilcoxon because of some ill-motivated fear of using the t-test.

%


\subsection{Wilcoxon signed-rank test: matched-pairs or single sample test}

When we have paired data, the idea here is to assess whether the within-pair differences are symmetrically distributed (generally around zero). As we are talking about symmetry, this means that this test can be used \textbf{only with interval scale} data (see also section \ref{wpi}).


The null hypothesis is that the differences are symmetrically distributed around some value (generally zero). We can therefore reject the null if the differences are symmetrical around some other value, or if the differences are not symmetrical around the posited value (as we said, generally 0), or a combination of both. Put differently, if we reject the null, we could be rejecting it for different reasons (asymmetry, symmetry around a value different from the one specified by the null), or combinations of those reasons\footnote{This is discussed, for example, in \Burl{https://stats.stackexchange.com/questions/348057/wilcoxon-signed-rank-symmetry-assumption}. Note, by the way, that this is not unique to this test: many tests can reject the null for a variety of reasons, and this is common in permutation-based tests.}. As usual, try to look at plots to disentangle what might be the case. It is often the case that, if we are dealing with within-individual differences, for example a ``before vs.~ after'' design, those within-individual differences will tend to be symmetric even if the center of the distribution shifts (regardless of the original distribution of the ``before'' measure ---see also the example in \ref{sym-paired-t}), but not always.


How does the test work? In a nutshell, this is what the test does: for each pair, it computes the difference of the two measures (thus, taking differences must make sense, and it does not for ordinal data but it does for interval data). These differences (discarding the sign) are then ranked, and then the sum of the ranks of all the positive differences is computed and compared to the null distribution.




Go do it with the dmyc data we used for the paired t-test (section
\ref{pairedt}). Remember we had to reshape the data (section
\ref{reshaping}) so I will reuse our \texttt{dmycWide} data set:


<<>>=
wilcox.test(dmycWide$myc.NC, dmycWide$myc.Cancer, alternative='two.sided',
            paired=TRUE)
@

You can verify that the results are the same as doing a single sample
Wilcoxon, or a Wilcoxon signed-rank test, on the within-subject
differences. %% The single sample Wilcoxon test is not directly available
%% from R Commander of versions before 2.1-0 and you can run it from
%% the console as: %% it is in 2.1-7


Remember we computed intra-subject differences before
(section \ref{paired-single}), so we will reuse them:

<<>>=
wilcox.test(dmycWide$diff.nc.c)
@

For versions of R Commander 2.1-0 or later you can run the Wilcoxon signed
rank test directly from the menu.


Before we move on: what is the role of symmetry in the paired t-test? To try to make progress thinking about this, remember that the paired t-test is the same as testing the mean of the within-individual differences against a pre-specified mean. Go to Appendix, section \ref{sym-paired-t} for an example.




\subsection{A bad (very bad, terrible!) way to choose between nonparametric and parametric
  procedures}

Don't do the following:
\begin{enumerate}
\item Carry out a test for normality (I guess for each group separately if and independent-samples t-test ---testing for normality collapsing the two groups would be absolutely nonsensical).
\item If failed, use a nonparametric test. Otherwise a t-test.
\end{enumerate}



There are several problems with the above procedure, among them:

\begin{itemize}
\item Tests for normality do not have a power of 1, and in fact can have
  very low power with small sample sizes.
\item They can have power to detect a minor and irrelevant deviation from
  normality (e.g., slightly heavy tails) that have no effects on, say, a
  t-test. So we should really prefer a t-test, but we might be mislead
  into doing a Wilcoxon.
\item They might fail to detect a large deviation from normality in a very
  non-normal small sample where we really, given our ignorance, might want
  to conduct a Wilcoxon test.
\item They lead to ``sequential testing'' problems, where the second
  p-value (the one from the t or Wilcoxon) cannot be simply interpreted at
  face value (as it is conditional on the normality test).
\end{itemize}

Even if this procedure is (or was) common in some circles, you now know why it is a bad idea. Please, do not do this.

Again, this is the type of ``cargo-cult statistics''\footnote{``going through the motions with scant understanding''; Stark and Saltelli, 2018. Cargo-cult statistics and scientific crisis. \textit{Significance}, 15.} that does no body any good. Think about your data, what you want to test (is it means? or is it something else?), what are reasonable assumptions (heavy tails, asymmetries, etc) and whether the deviations are likely to cause trouble. The above simple-minded (brain-dead, actually) procedure is no substitute for thinking.





\clearpage
\section{Non-independent data}\label{pseudorep}

\subsection{Multiple measures per  subject}
Paired data are non-independent: they are associated via the subject, or
id. There are other forms of dependency. Many. We will now look at the
most common one: multiple measures per subject. (This, by the way, you saw
in Lesson 1).

<<create_brca, echo=FALSE, results='hide'>>=
set.seed(27)
n1 <- 8
n2 <- 5
nn <- 3
s <- rnorm(n1 + n2, 0, 2)
s <- rep(s, rep(nn, n1 + n2))
effect <- 1
cond.effect <- rep(c(0, effect), c(n1 * nn, n2 * nn))
cond <- rep(c("Cancer", "NC"), c(n1 * nn, n2 * nn))
y <- rnorm(length(cond.effect)) + s + cond.effect
y <- y - min(y) + 0.1
id <- replicate(n1 + n2, paste(sample(letters, 10), collapse = ""))
id <- rep(id, rep(nn, n1 + n2))
dbrca <- data.frame(brca2 = round(y, 3),
                    cond = cond,
                    id = id)
## t.test(brca2 ~ cond, data = dbrca)
## aggbrca2 <- aggregate(dbrca[,c("brca2"), drop=FALSE], by=list(cond=dbrca$cond,
##   id=dbrca$id), FUN=mean)
## t.test(brca2~cond, alternative='two.sided', conf.level=.95, var.equal=FALSE,
##   data=aggbrca2)
## summary(aov(brca2 ~ cond + Error(id), data = dbrca))
## t.s <- summary(lmer(brca2 ~ cond + (1|id), data = dbrca))$coefficients[2, 3] ## t-statistic
## t.s
## t.s*t.s ## compare with aov

write.table(dbrca, file = "BRCA2.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(dbrca)
@


<<echo=FALSE, results = 'hide'>>=
dbrca <- read.table("BRCA2.txt", header = TRUE, stringsAsFactors = TRUE)
@


We will use the \Robject{BRCA2.txt} data set. Import it, etc. Look at the
data carefully. It looks like there are \Sexpr{nn} observations per
subject in a total of \Sexpr{n1+n2} subjects. Basically, each subject has
been measured \Sexpr{nn} times. Thus, there are not \Sexpr{nn * (n1+n2)}
independent measures. Again, ask yourself: what is the true ``experimental
unit'' (or observational unit)?  It is arguably the subject, which is the
one that has, or has not, cancer in this case. The \Sexpr{nn} measures per
subject are just that: multiple measures of the same experimental
unit. This is an idea that should be crystal clear; make sure you
understand it.

We can do a t-test ignoring this fact, but it would be wrong. Look at the
degrees of freedom:


<<>>=
t.test(brca2 ~ cond, data = dbrca)
@


What can we do? The most elegant approaches are not something we can cover
here\footnote{A general approach is a mixed-effects linear model with
  random effects for subjects; alternatively, and in this simple case, we
  can use an ANOVA with the correct error term, for instance from a
  multistratum linear model similar to the ones used to analyze split-plot
  experiments.}. However, fortunately for us in \textbf{this particular
  case}, all subjects have been measured the same number of times. Thus,
we can simply take the mean per subject, and then do a t-test on those
mean values per subject.

In R Commander, go to ``Data'', ``Active data set'', ``Aggregate variables
in active data set''. We want to aggregate (using the mean) the values of
brca2, and we want to aggregate by ``id''. However, we want to keep
``cond'' also. So we will aggregate by ``cond'' and ``id''. In fact, this
is a double check that each subject is in one, and only one, of the
groups. Call this data ``aggbrca''.

<<>>=
aggbrca <- aggregate(brca2 ~ cond + id, data = dbrca, FUN = mean)
@

%% In this case, you can also call \Rfunction{aggregate} in a simpler way if
%% you directly type it:

%% <<>>=
%% aggbrca <- aggregate(brca2 ~ cond + id, data = dbrca, FUN = mean)
%% @



We want to double check that we have the exact same number of measures per
subject. How? A simple procedure is to aggregate again (give the output
another name) but using ``length'' instead of ``mean''. Now we will be
aggregating over only ``id'' (so we aggregate brca2 and cond) and also
over both ``id'' and ``cond'', to double check. Do you understand why we
are doing this?


<<>>=
aggregate(brca2 ~ cond + id, data = dbrca, FUN = length)
aggregate(brca2 ~ id, data = dbrca, FUN = length)
aggregate(. ~ id, data = dbrca, FUN = length)
@


Now, let's do a t-test on the aggregated data. Pay attention to the
degrees of freedom (and the statistic and p-values):

<<>>=
t.test(brca2~cond, alternative='two.sided', conf.level=.95, var.equal=FALSE,
  data=aggbrca)
@

You can see that doing things correctly makes, in this case, a large
difference. When we do things incorrectly, we can end up believing that
there is a strong effect when, really, there is no evidence of effect.


What if the subjects had been measured a different number of times? What
would be the problems of simply taking the averages over subjects?



\subsection{Nested or hierarchical sources of variation}
A general way of thinking about these issues is that we can have nested, or
hierarchical, levels of variation (e.g., multiple measures per cell, multiple
cells per subject, multiple subjects for two different treatments) and it is
crucial to understand what each level of variation is measuring (e.g., the
difference between technical and biological variation) and to conduct the
statistical analysis in a way that do incorporate this nestedness. A brief
introductory 2-page paper with biomedical applications is Blainey et al., 2014,
``Points of significance: Replication'', in \textit{Nature Methods}, 2014, 11,
879--880, where they discuss issues of sample size choice at different levels.

By the way, not surprisingly, split-plot ANOVA and nested ANOVA are
typical, traditional, ways of analyzing these kinds of designs. Many of
these issues are, thus, naturally addressed in the context of linear
models (Lesson 3).



\subsection{Non independent data: extreme cases}
In the extreme, this problem can lead to data that should not be analyzed
at all because there is, plainly, no statistical analysis that can be
carried out. For instance, suppose we want to examine the hypothesis that
consuming DHA during pregnancy leads to increased mielinization in the
hippocampus of the progeny. An experiment is set to test this idea,
comparing the effects on mielinization of mice that have been born to
female mice with and without a DHA supplement\footnote{This is based on an
  actual data set I was once asked to analyze. I've changed enough details
  ---no DHA or anything similar. But the outcome was the same: no analyses
  were possible at all.}.


Now, a colleague comes to you with the data. She claims there are 40 data
points, 20 from brains of newborn mice under the DHA supplement and 20
from brains of newborn mice without the supplement. OK, it looks good: it
seems we can carry out an independent samples t-test with about 38 degrees
of freedom. However, on asking questions, this is what you find out:

\begin{itemize}
\item A total of two female mice were used. One female was given food with
  DHA and one female without. They were fed the specified diet, and then
  they mated and got pregnant.
\item From the litter for each female, five newborn mice were sacrificed
  immediately after birth, and four tissue slides were prepared from each
  brain (thus, 20 data points per female).
\end{itemize}


No statistical analysis can be conducted here to examine the effects of
DHA: there is only one data point per experimental unit. We cannot do a
t-test in the right way: there are no degrees of freedom because there is
no way to estimate the variance.  To put it simply, there is no way to
tell whether any differences that could be seen are due to DHA or to the
female herself or to any other factor that might have been confounded with
the single female per treatment (color of the gloves of the technician or
side in the animal room or how nice the male was while mating or \ldots).

It is this simple: \textbf{nothing} can be said from this data about the
effects of DHA. (It is not even worth importing the data for this
analysis. Maybe it is interesting to get a preliminary idea of the
intra-brain variation in mielinization, but not for the original question
of the DHA effect).

There are some recent papers about issues like this that go over the
pseudoreplication idea (e.g., Lazic, 2010, \textit{BMC Neuroscience} and
Lazic et al., 2018, \texttt{PLoS Biology}, ``What exactly is 'N' in cell
culture and animal experiments?'') and this is probably a pervasive (but
difficult to detect) problem. This kind of meaningless analysis can lead
to lots of non-reproducible results.

Which brings us to the fundamental idea of \textbf{thinking carefully
  about the experimental design}, something we saw already in Lesson 1 and will take a quick look at
in Lesson 3.

%% The opposite can also happen, as you can see from the ACRB.txt file (do it
%% at home).

%% <<create_acrb, echo = FALSE, results='hide'>>=

%% set.seed(3987)
%% n1 <- 4
%% n2 <- 5
%% nn <- 3
%% s <- rnorm(n1 + n2, 0, .031)
%% s <- rep(s, rep(nn, n1 + n2))
%% effect <- .48
%% cond.effect <- rep(c(0, effect), c(n1 * nn, n2 * nn))
%% cond <- rep(c("Cancer", "NC"), c(n1 * nn, n2 * nn))
%% y <- rnorm(length(cond.effect)) + s + cond.effect
%% y <- y - min(y) + 0.1
%% id <- replicate(n1 + n2, paste(sample(letters, 10), collapse = ""))
%% id <- rep(id, rep(nn, n1 + n2))
%% dacrb <- data.frame(acrb = round(y, 3),
%%                     cond = cond,
%%                     id = id)
%% t.test(acrb~ cond, data = dacrb)
%% aggacrb <- aggregate(dacrb[,c("acrb"), drop=FALSE], by=list(cond=dacrb$cond,
%%   id=dacrb$id), FUN=mean)
%% t.test(acrb~cond, alternative='two.sided', conf.level=.95, var.equal=FALSE,
%%   data=aggacrb)
%% summary(aov(acrb ~ cond + Error(id), data = dacrb))
%% ## t.s <- summary(lmer(acrb ~ cond + (1|id), data = dacrb))$coefficients[2, 3] ## t-statistic
%% ## t.s
%% ## t.s*t.s ## compare with aov

%% write.table(dacrb, file = "ACRB.txt", col.names = TRUE,
%%             row.names = FALSE, sep = "\t", quote = FALSE)

%% @






\subsection{More non-independences and other types of data}
What if some subjects had cousins and brothers in the data set? And if
some of them came from the same hospital and other from other hospitals?
And ... ? This all lead to multilevel and possible crossed terms of
variance. Mixed effects models can be used here. Go talk to a
statistician (after looking at the notes for Lesson 3).



However, for simple cases and to get going while we talk to the
statistician, the approach we used above (collapsing the data over lower
levels, leaving data that are independent at the experimental unit level)
can some times be used in other scenarios. The independence assumption is
actually crucial in many statistical analysis, be they t-tests, ANOVAs,
chi-squares, regressions, etc, etc. (As has been mentioned repeatedly,
there are ways to incorporate or deal with the non-independence, for
categorical, ordinal, and interval data, but they are far from trivial).


To make the point more clear, this is an example from the data for the TFM
(``trabajo fin de master'') from a former student of BM-2. Briefly, she was interested in the rates
of chromosomal aberrations in different types of couples that went to a
fertility clinic. For instance, suppose you want to examine incidence of
aneuploidy in embryos from two groups of fathers, ``younger fathers'' and
``older fathers''%% (wait, not older, just ``fathers that so far have
%% accumulated more hours of life on Earth'')
. The simplest idea here is to
use a chi-square ($\mathcal{X}^2$) test to compare the frequency of
aneuploidies between older and younger fathers (you will see chi-square
tests in Lesson 4). But the problem is that each couple (each father in
this case) contributes multiple embryos and we cannot simply do a
chi-square counting embryos, as we would again run into a non-independence
problem. A simple approach, especially if each father contributes the same
number of embryos, is to calculate, for each father, the proportion of
embryos with aneuploidies. And then, to examine if that per-father
proportion of aneuplodies differs between older and younger fathers with,
say, an independent samples t-test (possibly of suitably transformed data)
or a two-samples Wilcoxon test.




\clearpage




\section{Appendix}

\subsection{Symmetry and the paired t-test}\label{sym-paired-t}

In the paired t-test symmetry of the within-subject differences is important. This we know, because the paired t-test is just a one-sample t-test applied to within subject differences (and the usual assumptions of the one-sample t-test apply, one of them being that the data have an approximately symmetrical distribution).

In the paired t-test, though, the within-individual differences (more generally, the within-experimental unit differences) are coming from, well, differences of, say, ``before'' and ``after'' distributions. How relevant is the distribution of those ``before'' and ``after''? In other words, if we define within-individual-difference as \(W\), with \(w_i = u_i - v_i\) (where \(U\) could be the ``after'' and \(V\) the ``before''), what is the relevance of the distributions of \(U\) and \(V\)? What can we expect about \(W\)?


If you have access to it, Rupert Miller's ``Beyond Anova'' (Chapman and Hall, 1997), bottom of p.~8 and p.~9 discusses this issue. In class, we will quickly discuss the code below:

<<>>=
## "num" random deviates from an exponential.
## Play with num if you want. We use a very large num
## to make it easier to see the patterns.
num <- 1000
V <- rexp(n = num, rate = 1)
## The exponential is clearly asymmetric
hist(V)

## Shift those numbers by adding 3, and a little bit of noise
U <- V + 3 + rnorm(num, sd = 0.2)
hist(U)

## What about the differences?
W <- U - V
hist(W)
@

So in many cases skewness (asymmetry) tends to cancel out (e.g., for example when we add a constant value plus some random, symmetric noise).


And this emphasizes something else: when examining if assumptions are reasonable, think about what you are examining. Here, the distribution that matters is the distribution of the within-individual differences (\(W\)), not each of the separate distributions \(U\) or \(V\): your test is about individual differences, \(W\), not about \(U\) or \(V\).


\subsection{A first taste of linear models}\label{lm-paired}

In the paired test, we implicitly have a model like this:

\[Expression.of.MYC = function(subject\ and\ condition) + \epsilon\]

which we make simpler (assuming additive contributions of each factor) as


\[Expression.of.MYC = \mathit{effect.of.subject} + \mathit{effect.of.condition} + \epsilon\]



Lets go and fit that model! Do it under ``Fit models'', ``Linear models'',
using the full dmyc data set. You will see

\verb@myc ~ cond + id@ (or, equivalently here, \verb@myc ~ id + cond@)

(if you just double click on the dependent variables, here id and cond,
they will be placed on the right, with a ``+'' sign by default).

\begin{figure}[h!]
 \begin{center}
 \includegraphics[width=0.70\paperwidth,keepaspectratio]{lm-myc.png}
 \caption{ Our first linear model.}
 \end{center}
 \end{figure}

<<>>=
LinearModel.1 <- lm(myc ~ id + cond, data=dmyc)
summary(LinearModel.1)
@

Run it and look at the line that has ``cond'' (so, using your common
sense, ignore all the ``id[blablablabla]'' lines). What is the t-value and
the p-value for ``cond''?


This is a linear model. And this particular one is also a two-way
ANOVA. If you go to ``Models'', ``Hypothesis tests'', ``Anova table'' you
will get

<<>>=
Anova(LinearModel.1, type="II")
@

Relate this to the figures above. This should all make sense. Regardless,
linear models and ANOVAs will be covered in much more detail in a few days.





\subsection{The paired t-test directly from R}\label{ptr}
We can directly use our dmyc data set from R, without any need for
reshaping. We will do things step by step (though we could just combine
all in a single step)

<<>>=
myc.cancer <- dmyc$myc[dmyc$cond == "Cancer"]
myc.nc <- dmyc$myc[dmyc$cond == "NC"]
t.test(myc.cancer, myc.nc, paired = TRUE)
@

Of course, this \textbf{crucially assumes} that the data are ordered the
same way by patient: the first myc.cancer is the same patient as the same
myc.nc, etc. This is the case in our data, but need not be. We can check
it:

<<>>=
dmyc
@

However, to ensure that order is OK we could have pre-ordered the data
by patient ID and by condition within patient (this second step isn't
really needed here), as we did before (see section \ref{reshapestack}):

<<>>=
dmycO <- dmyc[order(dmyc$id, dmyc$cond), ]
dmycO
myc.cancer <- dmycO$myc[dmycO$cond == "Cancer"]
myc.nc <- dmycO$myc[dmycO$cond == "NC"]
t.test(myc.nc, myc.cancer, paired = TRUE)
@







\subsection{Wilcoxon's paired test and interval data}\label{wpi}

If you understand the logic of what the two Wilcoxon tests are doing, you
should understand why the one for the two-sample case works with ordinal
data but the one for the matched pairs requires interval data. This
explores the issue further.

In particular, if we were to transform the data using a monotonic
non-linear transformation (e.g., a log transformation), the Wilcoxon test
for two groups should never be affected, but the one for the paired case
could be affected. Why? Because in the first case we rank the values, and
the ranks of the values are the same as the ranks of the values after any
monotonic transformation. However, the rank of the within-subject
differences might differ if we use a transformation on the original data
and then rank the within-subject differences.


We will use R directly here (as before, you can copy this code in the R
Script window and click ``Submit'', or copy it in the RStudio console, etc)

%% <<>>=
%% ## taking log does not change things when using the two-sample Wilcoxon
%% wilcox.test(p53 ~ cond, data = dp53)
%% wilcox.test(log(p53) ~ cond, data = dp53)
%% @


<<>>=
## Without logs
wilcox.test(dmyc$myc[1:12], dmyc$myc[13:24], paired = TRUE)
## After taking logs
wilcox.test(log(dmyc$myc[1:12]), log(dmyc$myc[13:24]), paired = TRUE)
@

Notice how the statistic and the p-value change. That would not happen if
the test could be applied to ordinal data.


It is also easy to create examples that show that the one-sample Wilcoxon
does require interval data. This is left as a simple exercise.




%% The example for the usual one-sample
%% <<>>=
%% x <- c(-2, -1, 0.9, 1.9)
%% y <- c(-2, -1, 0.9, 2.9)
%% z <- c(-2, -1, 0.9, 0.91)
%% median(x)
%% median(y)
%% median(z)
%% wilcox.test(x)
%% wilcox.test(y)
%% wilcox.test(z)
%% @


\subsection{Relations between variables}
This was about comparing two groups. But we had several variables
(genes). An obvious thing to do is to look at how they are related AND
display the different (two, in this case) groups. Playing
around, you should be able to reproduce this figure (note: I've left the
smoothed histograms, though they are of questionable application here,
with so few data).

<<fig.width=8, fig.height=8,echo=FALSE>>=
scatterplotMatrix(~brca1+brca2+p53+pten | cond, data=dp53)
@





%% \bibliography{lesson2}


\section{Session info and packages used}

This is the information about the version of R and packages used when
producing this document:
<<echo=FALSE,results='hide',error=FALSE>>=
options(width=60)
@

<<>>=
sessionInfo()
@
\end{document}

%% remember to use bibexport to keep just the minimal bib needed
%% bibexport -o extracted.bib OncoSimulR.aux
%% rm OncoSimulR.bib
%% mv extracted.bib OncoSimulR.bib
%% and then turn URL of packages into notes


%%% Local Variables:
%%% ispell-local-dictionary: "en_US"
%%% coding: iso-8859-15
%%% End:




% thisusedtosayTeX-master: "Lesson-2"




%%% en clase, decir xq p-value es type I error rate e inversi?n de CI


%% el viernes
%% formulas para paired
%% plots para paired
%% conf int derivation
%% power demo
%% CLT demo
%% Bayesian
