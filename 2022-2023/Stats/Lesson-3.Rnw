%% Beware: knitting this remotely might lead to LaTeX Error: Environment kframe
%% undefined

%% just run locally or probably exporting the X or something. This is an issue
%% with the locale, I think.

% \VignetteEngine{knitr::knitr}
\documentclass[a4paper,11pt]{article}
\synctex=1
<<echo=FALSE,results='hide',error=FALSE>>=
suppressMessages(library(SparseM, quietly = TRUE, warn.conflicts = FALSE))
require(knitr, quietly = TRUE)
library(patchSynctex)
opts_knit$set(concordance = TRUE)
opts_knit$set(stop_on_error = 2L)
options(width = 65)
@ 

<<packages,echo=FALSE,results='hide',message=FALSE>>=
require(BiocStyle, quietly = TRUE)
suppressMessages(library(Rcmdr, quietly = TRUE, warn.conflicts = FALSE))
require(car, quietly = TRUE)
## require(doBy, quietly = TRUE)
require(multcomp, quietly = TRUE)
require(abind, quietly = TRUE)
require(e1071, quietly = TRUE)
require(effects, quietly = TRUE)
require(lattice, quietly = TRUE)
require(HH, quietly = TRUE)
require(ISwR, quietly = TRUE)
## require(lme4, quietly = TRUE)
@ 



%% Get subsubsubsection-like behavior
%% https://tex.stackexchange.com/a/60212
% \usepackage{titlesec}
% \setcounter{secnumdepth}{4}
% \titleformat{\paragraph}
% {\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
% \titlespacing*{\paragraph}
% {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

%% But the above does not work, with a clash for package titlesec that I can't debug.
%% so use this: https://tex.stackexchange.com/a/364574
%% which is not optimal
\newcommand{\subsubsubsection}[1]{\paragraph{#1}\mbox{}\\}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}


%% not if using BiocStyle
%% \usepackage[authoryear,round,sort]{natbib}
%% \usepackage{hyperref} 
%%\usepackage{geometry}
%%\geometry{verbose,a4paper,tmargin=23mm,bmargin=26mm,lmargin=28mm,rmargin=28mm}

\usepackage[margin=10pt,font=small,labelfont=bf,
labelsep=endash]{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{threeparttable}
\usepackage{array}
\usepackage{url}
\usepackage{xcolor}
%\definecolor{light-gray}{gray}{0.72}
%% \newcommand{\cyan}[1]{{\textcolor {cyan} {#1}}}
%% \newcommand{\blu}[1]{{\textcolor {blue} {#1}}}
%% \newcommand{\Burl}[1]{\blu{\url{#1}}}
\newcommand{\red}[1]{{\textcolor {red} {#1}}}
\newcommand{\Burl}[1]{{\textcolor{blue}{\url{#1}}}}
\usepackage[copyright]{ccicons} %% for the CC license icons

%% see http://tex.stackexchange.com/a/121871
% \newcommand*{\fullref}[1]{\hyperref[{#1}]{\autoref*{#1} \nameref*{#1}}} % One single link
\newcommand*{\qref}[1]{\hyperref[{#1}]{\textit{``\nameref*{#1}'' (section \ref*{#1})}}}

\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}





\usepackage{gitinfo}


<<style-knitr, eval=TRUE, echo=FALSE, results="asis">>=
BiocStyle::latex()
@


%% %% Modify margins
\geometry{verbose,a4paper,tmargin=23mm,bmargin=23mm,lmargin=28mm,rmargin=40mm}


\bioctitle[Lesson 3: ANOVA and linear models]{BM-1, Applied Statistics, Lesson 3: ANOVA and linear models.}

\author{Ramon Diaz-Uriarte\\
  Dept. Biochemistry, Universidad Aut\'onoma de Madrid \\ 
  Instituto de Investigaciones Biom\'edicas ``Alberto Sols'' (UAM-CSIC)\\
  Madrid, Spain{\footnote{ramon.diaz@iib.uam.es, rdiaz02@gmail.com}} \\
%% {\footnote{rdiaz02@gmail.com}} \\
{\small \Burl{http://ligarto.org/rdiaz}} \\
 }
 

%% \title{Lesson 3. ANOVA and linear models}
  
%% \author{Ramon Diaz-Uriarte\\
%% Dept. Biochemistry, Universidad Aut\'onoma de Madrid \\ 
%% Instituto de Investigaciones Biom\'edicas ``Alberto Sols'' (UAM-CSIC)\\
%% Madrid, Spain\\
%% {\small \texttt{ramon.diaz@iib.uam.es}} \\
%% {\small \texttt{rdiaz02@gmail.com}} \\
%% {\small \Burl{http://ligarto.org/rdiaz}} \\
%% }

\date{\gitAuthorDate\ {\footnotesize (Rev: \gitAbbrevHash)}}
\begin{document}
\maketitle

\tableofcontents

\clearpage

\section*{Warning: eternally provisional}
This file can get changed often. When asking questions in class or in the
forum, refer to the section names and section numbers (these change less
than the page numbers).


\section*{License and copyright}\label{license}
This work is Copyright, \copyright, 2014-2022, Ramon Diaz-Uriarte, and is
licensed under a \textbf{Creative Commons } Attribution-ShareAlike 4.0
International License:
\Burl{http://creativecommons.org/licenses/by-sa/4.0/}.

\centerline \ccbysa



All the original files for the document are available (again, under a
Creative Commons license) from
\Burl{https://github.com/rdiaz02/BM-1}. (Note that in the github repo you
will not see the PDF, or R files, nor many of the data files, since those
are derived from the Rnw file).


\clearpage

\section{Introduction}

Linear models and their extensions (which include logistic regression, but
also survival analysis, many classification problems, non-linear models,
analysis of experiments, dealing with many types of dependent data, etc,
etc) are one fundamental topic in statistics. Here, we will only scratch
the surface. But you should come away from this lesson understanding that
these methods are extremely powerful and flexible and that they can be
used to address a huge variety of different research questions. You would
spend your time wisely if you at least took a look at some of the
references we provide at the end. To emphasize again: ANOVAs, regression,
ANCOVAs, are just special type of linear models; the terminology is not
that important, but I'll use those terms as they might be more familiar to
you. 



\subsection{Files we will use}

\begin{itemize}
\item This one
\item \Robject{MIT.txt}
\item \Robject{Cholesterol.txt}
\item \Robject{AnAge\_birds\_reptiles.txt}
\item \Robject{CystFibr2.txt}

\end{itemize}



\clearpage
\section{Comparing more than two groups}

<<create_mit, echo=FALSE, results='hide'>>=
set.seed(789)
n1 <- 11
n2 <- 12
n3 <- 23
m <- c(0, 0, 1.3)
activ <- rnorm(n1 + n2 + n3) + rep(m, c(n1, n2, n3))
activ <- activ - min(activ) + 0.2
mit <- data.frame(activ = round(activ, 3),
                  training = rep(c(1, 2, 3), c(n1, n2, n3)),
                  id = 1:(n1 + n2 +n3))
write.table(mit, file = "MIT.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(list = ls())
@ 



\subsection{Recoding variables}\label{recode}

Import \Robject{MIT.txt} and call the object \Robject{dmit}. These are
data about mitochondrial activity related to three different training
regimes. 

<<>>=
dmit <- read.table("MIT.txt", header = TRUE)
@ 



Whoever entered the data, however, used a number for ``training'', which is
misleading, because this is really a categorical variable. The first thing
we must do, then, is fix that. Go to ``Data'', ``Manage variables ...'',
``Convert numerical variables to factors''. We will want to label 1 as
``Morning'', 2 as ``Lunch'', and 3 as ``Afternoon'', which are the times
at which exercise was conducted and makes everything much more clear and
explicit. Use a new variable (e.g., ftraining).

\begin{figure}[h!]
  %\begin{center}
    \includegraphics[width=0.60\paperwidth,keepaspectratio]{recode-1.png} \includegraphics[width=0.40\paperwidth,keepaspectratio]{recode-2.png}
    \caption{\label{recode1} Converting training to ftraining, using more
      reasonable names.}
 % \end{center}
\end{figure}

\clearpage
After recoding, you should see this command. Note the \Rfunction{factor}
function call:
<<>>=
dmit$ftraining <- factor(dmit$training, 
                         labels=c('Morning','Lunch','Afternoon'))
@ 


As usual, make sure to look at the data set.

If we were to not recode the factor (or use the original ``training'') it
would be a disaster (look at the output in section \nameref{nofactor}).




\subsection{A boxplot}
As in Lesson 2, you are advised to also plot the data. For instance, this
will do:

<<>>=
Boxplot(activ~ftraining, data=dmit, id.method="y")
@ 


(The output might show a  ``2''; that  is the identifier ---row name--- of a point that
has been flagged as a potential outlier and we will silent that output
from now on in these notes).

%% FIXME: using aov is misleading. Don't do it. Use anova(lm)
\subsection{An ANOVA}\label{oneway}

We want to see if time of exercise makes any difference. Conducting three
t-tests is not the best way to go here: our global null hypothesis is
$\mu_{Morning} = \mu_{Lunch} = \mu_{Afternoon}$ and that is what ANOVA
will allow us to test directly.

Find you way around the menu and do a One-way ANOVA (``Statistics'',
``Means''). You'll see this in the R Script window:

<<results='hide'>>=
AnovaModel.1 <- aov(activ ~ ftraining, data=dmit)
summary(AnovaModel.1)
numSummary(dmit$activ , groups=dmit$ftraining, statistics=c("mean", "sd"))
@ 



(Note: we are using \Rfunction{aov}. We could also use \Rfunction{lm} and then
show the summary with function \Rfunction{anova}. These are just syntactic
features ---annoyances?--- of R. I'll explain them in class. But the key concepts
do not change.)

Can you interpret the output? Look first at the output right after

<<>>=
summary(AnovaModel.1)
@ 

We will cover this in more detail in class, in case you do not remember
ANOVA.


Read \textbf{now} the 2 and a half pages from Peter Dalgaard's book available in
Moodle (\texttt{anova-basic-theory.pdf}), then read
\texttt{anova-theory-even-simpler}, and then re-read Dalgaard's. If there are
questions, we will cover them in class.



Now, things to notice about the output:

\begin{itemize}
\item The two rows; one of them is the effect you are interested in (ftraining)
\item The ``Df'' column: those are the degrees of freedom (three groups -
  1 for ``ftraining'').
\item The two columns Sum Sq (Sum of Squares) and Mean Sq (Mean
  Squares). Sum of Squares is a quantity related to the variance. Mean
  Squares is obtained from the ratio of Sum Sq over Df. Then, we use Mean
  Sq to compare how much variance there is between groups related to the
  variance within groups: the F value is the ratio of Mean Sq of ftraining
  over Mean Sq of the residuals. The larger that F value, the more
  evidence there is of groups being different.
\item There is a p-value associated with that F value. In this case it is
  very small.
\end{itemize}


By the way, notice how we created a ``Model'' which, by default, is called
\Robject{AnovaModel.1}. But we could have named it differently.

%% TODO: zz: explain what df are

\subsection{Confidence intervals for the parameters of the model}

We can easily do

<<>>=
confint(AnovaModel.1)
@ 

For one thing, we are all well aware that simply looking at p-values is not the
only thing we want to do.

Sometimes \Rfunction{confint} will give us a lot of insight. But often it will
not be easy to relate it back to our original scientific question. One of the
reasons is that the actual parameters of the fitted model depend, well, on the
parameterization (see details in section \nameref{anovaaslm}, or just take it on
faith). So, often, we will want to explicitly ask ``Which means are different'',
and that is what we do next.



\subsection{So which means are different? Multiple comparisons}
\label{multcomp1}
That small p-value leads us to reject the null hypothesis $\mu_{Morning} =
\mu_{Lunch} = \mu_{Afternoon}$. So there is strong evidence that all three
means are not equal. But which one(s) is(are) different from the other(s)?


If you look at the second piece of output, that from

<<>>=
numSummary(dmit$activ , groups=dmit$ftraining, statistics=c("mean", "sd"))
@ 

you can get an idea: it seems that Morning and Lunch are very similar to
each other, but Afternoon is very different. This agrees with the
impression we got from the boxplot. But we would like a more formal
procedure: we are going to compare all pairs of means and we will take
into account that we are carrying out multiple comparisons (tests of pairs
of means when the ANOVA is not significant are rarely
justified\footnote{Unless some specific, small number of, tests of
  specific pairs had been planned before the experiment.}.




\textbf{Comparing all pairs of means} is done using the ANOVA model, so
the results are not identical to comparing using t-tests (briefly: the
estimate of the variance might be slightly different, and probably
better).

\textbf{Multiple testing corrections} are needed because we are now
conducting three separate tests (in general, if there are $K$ groups and
if you compare all pairs of means you carry out ${K \choose 2} = \frac{K
  (K-1)}{2}$ tests). Here, we will control the family-wise error rate, the
probability of falsely rejecting one or more tests over the family of
tests performed ---three in our case. The logic is somewhat like that of
being struck by lightning: the chances of it happening are extremely
small, but every year people die from lightning because the probability
that at least one person is killed is huge since we have lots of people
exposed to the risk. So even if all null hypotheses for our three tests
are true:
\begin{itemize}
\item $\mu_{Morning} = \mu_{Lunch}$
\item $\mu_{Morning} = \mu_{Afternoon}$
\item $\mu_{Lunch} = \mu_{Afternoon}$
\end{itemize}

if we run the three comparisons, the chances of incorrectly rejecting at
least one of the null hypotheses is larger than, say, 0.05 if we simply
look at each one of the three p-values and keep any with a p-value $\leq
0.05$.  You will see more about multiple testing below
(\nameref{sec:mult-comp-fdr}). 


Before we continue, though, we will use the following two examples. If you understand them, you understand why we need multiple comparisons:

\begin{itemize}
\item Russian roulette. (Disclaimer: we of course strongly discourage playing Russian roulette!). Suppose you have two friends, A and B. ``A'' intends to play Russian roulette once a year, ``B'' intends to play it ten times a year. Whose funeral are you likely to attend first?
\item Lottery: we can give you either one lottery ticket or 20 lottery tickets (with different numbers). Assuming you have no problem with becoming rich, what option do you prefer?
\end{itemize}


Now, the following should be clear:
\begin{itemize}
\item If you test, say, 3 null hypotheses, and each one is true, and you reject any of the hypothesis at the 0.05 level, the probability that you will reject one or more null hypotheses when you shouldn't is larger than 0.05.
\item If instead of testing 3 null hypotheses you test 20, the probability of rejecting one or more when you shouldn't is much larger than if you were just testing 3 null hypotheses.
\end{itemize}

If the above is still unclear, think about why it is more likely ``B'' will get killed first. Or why it is more likely you will become richer if we give you 20 different lottery tickets.




To carry out all pairs of tests and control for multiple testing, carry
out the ANOVA again, but now make sure to click on ``Pairwise comparisons
of means''. To be much more explicit, I will also name the model as
\Robject{AnovaMIT} (entering that in ``Enter name for model'')


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.80\paperwidth,keepaspectratio]{anova-comps.png}
    \caption{\label{pairwise} ANOVA, asking for pairwise comparisons of
      means and naming the model.}
  \end{center}
\end{figure}


A whole bunch of new commands are added to the R Script and a figure is
produced. I'll go over them (but I skip the one that asks for the means
and sd of groups by commenting it out and I also add comments to some
commands):



<<tukey1,fig.cap='Plot of pairwise differences with Tukey contrasts', fig.lp='fig:', fig.width=5,fig.height=5, fig.show='hold', results='hide'>>=
AnovaMIT <- aov(activ ~ ftraining, data=dmit)
summary(AnovaMIT)

## The next I comment out
## numSummary(dmit$activ , groups=dmit$ftraining, statistics=c("mean", "sd"))

## The next two lines carry out the multiple comparisons and the following
## lines plot them
.Pairs <- glht(AnovaMIT, linfct  = mcp(ftraining = "Tukey"))
summary(.Pairs) # pairwise tests
confint(.Pairs) # confidence intervals
cld(.Pairs) # compact letter display
old.oma <- par(oma=c(0,5,0,0))
plot(confint(.Pairs))
par(old.oma) ## restore graphics windows settings
remove(.Pairs) ## remove the object that stored the multiple comp. output
@ 



Note that we can get some or most of that also from a call to
\Rfunction{TukeyHSD}:
<<>>=
TukeyHSD(aov(activ ~ ftraining, data = dmit))
@ 

More details are provided in section \nameref{multcomp-two-way}. A quick explanation
of what is happening with the p-values in
\Burl{https://stats.stackexchange.com/a/488655}.



\subsubsection{The plot of pairwise differences}
Look carefully at the plot in Figure \ref{fig:tukey1}: for each difference
(for each \textbf{contrast}), it shows the estimate and a 95\% confidence
interval around it. The plot title says ``95\% family-wise confidence
interval'', and that indicates that multiple testing correction has been
used. (You might want to make that even more explicit by using a title
such as ``95\% family-wise confidence interval using Tukey contrasts'').

Given how far two of the contrasts are from 0.0, it seems those are highly
significant differences.


Now, answer these questions (we will discuss them in class):

\begin{itemize}
\item If we had constructed \textbf{only} one of the confidence intervals (i.e., if we had not adjusted for multiple testing), that confidence interval would have been:
  \begin{itemize}
  \item narrower
  \item wider
  \end{itemize}

\item If we had constructed, and adjusted for multiple testing 10 confidence intervals instead of three they would have been
  \begin{itemize}
  \item narrower
  \item wider
  \end{itemize}
\end{itemize}


 Let's go now to the numerical output.
\subsubsection{The numerical output}

This

<<>>=
.Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
summary(.Pairs)
@ 

explicitly shows that we are using Tukey's method and it shows the
p-values of each contrast (each comparison), and it makes it clear that we
are being reported adjusted p values. There is strong evidence of a
difference between Afternoon and the other two levels, but no evidence of
differences between Lunch and
Morning. %% Incidentally, note that the whole process is done using ``Tests
%% for the general linear hypothesis'' and how the contrasts are explicitly
%% shown: this is a very general and powerful procedure (see section
%% \nameref{othercontrasts}).



The values that are plotted are generated here:

<<>>=
confint(.Pairs) 
@ 

Note how the usage of Tukey's procedure is again explicit.



\subsubsection{And can I plot the means with s.e from the model?}

Sure. Go to ``Models'', ``Graphs'', ``Effects plots'':

<<out.width='9cm'>>=
## trellis.device(theme="col.whitebg")
plot(allEffects(AnovaMIT), ask=FALSE)
@ 


That shows the estimates for each group and a 95\% confidence interval
(again, based on the whole ANOVA model). But from that figure it is not
easy to tell which pairs differ, especially taking multiple comparisons
into account.


\subsubsection{This is a mess. What figures do I use?}
That is up to you. But this can work: present both the original means
and the plot with the contrasts. You can just copy and paste code
judiciously. %% Note that I modify slightly the title of the first figure, so
%% that the usage of Tukey contrasts is explicit and I modify the title of
%% the second, so we use ``Training'' in the name, not ``ftraining'':


I would actually modify slightly the title of both figures, so that they
look better:

<<out.width='7cm', out.height='7cm', fig.show='hold'>>=
plot(allEffects(AnovaMIT), ask=FALSE, main = "Training: effect plot")
@ 

<<out.width='12cm'>>= 
.Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
tmp <- cld(.Pairs) ## silent assignment 
old.oma <- par(oma=c(0,5,0,0))
plot(confint(.Pairs), 
     main = "95% family-wise confidence interval using Tukey contrasts")
par(old.oma) ## restore graphics windows settings
@ 


%% <<out.width='7cm', out.height='7cm', fig.show='hold'>>=
%% .Pairs <- glht(AnovaMIT, linfct = mcp(ftraining = "Tukey"))
%% tmp <- cld(.Pairs) ## silent assignment 
%% old.oma <- par(oma=c(0,5,0,0), mfrow = c(1, 2))
%% plot(confint(.Pairs), 
%%      main = "95% family-wise confidence interval using Tukey contrasts")
%% ## plot(allEffects(AnovaMIT), ask=FALSE, main = "Training: effect plot")
%% par(old.oma) 
%% @ 

%% \red{FIXME}:


%% \red{This will actually produce two figures!!! I can include
%%   it as one, because of the way I do it from latex}

%% Nope, it was not working.

%% %% An alternative is to use the MMC plots provided by package ``RcmdrPlugin.HH'':

%% %% <<>>=
%% %% @ 





\subsubsection{Side note: Interpreting confidence intervals}
If this is not obvious to you, ask it in class: a figure that shows an
estimate (e.g., a mean) and a 95\% confidence interval, where the interval
goes from, say, 1 to 2, \textbf{should not} be interpreted as saying that
there is a 95\% probability that the mean is between 1 and 2. That is not
the correct interpretation of a confidence interval. Make sure you
understand this!!! (But we have discussed this already in Lesson 2). 




\subsubsection{Multiple comparisons, other contrasts,
  etc} \label{othercontrasts_multcomp} There is a wide literature on methods for
adjusting for multiple comparisons in ANOVA and linear models. And
sometimes a distinction is made between pre-planned and post-hoc
comparisons. Tukey's approach is a widely accepted one (though there are
others) and the distinction between pre-planned and post-hoc does not
arise when researchers directly want to do all possible pairs right from
the beginning. However, many of these issues can become important if you
know, from the start, that some comparisons do not matter to you, and/or
there are many groups. As well, we could be interested in other types of
contrasts, for instance, that the mean of groups 2 and 3 is different from
the mean of group 4. Etc, etc. We will not get into this. 



\subsection{t-test as ANOVA}
Of course, in general, you can just carry out any two-group comparison as
an ANOVA. There is nothing wrong with that (and there is a simple
correspondence between a t statistic and an F statistic).


\subsection{Other ways of obtaining summaries}

You can skip this section. This is a detail for those of you who want to see how to directly get things done in R.

We have used above the \Rfunction{numSummary} function from
\CRANpkg{RcmdrMisc} but we could have done it with other functions in R.
  
For example with \Rfunction{aggregate}

<<>>=
with(dmit, aggregate(activ, list(Training = ftraining), 
                      function(x) c(mean = mean(x), 
                                    sd = sd(x), 
                                    n = sum(!is.na(x)))
                      ))
@ 

or \Rfunction{by}:
<<>>=
with(dmit, by(activ, ftraining, 
              function(x) c(mean = mean(x), 
                             sd = sd(x), 
                             n = sum(!is.na(x)))
              ))
@ 



\subsection{Can you do an ANOVA with only one sample per group?}\label{anova-only-one}
Why or why not?


What happens here?
<<>>=
y <- c(1, 2, 3)
gr <- factor(c("g1", "g2", "g3"))
anova(lm(y ~ gr))
summary(aov(y ~ gr))
@ 



And here?


<<>>=
y2 <- c(1, 2, 3, 4)
gr2 <- factor(c("g1", "g2", "g3", "g3"))
anova(lm(y2 ~ gr2))
summary(aov(lm(y2 ~ gr2)))
@




\subsection{One way ANOVA: summary of steps}

\begin{enumerate}
\item Enter the data.
\item Recode the factor (the independent variable), if needed.
\item Run the model.
\item Assess model diagnostics (see section \nameref{diagnostics}).
\item Carry out comparisons between pairs of means with appropriate
  adjustment for multiple comparisons.
\end{enumerate}



\clearpage


\section{Multiple comparisons: FWER and FDR}
\label{sec:mult-comp-fdr}

\subsection{Family-wise error rate}
\label{sec:family-wise-error}


In section \nameref{multcomp1} we covered multiple comparisons. Here we go
into a little bit more detail before continuing with ANOVA. As in section
\nameref{multcomp1}, suppose we are testing a number of null hypothesis. Table
\ref{table-multcomp} shows a depiction of what we are concerned about,
where the letters in each cell refer to the number of means tested that fall in
each case.



\begin{table}[t!]
\begin{tabular}{p{5.5cm}|>{\centering}p{2.5cm}|>{\centering}p{2.5cm}|}
  \multicolumn{1}{c}{} & \multicolumn{1}{>{\centering}p{2.5cm}}{Null hypothesis not rejected} & 
  \multicolumn{1}{>{\centering}p{2.5cm}}{Null hypothesis rejected}\tabularnewline
  \cline{2-3} 
  Means do not differ ($H_0$ \textit{true}) & U & V\tabularnewline
  \cline{2-3} 
  Means differ ($H_0$ \textit{false}) & T & S\tabularnewline
  \cline{2-3} 
\end{tabular}


% \begin{tabular}{p{5.5cm}p{2.5cm}p{2.5cm}}
%   & Null hypothesis not rejected & Null hypothesis rejected \\
%   \cline{2-3}
%   Means do not differ ($H_0$ \textit{true}) & U &  V \\
%   \cline{2-3}
%   Means differ ($H_0$ \textit{false}) & T  & S  \\
%   \cline{2-3}
% \end{tabular}
\caption{\label{table-multcomp} Multiple comparisons. In rows is the ``truth'' (how things really
  are), and in columns the output from our testing procedure (what we end
  up claiming or believing). The sum of all entries is the total number of
  comparisons made.}

\end{table}







In the example in section \nameref{multcomp1} $U + V + T + S = 3$ (beware, 3
is the number of \textbf{hipothesis tests}, it is not the number of means;
in our case, both are three, but Table \ref{table-multcomp} reflects
number of hypothesis, not tests). Procedures such as the one we used
(Tukey) or Bonferroni or similar ones, try to control the probability that
$V \geq 1$. They control what is called the ``family-wise error rate''
(FWER).

The intuitive idea is: ``I want to control very tightly the probability of
falsely rejecting any hypothesis'', and that is the same as saying ``I
want to control very tightly the probability that $V$ is equal or larger
than 1''. (I use the expression ``control very tightly'' because if we
insist in ``I NEVER want to falsely reject any null hypothesis'' then
$\ldots$ we will never reject any null hypothesis). What Tukey,
Bonferroni, and other procedures for controlling the family wise error
rate provide are mechanisms for ensuring that $Pr(V \geq 1)$ is below a
number you specify (e.g., 0.05).


Note that, in our usage of Tukey, we did not pre-specify that $Pr(V \geq
1)$. The procedure is run, and it gives us ``adjusted p-values''. And what
is an adjusted p-value?  The classical paper from Wright, 1992, ``Adjusted
p-values for simultaneous inference'', \textit{Biometrics}, 48:
1005--1013, has in p.\ 1006 this definition of adjusted p-value: ``The
adjusted $P$-value for a particular hypothesis within a collection of
hypotheses, then, is the smallest overall (i.e., 'experimentwise')
significance level at which the particular hypothesis would be rejected.''
This might sound like a mess, but it really ain't. Think about it. It is
so nice that it makes comparisons very simple, as Wright explains: ``An
adjusted $P$-value can be compared directly with any chosen significance
level $\alpha$: If the adjusted $P$-value is less than or equal to
$\alpha$, the hypothesis is rejected.'' (If you find this too advanced for now, don't worry. But remember this, once you feel more comfortable with these issues. And no, this paragraph will not be in the exam.)




\subsection{False discovery rate (FDR)}
\label{fdr}

There is a different approach to the multiple testing problem. In this
approach we focus on controlling the fraction of false positives. The
total number of null hypothesis we reject is $V+S$. The intuitive idea
behind the control of the false discovery rate (\textbf{FDR}) is to bound
(to set an upper limit to) to the ratio $\frac{V}{V+S}$\footnote{There are
  several different approaches. The most common one is to control $FDR =
  E(Q)$ where $Q=V/(V+S)$ if $V + S > 0$ (and $Q = 0$ otherwise). But
  there are others, such as the $pFDR$, etc.}.


One key difference is that the FDR can be kept reasonably low (say,
0.01) even when it is almost sure that $V\geq 1$. When could this happen?
For instance, when we are conducting tens of thousands of hypothesis
tests. Again, the FDR will control the fraction of false discoveries
whereas the control of the family wise error rate (FWER) is emphasizing
that $V$ don't become 1 or more.


As we did with Tukey and the FWER procedures, we generally do not
pre-specify the level of FDR we want to attain but, rather, we obtain
``adjusted p-values''. The difference in the meaning of ``adjusted'' is
that now these p-values are adjusted for FDR (not adjusted for control of
the family wise error rate).  So, when we deal with FDR, the adjusted
p-value of an individual hypothesis is the lowest level of FDR for which
the hypothesis is first included in the set of rejected hypotheses (e.g.,
Reiner et al., 2003, \textit{Bioinformatics}).


The FDR is usually employed in screening procedures, where we are willing
to allow some false discoveries, because we are screening over thousands
of hypothesis. The cost of requiring $V = 0$ would be to miss many
discoveries. One example? Suppose that you have measured the expression of
20000 genes in two sets of subjects some with colon cancer and some
without.  Now, you can do the equivalent of 20000 t-tests. So you will get
20000 p-values, and you will want to adjust those 20000 tests for multiple
testing. % When you declare some of these genes as ``significant'' under the
% control of the FDR, you are not claiming you have tightly controlled that
% $V = 0$, but rather you are controlling the rate or fraction of false
% discoveries.


How do you adjust for multiple testing in R? This is easily done with the
function \Rfunction{p.adjust} (there is no menu entry in R Commander, so
we will type it by hand). When you are applying FDR you often have a
collection of p-values already. 


I will make a simple example up and will only use four p-values (not
20000) for the sake of simplicity. Suppose we have done a screening
procedure, testing four genes. You get the p-values I show below. To use
an FDR correction method I use \Rfunction{p.adjust} with the
\texttt{method = ``BH''} argument (BH is one of several possible types of
FDR correction). To show what happens, I have then combined the two, side
by side, so you can see the original p-value and the FDR-adjusted one.

<<>>=
p.values <- c(0.001, 0.01, 0.03, 0.05)
adjusted.p.values <- p.adjust(p.values, method = "BH")
cbind(p.values, adjusted.p.values)
@ 


How do we interpret this? Here I will only cover the very basics (if you
want more details, sing up for some of the other classes we teach :-) ). But go back a couple of
paragraphs, and re-read the definition of adjusted p-value for the
FDR. So, for example, if we keep as ``significant'' all the genes with a
p-value (not adjusted p-value, but p-value, so the last first three) $\leq
0.030$, the FDR (the expected number of false discoveries) will be 0.040
(the FDR-adjusted p-value for the gene with $p-$value of 0.03).




Note that the FDR applies not just to comparisons between means or
t-tests, but to any kind of test (comparing variances, correlations, etc).





\subsection{Multiple comparisons: struck by lightning}

This has been a short section, because we are skipping the technicalities
and focus just on the big ideas. But this is a \textbf{VERY IMPORTANT}
section to remember. When you do many tests, some of them might have low
p-values just by chance and you need to adjust for this. If any gene with
a low p-value is declared significant (regardless of the size of the
collection of tests) you will be likely to start claiming that many purely
chance results are ``significant''. And you do not want that.


Remember that very rare events do happen, and they are almost certain to
happen if the experiment is repeated many times (by the way, this is why
most of us are not afraid of dying from lighting, even if every year some
people do in fact die from lightning).


When you screen 20000 genes, you are running 20000 times the experiment of
the p-value and the null hypothesis. And remember the rules: for one true
null hypothesis, the probability of finding a p-value $\le 0.05$ is
$0.05$. Now imagine you do that 20000 times; you are almost certain to
have many p-values $\le 0.05$. (Same thing with lightning: even if the
chances of dying from lightning are $\le \frac{1}{300000}$, with millions
of people on earth, some are almost sure to die from lightning).


There are many reviews about multiple testing, FDR, etc. You might want to
take a look at a three-page one by W.\ Noble, in \textit{Nature
  Biotechnology}, 2009, 27: 1135--1136, ``How does multiple testing work''.


\clearpage
% FIXME: start first without interactions!!! Use the data from orderfactors
%% we can't if we use Rstudio
\section{Two-way ANOVA}

A key issue in models with two or more predictor variables (such as two-way
ANOVA) are interactions. What is an interaction? If this is not clear, we will
explain it in class (but you might want to recall what ``epistasis'' is, for
example, or what is the deal with the ``Interactions'' section in the leaflet
(prospecto) of any drug you take).


\subsection{A very simple two-way ANOVA}\label{simpletwo}

Let us create some fake data

<<>>=
set.seed(3)
df1 <- data.frame(y = runif(8),
                  A = rep(c("a1", "a2"), 4),
                  B = rep(c("b1", "b1", "b2", "b2"), 2))


df1
@ 

Some summaries of data:
<<>>=
(means <- with(df1, tapply(y, list(A, B), mean)))
@ 

And now, several ANOVA models. We will explain each of the terms in turn
in class if this is not familiar to you:

\subsubsection{No interaction model}
<<>>=
m1 <- lm(y ~ A + B, data = df1)
anova(m1)
@ 

The above is the anova table; we will say many more things later about
that. 


But for now notice also this output, that gives the estimated coefficients:
<<>>=
summary(m1)
@ 

We will explain it in more detail later (\nameref{anovaaslm}), but for now
remember we are using an additive model, so here we are estimating only
three things from a four-means design.  What three
things? %% And how do we interpret the
%% coefficients? We are using \texttt{contr.treatment}, the default in R,
%% that compares each mean against the reference, the first level (``a1b1''
%% here, but that is not just the first cell; see below if you want). As I
%% said, we will get back to this later (\nameref{anovaaslm}).


We will not emphasize understanding the coefficients in this class; we just wanted to show them to you, so you can see we are actually estimating three things. More details in \nameref{anovaaslm}, but those details \textbf{are not} needed.


\subsubsection{Interaction model}

Now all cell means are modeled:

<<>>=
m2 <- lm(y ~ A * B, data = df1)
anova(m2)
@ 


Again, we could ask for the estimated coefficients:
<<>>=
summary(m2)
@ 




Check the interpretation of the interaction coefficient:
<<>>=
means[2, 2] - 
    (means[1, 1] + coefficients(m2)[2] + coefficients(m2)[3])
@ 



Again: we will not emphasize understanding the coefficients in this class; we just wanted to show them to you, so you can see we are actually estimating four things (in the previous example, we were estimating three). More details in \nameref{anovaaslm}, but those details \textbf{are not} needed.



But let's do the following thought experiment. Suppose you have a two-way ANOVA. The first factor has four levels, the second factor has seven levels.
\begin{itemize}
\item How many coefficients will you be estimating in a model without interactions?
\item How many coefficients will you be estimating in a model with interactions?
\end{itemize}

% \subsubsection{One observation per cell}
% \label{sec:one-observation-per}


% What if we only had one observation per cell?

% We can fit the additive model (but only 1 df left)
% <<>>=
% df2 <- df1[1:4, ]
% m3 <- lm(y ~ A + B, data = df2)
% anova(m3)
% summary(m3)
% @ 

% But we can't really fit the interaction model:
% <<>>=
% m4 <- lm(y ~ A * B, data = df2)
% anova(m4)
% summary(m4)
% @ 



%% \subsubsection{What is the overall intercept with \texttt{contr.treatment}
%%   in the no-interaction model?*}

%% If you recall the model we write, then, if we look at the cells, with
%% \texttt{contr.treatment}, the intercept (the abstract ``a1b1'' reference
%% level) is either:

%% Use the first row, all those that are ``a1''
%% <<>>=
%% 1/2 * (means[1, 1] + (means[1, 2] - coefficients(m1)["Bb2"]))
%% @ 

%% or the first column, those that are ``b1''

%% <<>>=
%% 1/2 * (means[1, 1] + (means[2, 1] - coefficients(m1)["Aa2"]))
%% @ 

%% (I am keeping things simple here; we could have looked at
%% \Rfunction{model.matrix}, etc, too)

%% \subsubsection{Other types of coding: \texttt{contr.sum}}

%% (We will use \Rfunction{contr.Sum} from \CRANpkg{car}, since clearer labeling)

%% <<>>=
%% opt <- options(contrasts = c("contr.Sum", "contr.poly"))
%% m11 <- lm(y ~ A + B, data = df1)
%% anova(m11)
%% summary(m11)
%% @ 


%% <<>>=
%% (overallMean <- mean(df1$y))
%% mA <- with(df1, tapply(y, A, mean))
%% mA - overallMean

%% mB <- with(df1, tapply(y, B, mean))
%% mB - overallMean
%% @ 

%% Interaction now (note the estimates!)
%% <<>>=
%% m12 <- lm(y ~ A * B, data = df1)
%% anova(m12)
%% summary(m12)
%% @ 


%% What if we changed the reference?
%% <<>>=
%% summary(lm(y ~ A + B, data = df1b))
%% @ 

%% Return contrasts to usual state
%% <<>>=
%% options(opt)
%% @ 


%% FIXME: should use an example with, say, four diets and 2 or 3 drugs
%% so that when I do not use a model with  interaction, some terms
%% more than 1 df



\subsection{Loading the cholesterol data set}\label{twoway}

The following data come from an experiment about the effects of three
diets and two cholesterol-controlling drugs in the reduction of
cholesterol levels (note: the response variable is change in cholesterol,
so the larger the value, the larger the reduction of cholesterol). As
usual, read the data and look at them. Since the author used names for the
levels within each of the two factors, Diet and Drug, we do not need to
transform them into factors, in contrast to what we did in section
\nameref{recode}. Call the data \Robject{dcholest}.

(Note: since a few versions ago, R no longer converts characters to factors when
reading with \Rfunction{read.table}. In some places that is inconsequential, in
other places we will have to use factors. We will deal with it.)


<<create_tooth, echo=FALSE, results='hide'>>=

set.seed(45)
n <- c(4, 7, 9, 5, 7, 8)
m <- c(2.1, 1, 2, -.2, 4, 5)
y <- round(unlist(mapply(function(x, y) rnorm(x, y), n, m)), 3)
Drug <- c(rep("A", sum(n[1:3])),
          rep("B", sum(n[4:6])))
Diet <- rep(rep(c("HF", "M1", "M2"), 2), n)
chol <- data.frame(y = y,
                   Drug = Drug,
                   Diet = Diet)
rm(y, n, m, Drug, Diet)
## lm1 <- lm(y ~ Drug * Diet, data = chol)
## lm0 <- lm(y ~ Drug + Diet, data = chol)
## anova(lm1)
## Anova(lm1)
## anova(lm0)
## anova(lm(y ~ Diet + Drug, data = chol))
## Anova(lm0)
write.table(chol, file = "Cholesterol.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(chol)

@ 
<<>>=
dcholest <- read.table("Cholesterol.txt", header = TRUE)
@ 


\subsection{Fitting a two-way ANOVA}\label{twowayfit}

As usual, go to ``Statistics'', ``Means'', and then ``Multiway
ANOVA''. Call it ``cholanova''

Look at the output, which I comment below
<<>>=
## This fits the model. Pay attention to the "*"
cholestanova <- (lm(y ~ Diet*Drug, data=dcholest))
## This shows the ANOVA table. Notice the "Type II"
Anova(cholestanova)

## Now we are shown the 3 by 2 table of means, standard deviations, and number 
## of observations
tapply(dcholest$y, list(Diet=dcholest$Diet, Drug=dcholest$Drug), 
       mean, na.rm=TRUE) # means
tapply(dcholest$y, list(Diet=dcholest$Diet, Drug=dcholest$Drug), 
       sd, na.rm=TRUE) # std. deviations
tapply(dcholest$y, list(Diet=dcholest$Diet, Drug=dcholest$Drug), 
       function(x) sum(!is.na(x))) # counts
@ 


\subsection{Interactions}\label{2way-int}

Now, go to the ``Models'' menu and do a ``Effects plot''

<<out.width='12cm', out.height='12cm'>>=
## trellis.device(theme="col.whitebg")
plot(allEffects(cholestanova), ask=FALSE)
@ 


What do you see? Do you understand what an interaction is? Do you see it
in the plot? Basically, an interaction means that the effect of one
variable depends on the effect of the other. In this case, even if Drug B
overall leads to a larger change (decrease) in cholesterol, its effects
depend on the Diet. This has practical consequences: is Drug B a better
drug?  It depends on the diet of the patient: for the HF (high fat) diet,
Drug B is clearly worse than Drug A.

Interaction is also called ``non-additivity'' because the model deviates
from a simple model like
\[ y = Drug + Diet\]

as the effect of Drug depends on the value of Diet (or the other way
around). The phenomenon of interaction should be familiar to you: it is
very common in life in general, and in biology you might have previously
seen at as epistasis in genetics.



You can also see interaction plots using this directly from R:

<<out.width = '8cm', out.height = '8cm'>>=
with(dcholest, interaction.plot(Drug, Diet, y, type = "b"))
@ 

or using the \CRANpkg{RcmdrPlugin.HH} and going to ``Graphs'',
``Plot of two-way interactions'' %this from package ``HH'' (I will first load it)

<<out.width='14cm', out.height='14cm'>>=
interaction2wt(y ~ Diet + Drug, data=dcholest)
@ 

Notice how this last figure displays both main effects and interactions. So
even if the main effect of Drug B is to lead to a larger change in
cholesterol (as you can see in the upper right panel), Drug B actually
leads to much smaller change in cholesterol if given to patients with diet
HF (as seen in the bottom right panel). In fact, under Drug A, it seems
that diet HF is actually slightly better than diet M1 (as seen in the
bottom right panel or in the effects plot).

Finally, a boxplot can also help show the interaction. I will use two
different ones, that differ by the order in which factors are specified
(one or the other might be easier to decode visually):

<<out.width='8cm', out.height='8cm', fig.show='hold'>>=
boxplot(y ~ Drug * Diet, data = dcholest, col = c("salmon", "gold"))
boxplot(y ~ Diet * Drug, data = dcholest, col = c("salmon", "gold", "lightblue"))
@ 



Given these results (the strong interaction, that can even revert effects
of one factor), it makes little sense to report any global main effects
and we would rarely be interested in interpreting the significance (or
not) of the Diet or Drug term. In general, \textbf{in the presence of
  interactions, we often refrain from interpreting main
  effects; this is often referred as the ``marginality principle''}\footnote{Properly formulated, which also generally involves
  using other types of contrasts ---such as contr.sum, in R parlance---
  marginal tests in the presence of interactions, what are called Type
  III, can make sense, but are not always of interest. See also footnote
  \ref{fn:2} in section \nameref{orderfactors} for some entries and
  references.}. We return to this later.


\subsection{An ANOVA without interactions}\label{anovanoint}
Could we fit a model without interactions? Yes, of course, but not from the
the above menu. The idea is to change the ``*'' by a ``+'' (and we will
see that again when we deal with multiple regression et al. in section
\nameref{regr-int}).

<<>>=
amodelnoint <- (lm(y ~ Diet + Drug, data=dcholest))
Anova(amodelnoint)
@ 


There are, however, good reasons to start fitting a model \textbf{with}
interactions first, and \textbf{only} if there are no interactions, fit a
simpler, additive model.


\subsection{Getting ready for how things change with the order of
  factor}\label{before-orderfactors}

The next section introduces a common phenomenon that is sometimes surprising. We
will try to provide some intuition about why this phenomenon happens.

These are the key steps of the argument (take a piece of paper, and draw a
bunch of two-by-two boxes with the sample sizes in each cell; \textbf{really, do this now}):

\begin{enumerate}
\item Suppose you do a two-way ANOVA where the dependent variable (Y) is ``awake
  in class'' and your predictor variables are sex (female or male) and coffee in
  the morning (yes or no).
\item Suppose you have, in the sample, 10 women, all of whom drank coffee, and 10
  men, none of whom drank coffee. Can you say anything about the effect of sex
  that is not saying something (or even everything) about the effect of coffee
\item Now, suppose instead that the design is perfectly balanced: 5 women who
  drank coffee, 5 women who did not drink coffee, 5 men who drank coffee, 5 men
  who did not drink coffee. If I told you ``I measured a woman'', do you know if
  she is also a coffee drinker or not?
\item Now, think about intermediate scenarios.
\item Was the above a silly example? OK, replace the Y by ``cardiovascular
  disease'' and the predictor variables by ``smoking'' (yes and no) and
  ``exercise'' (yes and no). Can you say anything about the effects of exercise
  if all the people in your sample who exercise are also non-smokers? (Repeat
  this with other pairs of variables, such as diet and exercise, gene expression
  and age, gene expression and sex, etc, etc).
\end{enumerate}


Some of this might still seem mysterious after the above exercise. Think about
doing a regression of body height on the length of both the left and right arms
(we will actually do something very similar later: \qref{multregr-example}). And
think about how unbalanced data, with categorical independent variables, is
somewhat similar to inducing a correlation between variables. We will discuss
this in class.

Anyway, let's go and see this happening!


\subsection{The order of factors}\label{orderfactors}

Let's pretend there are no interactions. We can do that by creating a data
set without the ``HF'' subjects. Go to ``Active data set'', ``Subset
active data set''. 



\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.40\paperwidth,keepaspectratio]{subset-chol.png}
    \caption{\label{subset} Subset a data set. Note the ``Subset expression'.}
  \end{center}
\end{figure}


<<>>=
dcholest2 <- subset(dcholest, subset=Diet != "HF")
@ 

Now do a two-way ANOVA:

<<>>=
cholest2anova <- (lm(y ~ Diet*Drug, data=dcholest2))
Anova(cholest2anova)
@ 

So no evidence whatsoever of interactions. For simplicity, we can go and
refit the model without the interaction, and that we do in ``Statistics'',
``Fit models'', ``Linear model''. We will actually fit two models, which
differ only by the order in which we give Diet and Drug in the formula (we
showed this already in Lesson 2) and we will call them lm1 and lm2

<<>>=
lm1 <- lm(y ~ Diet + Drug, data=dcholest2)
lm2 <- lm(y ~ Drug + Diet, data=dcholest2)
@ 


Now, go to ``Models'', ``Hypothesis tests'' and do ``ANOVA table (Type I
sums of squares)'' (if you do all of this via the GUI and mouse, remember
to change the model clicking on ``Models'', ``Select active model'' or via
the box that says ``Model:''):

<<>>=
anova(lm1)
anova(lm2)
@ 

As you can see, the F statistic and the p-value are different!!! What
gives here?

Now, use Type II sums of squares:

<<>>=
Anova(lm1)
Anova(lm2)
@ 

Nothing changes between those two. But if you look carefully, the F value
(and p-value) of the Type II Sums of Squares ANOVA table are the same as
those for the term that enters last in the Type I (those produced via
\Rfunction{anova}, without a capital ``A'').


By the way, R might tell you something might be going on in here if you look
carefully:

<<>>=
aov1 <- aov(y ~ Diet + Drug, data=dcholest2)
aov1 ## Notice the "Estimated effects may be unbalanced"
@ 


This is an \textbf{extremely common phenomenon} when the design is not
perfectly balanced (with categorical independent variables) or there are
correlations (with continuous covariates, as in regression). What is
happening?

\begin{itemize}
\item Type II sums of squares (similar to t-statistics from a linear
  model) show what that term contributes, \textbf{given all the rest} are
  already in the model (``given all the rest'': all the rest that do not
  include this term, so no interactions with this term). In other words, given all the other terms (that do
  not include this term) have already been taken into account. This is
  actually the output we would get from comparing two models, one with all
  terms, and one with all terms except the term in question. (Always
  assuming interactions with the term in question are zero). The package
  \CRANpkg{car}, by default, gives you this via \Rfunction{Anova}. I
  routinely use \Rfunction{Anova}.
  
\item Type I (or sequential) sums of squares do not. They are sequential,
  in the order shown in the output. R, by default, gives you this via
  \Rfunction{anova}.

\item {\footnotesize (More details about Type I and Type II Sums of Squares are
    provided in the Appendix in \qref{ssI-to-III}, but you can skip it. This
    issue is also discussed, by focusing on the key issues from an applied
    perspective, in chapter 5 of Fox and Weisberg's ``An R companion to applied
    regression, 3rd edition''.)}

\end{itemize}


And this, of course, affects models with two, three, \ldots
factors. Always pay attention to what the program is giving you, and
beware that the defaults used by R need not the same as those used by
SPSS, SAS, etc.\footnote{More details about Type I and Type II Sums of Squares are
    provided in the Appendix in \qref{ssI-to-III}, but you can skip it. This
    issue is also discussed, by focusing on the key issues from an applied
    perspective, in chapter 5 of Fox and Weisberg's ``An R companion to applied
    regression, 3rd edition'' and Fox's ``Applied regression analysis and
    generalized linear models''. An
  email-length discussion of these topics can be found here
  \Burl{https://stat.ethz.ch/pipermail/r-help/2006-August/111854.html} and
  \Burl{https://stat.ethz.ch/pipermail/r-help/2006-August/111927.html}.\label{fn:2}}

Some of this might still seem mysterious. Think about doing a regression
of body height on the length of both the left and right arms. And think
about how unbalanced data, with categorical independent variables, is
somewhat similar to inducing a correlation between variables. We will
discuss this in class. 





%% \red{FIXME}
%% \red{Explain why order of factors matter with unbalanced designs and how
%%   it relates to correlation of indep. vars. with regression. Example of
%%   hegith on left and right arms}


\subsection{Does order always matter?}\label{ordermatter}
Nope. When the design is balanced, order does not matter\footnote{Technically,
  orthogonality of the design matrix does not require identical cell counts; if
  row/column counts are proportional, for instance, we should be OK.  For
  simplicity, many of our examples when order does not matter use 
  balanced examples.}.

If you want a very simple message: \textbf{unless you know what your are doing
  and/or you know your data fulfills certain properties, always expect order to
  matter}.

In section \qref{ordermatter-appendix} we provide more details and examples. 



\subsection{One observation per cell}\label{one-obs-cell}


Briefly: try not to do this. You will not be able to test interactions.

So far, we have had more than one observation for each combination of
levels (i.e., more than one observation per cell, where cell means each of
the ``places'' or ``boxes'' in a table that shows the combinations of
treatments). What if we only had one?


Let us simulate some data:

<<>>=

set.seed(3)
df1 <- data.frame(y = runif(6),
                  A = rep(c("a1", "a2", "a3"), 2),
                  B = rep(c("b1", "b2"), rep(3, 2)))
df1

@ 

Some summaries of data:
<<>>=
(means <- with(df1, tapply(y, list(A, B), mean)))
@ 


We can fit the additive model
<<>>=
m1 <- lm(y ~ A + B, data = df1)
anova(m1)
summary(m1)
@ 

But we can't really fit the interaction model:
<<>>=
m2 <- lm(y ~ A * B, data = df1)
anova(m2)
summary(m2)
 
@ 


Do you understand what is going on?


\subsection{ANOVA/linear models with more than two factors}\label{more-than-two}
We will present no examples, but life is filled with them. Think about
cholesterol: to the experiment with drug and diet add a third factor:
an exercise program. Or maybe a fourth factor too: a stress reduction
program. Or maybe \ldots.



Suppose we have a model with three factors, so a three-way ANOVA, with factors U, V, W. Before we continue, please write down the rows of the ANOVA table; do not fill up the numbers of SS, etc, but write down the rows. For example, one row will have ``V'', another ``U:V:W'', etc, etc. Fill up also the column with degrees of freedom, assuming that U has two levels ($U_1$, $U_2$), V has three, and W has four. Really, write down that table.

Make sure you can understand why we have this table and what each row would be testing.


Suppose we find out that:
\begin{itemize}
\item There is no evidence of three-way interaction.
\item There is no evidence of interaction U-V.
\item There is no evidence of interaction U-W.
\item Only the interaction V-W is significant.
\end{itemize}

So that the next exercise is easy to do, make an annotation as ``highly signficant'' or ``obviously not significant'' in the interactions according to what we wrote above (e.g., the row with the U-V-W interaction with be labelled ``obviously not significant''.)


Now, following what explained before (``the marginality principle''), where we said (\qref{2way-int}) ``in the 
presence of interactions, we often refrain from interpreting main effects'', we will not test main effects that are marginal to any significant interaction. This means that:

\begin{itemize}
\item We will only examine the main effect of U.
\item We will not examine the main effect of V, because there is a significant interaction V-W; in other words, the effect of V changes with the level of W.
\item We will not examine the main effect of W, because there is a significant interaction V-W; in other words, the effect of W changes with the level of V. (This is exactly the same as in the previous line).
\end{itemize}


Let us repeat what we just did to make sure we understand what is going on. Write down what rows we would see in the ANOVA tables for the following cases; write down the terms and the d.f. 

\begin{itemize}
\item An experiment with three factors, A, B, C. A has 3 levels, B has four, C has five. We fit a model without any interactions.
\item As above, but the model is one with all possible interactions.
\item As in the first case, but the model is one with interactions between A and B, and interactions between A and C, and interactions between B and C (but no three-way interaction).
  \begin{itemize}
  \item Now, suppose the A-C interaction is NOT at all significant. What main effects can you test?
  \item Now, suppose the A-C and A-B interactions are NOT at all significant. What main effects can you test?
  \end{itemize}
\end{itemize}



\subsection{Multiple comparisons of means in two-way ANOVA}\label{multcomp-two-way}
% Can they be done? Yes. You can use the \Rfunction{glht} or the TukeyHDS
% functions directly. Or you fit the model using \Rfunction{aov} (not
% \Rfunction{lm}), and use the package \CRANpkg{RcmdrPlugin.HH}, and ask for
% the ``MMC plot''. We will not pursue this any further here (among other
% questions you should ask yourself, at what levels of one variable will you
% be comparing the other? How does lack of balance affect the contrasts? Do
% you really want all possible contrasts?) A good place to start reading on
% these issues is chapter 14 (and section 5.3.2) of Everitt and Hothorn's
% ``A handbook of statistical analysis using R, 2nd ed''.

% %% For instance, this would work. But this is not necessarily what you might
% %% really want:

% %% <<>>=
% %% cholaov <- aov(y ~ Diet * Drug, data = dchol)
% %% chol_tukey <- TukeyHSD()
% %% @ 


% Using the example from the help of mmc.



 
% % <<>>=
% % data(display)
% % display_int <- aov(time ~ emergenc * panel, data=display)
% % display_add <- aov(time ~ emergenc + panel, data=display)

% % display_int.mmc <- mmc(display_int, focus = "panel")
% % display_add.mmc <- mmc(display_add, focus = "panel")

% % display_int.mmc2 <- mmc(display_int,
% %                         linfct=mcp(panel="Tukey",
% %                                    `interaction_average`=TRUE,
% %                                    `covariate_average`=TRUE))

% % display_add.mmc2 <- mmc(display_add,
% %                         linfct=mcp(panel="Tukey",
% %                                    `interaction_average`=TRUE,
% %                                    `covariate_average`=TRUE))




% % @ 
   

This section is way too long for this course. We will cover it quickly, focusing
on the key conceptual issues. But we leave the code, examples, and more advanced
comments for your future reference. Why is this that long, then? Because we want
to emphasize that \textbf{``p values are just not enough''}: in many/most cases,
you will want to look at confidence intervals and other measures of uncertainty
around the parameters you have estimated and the contrasts (differences in means)
of interest. And, of course, you will need to correct for multiple testing when
you, well, conduct multiple tests.


In two-way and, more generally, multi-way ANOVA, this is more complicated. Among
other questions you should ask yourself, at what levels of one variable will you
be comparing the other? How does lack of balance affect the contrasts? Do you
really want all possible contrasts? A good place to start reading on these issues
is chapter 14 (and section 5.3.2) of Everitt and Hothorn's ``A handbook of
statistical analysis using R, 2nd ed'', and then the documentation of \CRANpkg{multcomp}.

\subsubsection{Multiple comparisons in multi-way ANOVA: main messages}
\label{sec:mult-comp-multi}

Again, it is the main messages that you need to understand. You can skip sections \ref{mult-comp-2-way-no-int} and \ref{mult-comp-2-way-int} and use them as reference when you need it. (Nope, those two sections will not be in the exam).

\begin{itemize}
\item In two-way (or three-way or \ldots, generally, multi-way) ANOVA
  \textbf{without interactions} carrying out multiple comparisons between pairs
  of means is straightforward.
  \begin{itemize}
  \item You might need to pay attention to which procedure (function) you use
    when the data are unbalanced.
  \end{itemize}
\item When there are interactions, this is more complicated. For example, at what
  level of each variable do you want to estimate the differences in the other? A
  very simple numerical example illustrating the problem is shown in \qref{ci-two-way-int-example}.
\item Finally, in both cases, you might want to think if carrying out comparisons
  between \textbf{all} pairs of means is what you really want to do. (Of course,
  you \textbf{must} report what you really did: carrying out comparisons between
  all pairs and reporting only a few is incorrect).
\end{itemize}


\paragraph{Comparisons between means when there are interactions: a simple
  example of why we need to specify at what level of the other variable/of the
  interaction}\label{ci-two-way-int-example}


This is an intuitive explanation of the problem (which is what we want for this
class). Suppose we have the model

\verb@Y ~ A * B@

where both A and B have two levels (A: a1, a2; B: b1, b2)


Now, suppose these are the means of each of the cells (e.g., 3 is the average of
observations that have a1 and b1):


\begin{tabular}[h]{c|c|c}
  \hline
  Level of A & Level of B & Mean \\
  \hline
  a1 & b1 & 3 \\
a1 & b2 &  5 \\
a2 & b1 &  8 \\
a2 & b2 &  2\\
  
  \hline
\end{tabular}

Please, draw a figure (by hand) with those means, like any of the interaction
plots we have seen in class.  Please, \textbf{really draw that figure}. Actually,
\textbf{draw 2 figures}: the first will have A in the X (abscissas) axis, the
second will have B in the X axis. You will see that in both plots the lines
clearly cross.


Now, suppose we want this: "95\% confidence interval for the difference between the
means of a1 and a2". Sounds simple enough but ...

... at what level of B do you want that? Because, even without thinking about the
confidence interval, the difference between the means of a1 and a2 (a1 - a2) is:

\begin{verbatim}
3 - 8 = -5   at b1
5 - 2 =  3   at b2
\end{verbatim}


Now, please repeat the above for B. \textbf{Really, do it now}: compute the
difference between b1 - b2 at each level of A.



So "what is the difference between a1 and a2" depends on the level of B (and this
is not news: this is because there is interaction). Thus, returning a confidence
interval (CI) for that difference requires us to specify at what level of B we
want the CI. And the same thing happens if we want a CI for the difference
between the two levels of B (at what level of A do we want it?).


But then, how do we even see plots for CI under interactions
(\qref{mult-comp-2-way-int})?  Because it is possible to return that CI at
different levels of the other factors. We just need to say "at this level of the
other factor". For example at "the average" (because we can define the "average
effect of A" and "the average effect of B", and "the average effect of the
interaction", in a model with interactions). But computing the CI at this
"average" might not be what you want to do. With data that look like the one
above, with such extreme crossing of lines, that might not make much sense.
Alternatively, you might compute the difference and confidence interval at some
other level of the other factor that is more relevant for your question.  Whether
or not it makes sense to compute the CI at the average level of the interaction,
or at what level it makes sense to do it is \textbf{highly context dependent}:
this depends on how strong the interactions are (e.g., are effects being reverted, as in this example?), and what your scientific question is.
Regardless, how that average of B/average of A/average of the interaction is
defined and computed, how the average difference at the average of the other
variable is computed, how that is done in R, how to specify other levels, etc, do
not matter to us here.


What matters is understanding that, with interactions, asking a question about
"the difference between levels of A" (or "a confidence interval around the
difference between levels of A") cannot be answered without saying something
about the level of B at which we want to compute that difference.

And this, by the way, is strongly related to the ``marginality principle'' we have already discussed twice (\ref{2way-int}, \ref{more-than-two}).

Now you can skip sections \ref{mult-comp-2-way-no-int} and \ref{mult-comp-2-way-int} and continue to section \ref{nonparanova}.



\subsubsection{Multiple comparisons in two-way ANOVA: example with no
  interactions}\label{mult-comp-2-way-no-int}

I will show a few examples below. % Let us show confidence intervals for the
% comparisons of all pairs of means.
We will use function \Rfunction{mmc} from package \CRANpkg{HH} as well as
function \Rfunction{glht} from package \CRANpkg{multcomp}. I will skip most of
the syntax here. 

[We could try doing some of the stuff below with menus, but they quickly become
limiting, so I'll directly write R commands (that you can evaluate from R
commander, or from RStudio or from the R console itself). (If you wanted to use
the R commander menus for some of the stuff below, you might want to fit the
model using \Rfunction{aov}, not \Rfunction{lm}, and you will use the package
\CRANpkg{RcmdrPlugin.HH}, and ask for the ``MMC plot''.)]


For the sake of illustration, let's pretend (even when we know it is wrong!!!)
that there is no interaction between Drug and Diet in the cholesterol data.

<<>>=
## We must make sure we are using factors with mmc
dc2 <- dcholest
dc2$Drug <- as.factor(dc2$Drug)
dc2$Diet <- as.factor(dc2$Diet)
clmA <- lm(y ~ Diet + Drug, data = dc2)
cholA_mmc <- mmc(clmA, focus = "Diet")
cholA_mmc

mmcplot(cholA_mmc)
@ 

In this two-way ANOVA we might want to compare, simultaneously, the levels of
Diet and Drug (see the documentation of \CRANpkg{multcomp} (in particular,
\Burl{https://cran.r-project.org/web/packages/multcomp/vignettes/multcomp-examples.pdf}).


<<>>=
KA1 <- glht(clmA, mcp(Diet = "Tukey"))$linfct
KA2 <- glht(clmA, mcp(Drug = "Tukey"))$linfct
clmA_glh <- glht(clmA, linfct = rbind(KA1, KA2))

summary(clmA_glh)

## Note that these are wider than the ones above, as we should expect.
confint(clmA_glh)

plot(clmA_glh)
@

You can also obtain the above using \Rfunction{TukeyHSD} (there are further
comments about the differences below ---bottom line here: when sample sizes are
not very unbalanced, the results will be very, very similar)\footnote{Though the
  plots might not look as nice or you might need to do more work to put all plots
  on the same page}. % We need to use an
% ``aov'' model.

% <<>>=
% clmAaov <- aov(y ~ Diet + Drug, data = dc2)
% plot(TukeyHSD(clmAaov, which = c("Diet", "Drug")))
% @ 


You can stop reading here, but remember that there is extra material below if you
needed it. 


\subsubsection{Multiple comparisons in two-way ANOVA: example with interactions}\label{mult-comp-2-way-int}

But we know there are interactions. Let us use that model to compare levels of Diet.


<<>>=
clm <- lm(y ~ Diet*Drug, data = dc2)
chol_mmc <- mmc(clm, focus = "Diet")

chol_mmc
mmcplot(chol_mmc)
@ 

What is happening with the interactions? If you read the help of \Rfunction{mmc}
you will see it mentions options \texttt{interaction\_average} and
\texttt{covariate\_average}.

What if we set those to false? Notice the warning, and see the differences!

<<>>=
chol_mmc2 <- mmc(clm,
                 linfct = mcp(Diet = "Tukey",
                              `interaction_average`= FALSE,
                              `covariate_average` = FALSE))
chol_mmc2

mmcplot(chol_mmc2)
@ 



Using directly \Rfunction{glht} to compare both Diets and Drugs:

<<>>=
K1 <- glht(clm, mcp(Diet = "Tukey"))$linfct
K2 <- glht(clm, mcp(Drug = "Tukey"))$linfct

K1B <- glht(clm, mcp(Diet = "Tukey",
                     interaction_average = TRUE,
                     covariate_average = TRUE))$linfct

K2B <- glht(clm, mcp(Drug = "Tukey",
                     interaction_average = TRUE,
                     covariate_average = TRUE))$linfct


confint(glht(clm, linfct = rbind(K1, K2)))
confint(glht(clm, linfct = rbind(K1B, K2B)))

## Compare with previously computed ones
## A few change sign as we do HF - M1 instead
## of M1 - HF
## chol_mmc2
## chol_mmc
@ 




% Of course, life is a lot simpler in models without interactions. For the sake of
% illustration (even when we know this is wrong!)


% <<>>=
% clmA <- lm(y ~ Diet + Drug, data = dc2)
% cholA_mmc <- mmc(clmA, focus = "Diet")

% cholA_mmc2 <- mmc(clmA,
%                  linfct = mcp(Diet = "Tukey",
%                               `interaction_average`= FALSE,
%                               `covariate_average` = FALSE))

% cholA_mmc
% cholA_mmc2
% @ 


%% Kind of trivial, as it is the output of the model itself
% First we use the data set where we removed HF. Now, we compare Drug.

% <<>>=
% dc3 <- dcholest2
% dc3$Drug <- as.factor(dc3$Drug)
% dc3$Diet <- as.factor(dc3$Diet)

% clmA <- lm(y ~ Diet + Drug, data = dc3)
% cholA_mmc <- mmc(clmA, focus = "Diet")

% cholA_mmc2 <- mmc(clmA,
%                  linfct = mcp(Diet = "Tukey",
%                               `interaction_average`= FALSE,
%                               `covariate_average` = FALSE))

% cholA_mmc
% cholA_mmc2

% @ 


But in models with interactions, we might be interested in doing other
things. For example, comparing the levels of Drug within levels of Diet. There
are examples in the documentation of \CRANpkg{multcomp} (see
\Burl{https://cran.r-project.org/web/packages/multcomp/vignettes/multcomp-examples.pdf}).

Or comparing, simultaneously, the levels of Diet and Drug (again, see the
vignette):


<<>>=

K1 <- glht(clm, mcp(Diet = "Tukey"))$linfct
K2 <- glht(clm, mcp(Drug = "Tukey"))$linfct


K1A <- glht(clm, mcp(Diet = "Tukey",
                     interaction_average = TRUE,
                     covariate_average = TRUE))$linfct

K2A <- glht(clm, mcp(Drug = "Tukey",
                     interaction_average = TRUE,
                     covariate_average = TRUE))$linfct


summary(glht(clm, linfct = rbind(K1, K2)))
summary(glht(clm, linfct = rbind(K1A, K2A)))

## Compare with previously computed one
chol_mmc2
chol_mmc
@ 


Note that many of those can be obtained, too, using the \Rfunction{TukeyHSD}
function. For example, as simple as this:
<<tukey_multcomp,fig.width=7, fig.height=5,out.width = '12cm', out.height = '10cm'>>=
## Fit as aov
clmaov <- aov(y ~ Diet * Drug, data = dc2)

## Compare to, for example, chol_mmc
TukeyHSD(clmaov, which = "Diet")

## We want all possible contrasts for all combinations
TukeyHSD(clmaov, which = "Diet:Drug")

## Plot them
## But make sure y-axis labels are horizontal
## and y-axis labels fit
op <- par(las = 1, mar = c(5, 8, 4, 4))
plot(TukeyHSD(clmaov, which = "Diet:Drug"))
par(op) ## return graphical parameters to previous stage

## Though if we only care about the contrasts
## between all factor combinations maybe we really just want this?
TukeyHSD(aov(y ~ Diet:Drug, data = dc2))


## Diet and Drug. Compare to summary(glht(clm, linfct = rbind(K1A, K2A)))
TukeyHSD(clmaov, which = c("Diet", "Drug"))

@ 



A virtue of \Rfunction{TukeyHSD} is its simplicity. In models without
interactions, results of these procedures are often very, very similar, though
not necessarily identical (with unbalanced designs, specially heavily unbalanced,
\Rfunction{glht} is often preferable --- see for example the discussion in
Everitt and Hothorn, chapters 5 and 14).

In models with interactions, \Rfunction{glht} allows us to control what to do
with averaging over interactions and it will not necessarily give the same
results as \Rfunction{TukeyHSD} (if there is no unbalance in sample sizes,
results will often be very similar if we use \texttt{interaction\_average =
  TRUE}). See further discussion, for example, here:
\Burl{https://stat.ethz.ch/pipermail/r-help/2012-January/299623.html}. But this
is, as we anticipated, a complicated issue. What comparisons are you interested
in when there are interactions?


Finally, another package that might be worth checking is \CRANpkg{emmeans}
(\Burl{https://cran.r-project.org/web/packages/emmeans/index.html}). In many
cases, or in other types of models, not covered in this course (e.g.,
mixed-effects, generalized linear models, etc) it might be easier or more
flexible than the packages used above (see, for instance,
\Burl{https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html}). The
latest edition of Fox and Weisberg's ``An R companion to applied regression''
makes extensive use of \CRANpkg{emmeans} (to obtain confidence intervals and many
other things).



\subsection{Nonparametric alternatives}\label{nonparanova}
Are there nonparametric versions of the above procedures? For the one-way
ANOVA the Kruskal-Wallis test is popular. For two-way designs with one
observation per cell the Friedman test and the Quade test. But testing
interactions is not easy; one needs to use more sophisticated approaches
as in permutation tests conditioning on permuting only within rows or
columns, etc. We will not pursue this any further.


\clearpage




\section{Simple linear regression}\label{regr}

This is another form of a linear model. But, now, the independent variable
is continuous. So we will fit a line:

\[Y = \alpha + \beta X + \epsilon\] where $Y$ is, as usual, the dependent
variable, $X$ the independent, $\beta$ is the slope and $\alpha$ the
intercept (this is just the equation for a line). The simple linear
regression procedure will estimate $\alpha$ and $\beta$, finding values
($\hat{\alpha}, \hat{\beta}$) that produce a \textbf{best fitting line}
(note: it is a line, not an arbitrary curve).

%% Let's use a simple data set that relates data set from the base package, ``women''. Go to ``Data'',
%% ``Data in packages'' and select \Robject{women} (it is in package \CRANpkg{datasets})


%% \begin{figure}[h!]
%%   \begin{center}
%%     \includegraphics[width=0.80\paperwidth,keepaspectratio]{women-data.png}
%%     \caption{\label{women} Selecting the ``women'' dataset.}
%%   \end{center}
%% \end{figure}



We will use a subset of data from the AnAge data set (Animal Ageing and
Longevity Database) (accessed on 2014-08-19) from
\Burl{http://genomics.senescence.info/species/}. This file contains
longevity, metabolic rate, body mass, and a variety of other life history
variables. The data I provide you are a small subset that includes only
some birds and reptiles.



%% <<>>=
%% ## Dealing with the AnAge data set.  

%% ## I download the AnAge data set (Anmal Ageing and Longevity Database) (on
%% ## 2014-08-19) from \Burl{http://genomics.senescence.info/species/}. I
%% ## replace all ``''' by nothing (there are names like Whatever's fich,
%% ## etc). And read it into R. Then, remove a few strange points. Mammals are
%% ## interesting, since they show a curvilinear relationship after log-log.

%% ## anage <- read.table("anage_data.txt", header = TRUE, sep = "\t")

%% anage.birds.reptiles <- anage[-c(2174, 2145, 1939, 1945, 1406, 
%%                                  3975, 3954, 3956, 3925), ]
%% anage.birds.reptiles <- anage.birds.reptiles[anage.birds.reptiles$Class %in% 
%%                                              c("Aves", "Reptilia"), ]
%% anage.birds.reptiles <- anage.birds.reptiles[, c(
%%     "Class", "Order", "Family",
%%     "Genus", "Species",
%%     "Metabolic.rate..W.",
%%     "Body.mass..g.",
%%     "Temperature..K.",
%%     "Maximum.longevity..yrs."
%%     )]
%% write.table(anage.birds.reptiles, file = "AnAge_birds_reptiles.txt", sep = "\t",
%%             col.names = TRUE,
%%             row.names = FALSE, quote = FALSE)

%% ## For mammalia, can play with this
%% a4 <- anage[-c(3618, 3052, 2376, 2831, 2449, 2349, 2752, 3252, 2444, 3619), ]; anage.mam <- a4[a4$Class == "Mammalia", ]
%% @ 

Read the full data and call it \Robject{anage\_a\_r} (the a and r stand
for aves and reptilia, the proper Class names).

<<create_anage, echo=FALSE, results='hide'>>=
anage_a_r <-  read.table("AnAge_birds_reptiles.txt", 
                         header=TRUE, sep="", na.strings="NA", dec=".", 
                         strip.white=TRUE,
                         stringsAsFactors = TRUE)

@ 

We want to take the log of all the relevant continuous variables (yes, you
would not know this before hand, but I do, so create those new variables
now to avoid going back later)\footnote{Creating these new variables is
  not really necessary in general for fitting models. But some functions
  from the \CRANpkg{HH} package lead to problems if we don't.}. It is much faster
to just do it by typing the code in the R Script or RStudio console:

<<>>=
anage_a_r$logMetabolicRate <- log(anage_a_r$Metabolic.rate..W.)
anage_a_r$logBodyMass <- log(anage_a_r$Body.mass..g.)
anage_a_r$logLongevity <- log(anage_a_r$Maximum.longevity..yrs.)
@ 


For now, we will only use the birds. So use subsetting to keep only birds
and call it \Robject{anage\_a} (yes, you know how to do that; look at
Figure \ref{subset}, and recall that the Class is ``Aves''; look at the
data).

This is how you can do it directly in R:
<<create_anage_a, results='hide'>>=
anage_a <- anage_a_r[anage_a_r$Class == "Aves",]
@ 


We want to model metabolic rate as a function of body mass (note that this data
set is rather nice, because column names are nicely labeled and include
information about units). \textbf{Beware:} what we are going to do is not
correct, as the data are not independent (species share common ancestors, and
they are related in varying degrees, as any phylogenetic tree would show you, and
as you should be able to tell from looking at the names of some species). So we
are violating the assumption of independence. What we are doing here is just for
the sake of the example, and because this is a nice set of data\footnote{This can
  be done correctly, incorporating phylogenetic information in the regression
  model, but this is way out of the scope of this class. It is a really
  fascinating topic, though! This is often referred to as using the comparative
  method in evolutionary biology.}.


Now go to ``Statistics'', ``Fit models'', ``Linear regression'' and fit
that model. Let's call the model \Robject{metab}

<<>>=
metab <- lm(Metabolic.rate..W. ~ Body.mass..g., data=anage_a)
summary(metab)
@ 



The row of the output that says ``(Intercept)'' gives you the estimate of
the intercept. The t-statistic (under ``t value'') is testing that the
intercept is zero. And it is not. But tests about the intercept are rarely
interesting (except for cases with a natural and meaningful 0). The second
line is more interesting: that is the slope, how much metabolic rate
increases per unit increase in body mass (of course, to interpret this we
need to know the units!). And the t-statistic tests if the slope is
0. There is certainly strong evidence that Metabolic rate increases with
body mass.

Do you know what ``R-squared'' refers to? And the rest of the output?

By the way, did you see the note about missingness? Do you know what that
means? 

\subsection{And how does it look like}
Eh!!! We should probably have plotted the data as the first thing.  First, let's
do a scatterplot (you might want to unclick the ``Show spread''\footnote{I find
  the spread information to be confusing. But I like to leave the smoothed line,
  as it can help me see errors in the model specification.}):

<<out.width = '10cm', out.height = '10cm', results='hide'>>=

scatterplot(Metabolic.rate..W.~Body.mass..g., 
            smooth = FALSE, 
            data=anage_a)

@ 

% The second plot we can get from ``Models'', ``Confidence interval plot''
% (you need to have the \CRANpkg{RcmdrPlugin.HH} loaded) which will show
% something like

% <<out.width = '12cm', out.height = '12cm'>>=
% ci.plot(metab)
% @
 
% (and this shows confidence and prediction intervals for the linear model).


\subsubsection{Transforming the data}
Hummm\ldots. That plot does not look good. OK, let's refit a model, but
this time let's transform both the dependent and independent variables
with a log (why a log? theory and previous empirical evidence from the
field of allometry and life history suggest that it is a reasonable way to
go).

You can refit the model by creating new variables, etc. That is up to
you. An easier thing might be to copy the previous model, and modify it in
the R Script/ RStudio console, giving it a new name
(\Robject{metablog}). We will also replot, and pay attention because in
the scatterplot menu you can tell it to log both axis which is nicer than
plotting the log-transformed data directly.

First fit the model: \footnote{You could have fitted the model as\\ 
 \texttt{metablog <- lm(log(Metabolic.rate..W.) $\sim$ log(Body.mass..g.), data  = anage\_a)}\\
and that would have been fine. But then, functions \Rfunction{ci.plot} and \Rfunction{ancova} from \CRANpkg{HH} choke.}  
<<>>=
metablog <- lm(logMetabolicRate ~ logBodyMass, data=anage_a)
summary(metablog)
@ 



Now plot it:

<<fig.width=7, fig.height=7,results='hide'>>=

scatterplot(Metabolic.rate..W.~Body.mass..g., log="xy", 
            smooth = TRUE, boxplots='xy', 
            data=anage_a)

@ 
%% (I have suppressed the output that gives the flagged points and will do so
%% for the rest of scatterplots)



This looks much, much better. We will address this issue more formally
below (section \nameref{diagnostics}). Notice that the call to
\Rfunction{scatterplot} uses the original variables but the axis are in
log-scale, which is nicer than directly plotting (in linear scale) the
log-transformed variables: you can see the original values.



But this was a particularly simple example since I told you how to
transform the data. You should be asking yourself: how do I know what
transformation to use? Often, theory (should allometric patterns scale
with the log?  shouldn't we use the square root for phenomena that take
place on surfaces?  etc) can guide us. Otherwise, there are procedures to
try to identify transformations, including some diagnostic plots that can
help (e.g., component+residual plots, section \nameref{diagnostics}).



\subsection{Confidence intervals and predictions intervals or prediction and
  confidence bands}\label{ci-regression}


From ``Models'', ``Confidence interval plot'' (you need to have the
\CRANpkg{RcmdrPlugin.HH} loaded) which will show something like


<<fig.width=7, fig.height=7,out.width = '12cm', out.height = '12cm'>>=
ci.plot(metablog)
@ 

What are the bands?


Maybe an example with more noise will help:

<<ci_bands_reg_2,fig.width=7, fig.height=7,out.width = '12cm', out.height = '12cm'>>=
N <- 20
x <- runif(N, min = 10, max = 30)
y <- 3 + 5 * x + rnorm(N, sd = 10)
dummy_data <- data.frame(y = y, x = x)
rm(y, x)
lm_cib_2 <- lm(y ~ x, data = dummy_data)
ci.plot(lm_cib_2)
@ 

You can run the previous code changing the value of \texttt{N}, to get an
intuition of the difference between the two bands.



\subsection{Confidence intervals for the parameters}\label{ci-regr-params}

For this course, what follows is of much less interest, but just so that you have
it here. You can skip it if you want.

You can of course obtain confidence intervals for the parameters of the model:

<<>>=
confint(metablog)
@ 

Those are confidence intervals for the parameters themselves. The estimates of
the parameters, however, are correlated. This means that testing each parameter
on its own can lead to different answers from testing both at once. Let us show a
joint confidence ellipse. I follow here Faraway's ``Linear models with
R'' and Fox and Weisberg's ``An R companion to applied regression''. (There is no need for you to try to replicate this)

<<ci_ellipse,fig.width=7, fig.height=7,out.width = '6cm', out.height = '6cm'>>=
## Correlation of estimated coefficients
round(cov2cor(vcov(metablog)), 3)

## Plot of joint and each-at-time CIs
library(ellipse)
plot(ellipse(metablog), type = "l")
points(coef(metablog)[1], coef(metablog)[2], pch = 1, cex = 2)
abline(v = confint(metablog)[1, ], lty = 2)
abline(h = confint(metablog)[2, ], lty = 2)
@ 

An alternative to \Rfunction{ellipse} is \CRANpkg{car}'s
\Rfunction{confidenceEllipse}. 


(And no, there is nothing wrong with this correlation. It actually makes a lot of
sense.  Think about tilting the regression line, keeping the center of mass
fixed, so play with increasing or decreasing the slope: what happens with the
intercept?)



\clearpage



\section{Multiple regression}\label{multreg}

\subsection{Introduction to multiple regression and the cystfibr data}
In the previous section we had

\[Y = \alpha + \beta X + \epsilon\]

Now we can have two or more independent variables:

\[Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \ldots + \epsilon\]


I will use a dataset that is a small subset from the original
\Robject{cystfibr} data set from package \CRANpkg{ISwR} (by Peter
Dalgaard; this package is also material to accompany Dalgaard's book
``Introductory statistics with R'')

<<echo=FALSE,results='hide'>>=
data(cystfibr, package = "ISwR")
cystfibr2 <- cystfibr[, c("pemax", "age", "height", "weight", "sex")]
write.table(cystfibr2, file = "CystFibr2.txt", col.names = TRUE,
            row.names = FALSE, sep = "\t", quote = FALSE)
rm(cystfibr2)
rm(cystfibr)
@ 


Import the dataset. 

<<>>=
cystfibr2 <- read.table("CystFibr2.txt", header = TRUE)
@ 


The meaning of the variables is (this is copied
verbatim from the help of the original dataset):
\begin{verbatim}
     'age' a numeric vector, age in years.
     'sex' a numeric vector code, 0: male, 1:female.
     'height' a numeric vector, height (cm).
     'weight' a numeric vector, weight (kg).
     'pemax' a numeric vector, maximum expiratory pressure.
\end{verbatim}




\subsection{Multiple regression example}\label{multregr-example}

For the multiple regression we model

\[pemax = \alpha + \beta_1 age + \beta_2 height + \beta_3 weight + \epsilon\]


You should know how to work around the menus and get something like:

<<>>=
mcyst <- lm(pemax ~ age + height + weight, data=cystfibr2)
summary(mcyst)
confint(mcyst)
@ 

You should be able to interpret all output without problems. 

%% For surface plots

% <<>>=
% print(scatter3d(pemax~age+height, data=cystfibr2, 
%           fit="linear", residuals=TRUE, bg="white", axis.scales=TRUE, 
%           grid=TRUE, ellipsoid=FALSE))
% @ 

% print(scatter3d(pemax~age+height, data=cystfibr2, 
%           fit="quad", residuals=TRUE, bg="white", axis.scales=TRUE, 
%           grid=TRUE, ellipsoid=FALSE))
          
% The surface
% http://stackoverflow.com/questions/18147595/plot-3d-plane-true-regression-surface

% also with rms

% library(rms)
% lm2 <- lm(pemax~age*height, data=cystfibr2)
% persp.lm(lm2, ~ age + height)

Now, use ANOVA tables with Type I sums of squares and Type II sums of squares:

<<>>=
anova(mcyst)
Anova(mcyst)
@ 

Are you surprised? Age seems highly significant with the sequential sums
of squares (when entered first and using \Rfunction{anova}, so Type I) but
not when we test it after all other terms in the model (\Rfunction{Anova},
or Type II). Why? One possible explanation is that there is correlation
between explanatory variables, and that the information of age relevant
for predicting pmax is already contained in the height and weight. That
age, height, and weigth are correlated is easy to check with a scatterplot
matrix:

<<results='hide'>>=

scatterplotMatrix(~age+height+pemax+weight,  
                  data=cystfibr2)

@ 

In fact, if you refit the model and now put height first, and do a
sequential test you find ... that height seems significant:

<<>>=
anova(lm(pemax ~ height + weight + age, data = cystfibr2))
@ 

And similar if you place weight first.
<<>>=
anova(lm(pemax ~ weight + height + age, data = cystfibr2))
@ 


Is this a problem? Well, you are trying to model pemax as a function of
three variables, but those three variables are very highly correlated
among themselves. Is this a common phenomenon: yes, it is rather common.


\subsection{$R^2$ and Adjusted $R^2$}
\label{r2}

We will explain this class, in case it is not clear:

<<>>=
summary(mcyst)

cor(fitted(mcyst), cystfibr2$pemax)^2

all.equal(
    cor(fitted(mcyst), cystfibr2$pemax)^2,
    summary(mcyst)$r.squared
)

## 
all.equal(summary(mcyst)$sigma^2,
          sum(residuals(mcyst)^2)/21)

## and where is the (25 = 21 - 4) 4 from
21 == (length(residuals(mcyst)) - length(coefficients(mcyst)))

all.equal(
(var(cystfibr2$pemax) - summary(mcyst)$sigma^2)/var(cystfibr2$pemax),
summary(mcyst)$adj.r.squared)


## Also could see as

ssresid <- sum(residuals(mcyst)^2)
sstot   <- sum( (cystfibr2$pemax - mean(cystfibr2$pemax))^2 )

## Multiple R^2
1 - (ssresid/sstot)
length(resid(mcyst))
## Adjusted R^2
1 - ( (ssresid/(25 - 4))/(sstot/(25 - 1)) )



@ 












\subsection{Interactions between continuous variables}\label{regr-int}
Can we add interactions? Yes, of course. Interactions between continuous
variables, however, are harder to visualize (they represent curved
surfaces, because the slope of one of the variable changes as the other
variable changes, whereas an additive model is just a plane ---or
hyperplane)\footnote{If you want to play around with a regression plane or
  regression surfaces, go to ``Graphs'', ``3D graph'', ``3D
  scatterplot''. In options, you can choose a plane or three different
  surfaces ---you might need to play with the degrees of freedom if you
  get errors. You can zoom in and out with the mouse wheel and move/rotate
  the figure. Try doing that during your TFM defense! Of course, that
  allows only for up to one dependent variable and two independent ones:
  our brains do not seem ready for 4D and higher.}.



How do we interpret coefficients? Just a few hints here (you can skip this if you
want). Suppose we have\\
$y = \alpha + \beta_1 x_1 + \beta_2 x_22 + \beta_{1,2}\ x_1\ x_2 + \epsilon$
(where the term $\beta_{12}\ x_1\ x_2$ is literally the product of $\beta_{1,2}$
times the product of $x_1$ and $x_2$). Now, in a table of coefficients, what is
the meaning of $\beta_1$? That is a table that ``shows every variable when
everything else is also in the model'' (and that includes the interaction).  You
can think of $\beta_1$ as just the best term that solves the above equation
($y = \alpha + \beta_1 x_1 + \beta_2 x_22 + \beta_{1,2} x_1 x_2 + \epsilon$). But
think about the interpretation of ``a coefficient tells me how fast the response
changes when the predictor changes by one unit''. Let's do that:
$\frac{\partial y}{\partial x_1} = \beta_1 + \beta_{1,2}\ x_2$; see how $x_2$ is
there. How fast $y$ changes with $x_1$ is also a function of $x_2$.


This differs from the model without interaction where we have:
$\frac{\partial y}{\partial x_1} = \beta_1$. 

And a simple numerical example (again, skip if you want):
<<>>=
mah <- lm(pemax ~ age + height, data = cystfibr2)
mahi <- lm(pemax ~ age * height, data = cystfibr2)

## Note how the coefficients are VERY different
summary(mah)
summary(mahi)

## Note that the SS of age and height are the same in both
## though the RSS in mahi is smaller.
Anova(mah)
Anova(mahi, type = "II")

## SS of height same as for type II of mah and mahi
anova(lm(lm(pemax ~ age + height, data = cystfibr2)))
## SS of age same as for type II of mah and mahi
anova(lm(lm(pemax ~ height + age, data = cystfibr2)))

## But the coefficients in mahi are from a model that includes the
## interaction.
## Thus, the tests of coefficients of age and height in mahi are
## not the same as the tests of age and height in the ANOVA tables (Type
## II) for models mah and mahi.

@ 

% ## For the sake of curiosity, notice this and compare to the table of
% ## coefficients
% Anova(mahi, type = "III")
% opt <- options(contrasts = c("contr.sum", "contr.poly"))
% mahi2 <- lm(pemax ~ age * height, data = cystfibr2)
% Anova(mahi2, type = "III")
% options(opt)

% (For Type III SS: see section \ref{ssI-to-III}).


% <<>>=
% mahs <- lm(pemax ~ scale(age, scale = FALSE) + scale(height, scale = FALSE), data = cystfibr2)
% mahis <- lm(pemax ~ scale(age, scale = FALSE) * scale(height, scale = FALSE), data = cystfibr2)

% summary(mahs)
% summary(mahis)

% ## mahis coeff for age
% coefficients(mahi)["age"] + coefficients(mahi)["age:height"] * mean(cystfibr2$height)
% ## mahis coeff for height
% coefficients(mahi)["height"] + coefficients(mahi)["age:height"] * mean(cystfibr2$age)
% @ 



\subsection{Confidence intervals, confidence bands}\label{mult-regr-ci-pred}

This is also provided so that you have it in here if you need it, but you can
skip it.

With multiple regression, visualizing confidence intervals and prediction
intervals becomes much harder (we are no longer in 2D). There are ways of
visualizing 3D plots (see \nameref{regr-int}), but you will probably need to decide
what exactly you want to plot and for what. For example, predictions with respect
to each variable in different panels. You can easily obtain, for each
observation, its predicted (fitted) value and its confidence or prediction
values. For example (showing just the first five values)

<<>>=
predict(mcyst, interval = "confidence", level = 0.95)[1:5, ]
predict(mcyst, interval = "prediction", level = 0.95)[1:5, ]
@ 

Side note: pay attention to the warning: the predictions should not be used to assess how well the model predicts the data used for the fitting itself.

With those values, you could then construct the plots you might want.



Similar comments apply to plots of confidence intervals for the parameters:
visualizing the multidimensional ellipses will not be easy. Obtaining the
confidence intervals and the matrix of correlations is simple, though:

<<>>=
confint(mcyst)
round(cov2cor(vcov(mcyst)), 3)
@ 

(Note: we already knew that the mcyst is probably not a great model!!)


\subsubsection{Confidence intervals, confidence bands: the bootstrap}\label{mult-regr-ci-pred-boot}

A final comment: for real, one might want to use confidence intervals using the
bootstrap instead of \Rfunction{confint}. See section 5.1.3 in Fox and Weisberg's
``An R companion to applied regression, 3rd ed.'' or section 3.6 in Faraway's
``Linear models with R, 2nd ed.''

\subsection{Three way anovas, factors with more than two classes, etc}
\label{sec:three-way-anovas}
There is nothing conceptually new, but the accounting gets more complicated.




\clearpage
\section{Continuous and discrete independent variables 
  and ANCOVA}\label{ancova}

\subsection{A first example of ANCOVA with the cystfibr data}
Right now, nothing should stop us from thinking about models where the
right hand side contains both continuous and discrete variables. Let's do
it with the cystic fibrosis data set.  ANCOVA refers to this mixture of
ANOVA and regression and stands for ``Analysis of
covariance''\footnote{But this does not mean that we are comparing
  covariances, as in comparing correlations, between groups; we are
  comparing groups after adjusting for possible covariates, if that is
  warranted by the absence of interactions between the continuous and
  discrete predictors.}. Regardless, all of ANOVA, regression, and ANCOVA
are especial types of linear models.


Let's fit a model where the independent variables are sex (discrete,
obviously) and age. However, sex is coded with 0/1 and we want it to be a
factor, explictly. Let's recode it:

<<>>=
cystfibr2$sex <- factor(cystfibr2$sex, labels=c('Male','Female'))
@ 


Now, fit a model. You have to do it from the ``Statistics'', ``Fit
models'', and then the ``Linear model'', not the ``Linear regression'',
because one of the explanatory variables (sex in this case) is a
factor.


<<>>=
mcyst2 <- lm(pemax ~ age * sex, data=cystfibr2)
summary(mcyst2)
confint(mcyst2)
Anova(mcyst2)
@ 

Note we added a ``*'', so an interaction between a continuous and a
discrete variable. Here there is no evidence of interaction.




Back to the interactions.  What would an interaction have looked like?
Different slopes for each group.

Look at the plots:

<<results='hide'>>=


scatterplot(pemax~age | sex, 
            boxplots='xy', 
            smooth = FALSE,
            by.groups=TRUE, 
            data=cystfibr2)

@ 

(yes, the best fitting slopes are slightly different, but they are not
significantly different, as shown by the the ``age:sex[T.Female]'' term in
the model above, so no evidence for different slopes).

The different intercepts are captured by the term ``sex[T.Female]'' (that
is not significant in this example either). Anyway, we can often have
models where we have no evidence of different slopes (no interaction), but
evidence of different intercepts: these means parallel lines (we will see
one below: \nameref{ancova_rept}).



One can visualize this also with the function \Rfunction{ancova} in
package \CRANpkg{HH} (which is loaded when we load
\CRANpkg{RcmdrPlugin.HH}, so we do not need to load it again);
\Rfunction{ancova} also produces and anova table (with sequential sums of
squares, Type I):

<<>>=
ancova(pemax ~ age * sex, data = cystfibr2, pch = 1)
@ 

Note that function \Rfunction{ancova} is not available from the menus. You
have to type the above expression directly.

And, of course, do not forget to look at the output from \Rfunction{confint}. As
you see, all the information (summary of the model, confidence intervals,
figures) points in the same direction. 



One final thing to notice here: at the beginning of this section we have
used ``Anova'' and ``summary(lm)'' and the p-values for age and sex are
not identical\footnote{Neither is it the case that the sqrt of the F for
  those terms is identical to t statistics, which is a property of the
  relationship between t statistics and F statistics with one degree of
  freedom in the numerator}. Why?  Because Type II sums of squares (the
default of \texttt{Anova}) respect the marginality principle: age and sex
are tested after each other, but \textbf{not} after the interaction is in
the model (this is in contrast to what happens in the \texttt{summary(lm)}
output). The marginality principle and what Type II Sums of Squares do
applies not only to Ancova, but to linear models in general\footnote{Type
  III Sums of Squares, which you can ask for when using \texttt{Anova}
  would give, for p-values, output identical to the one of lm even if
  there is an interaction term in the model ---barring possible
  differences due to the contrasts used, if we have factors; we will not get
  into any of this.}.
%% if just regression, not factors, that equivalence of Type III and lm
%% often the case.

Finally, can you tell if \texttt{ancova} is using sequential or Type II
sums of squares? If you use \texttt{ancova} and change the order of age
and sex in the specification of the model, does the numeric output differ?

\subsection{A parallel slopes model}
\label{parallel-slopes}

We can fit a model with parallel slopes:

<<>>=
mcyst0 <- lm(pemax ~ age + sex, data = cystfibr2)
summary(mcyst0)
confint(mcyst0)
Anova(mcyst0)
@ 

Make sure you understand the differences with respect to the previous
model. What are we fitting here? What are we saying, biologically, in this
model? 



\subsection{Formally comparing models}\label{model-comp}
In this case, the sequential anova table (as produced by
\Rfunction{ancova} or \Rfunction{anova}) suggests that we could simplify
our model a lot, and use one with a single intercept and slope (i.e., just
like a simple linear regression as in section \nameref{regr}) as neither
``sex'' by itself nor the interaction is relevant. We will do that, and we
will then do a global model comparison to verify the simplified model is a
reasonable one (go to ``Models'', ``Hypothesis tests'', ``Compare two
models''):

<<>>=
mcyst3 <- lm(pemax ~ age, data=cystfibr2)
anova(mcyst3, mcyst2)
@ 

This is a comparison of two models using an F test: it tests whether the
larger (more complex, with more terms) model is significantly better than
the smaller one. It clearly shows that, in this case, that the larger
model model (the one with both a main effect of ``sex'' and an
interaction) is not significantly better than the one without ``sex''. So
we can just keep model \Robject{mcyst3}: there is no statistical evidence
of \Robject{mcyst2} being any better.




Three comments here:
\begin{itemize}
\item These tests only make sense for nested models (where the terms of
  one of the models is a subset of terms of the other). Beware that R does
  not check this, and you could easily do meaningless
  things\footnote{There are ways to compare non-nested models using other
    procedures, for instance based on AIC.}.
\item Whether you type \verb@anova(mcyst2, mcyst3)@ or
  \verb@anova(mcyst3, mcyst2)@ is inconsequential for the F statistic and
  p-values.
\item This was a very clear-cut case. Often, people will proceed in steps:
  first check no interaction and then, later, and if no interaction, check
  if there is a need for different intercepts.

In fact, we could have done this in a more step-by-step way:

<<>>=
anova(mcyst0, mcyst2)
@ 

where we compare the model and without interaction (though this is the
same, of course, as the term for interaction in \Robject{Anova(mcyst2)}).

And then
<<>>=
anova(mcyst3, mcyst0)
@ 

but, again, this is just the same as the term for \texttt{sex} in
\Robject{Anova(mcyst0)}.



\end{itemize}








\subsection{ANCOVA with the birds and the reptiles}\label{ancova_rept}
We will use the longevity and metabolic rate data we used in section
\nameref{regr}, but now the full one: \Robject{anage\_a\_r}. We will go pretty
fast here (this is just a kind of review), and will examine two things:
\begin{itemize}
\item If the relationship between metabolic rate and body mass is
  different between reptiles and birds.
\item If the relationship between longevity and body mass is
  different between reptiles and birds.
\end{itemize}


\textbf{Beware:} as we said before, what we are going to do is not
correct, as the data are not independent (species share common ancestors,
and they are related in varying degrees, as any phylogenetic tree would
show you, and as you should be able to tell from looking at the names of
some species). What we are doing here is just for the sake of the example,
and because this is a nice set of data\footnote{This can be done
  correctly, incorporating phylogenetic information in the regression
  model, but this is way out of the scope of this class. It is a really
  fascinating topic, though! This is often referred to as using the comparative
  method in evolutionary biology.}.

We will see interactions, parallel and non-parallel slopes, and more
comparison of models. Again, these analyses are not fully correct as we
ignore phylogenetic relatedness. But they are nice to illustrate a couple
of points. We will directly use log transformations (again, theory and
previous empirical evidence indicate this is the way to go ---and I looked
at a couple of different models already).


First, metabolic rate vs.\ body mass allowing for interaction with
``Class'' (bird vs.\ reptile):
<<>>=
metab_b_r <- lm(logMetabolicRate ~ logBodyMass * Class, data = anage_a_r)
summary(metab_b_r)
confint(metab_b_r)
@ 

The output is clear: parallel lines (i.e., different intercepts, but same
slope). Note that this is not a silly or irrelevant biological detail:
metabolic rates scales with body mass in the same way in an endothermic
group (birds) and a ectothermic one (reptiles), but their metabolic rates
are not the same (birds' are higher). 


We could simplify this model to a model without the interaction (so single
slope, two intercepts):

%% this was wrong in previous version; I had the summary of metab_b_r
<<>>=
metab_b_r_2 <- lm(logMetabolicRate ~ logBodyMass + Class, data = anage_a_r)
summary(metab_b_r_2)
confint(metab_b_r_2)
anova(metab_b_r_2, metab_b_r)
@ 

(of course, the model comparison via \Rfunction{anova} here is really
unneeded: we know what the p-value and F values should be, since we are
only removing the interaction, for which we already saw a test).


Let's see the plots:
\clearpage
<<results='hide'>>=
scatterplot(Metabolic.rate..W.~Body.mass..g. | Class, 
            log="xy", smooth=FALSE, 
            by.groups=TRUE, 
            data=anage_a_r)
@ 

\clearpage
<<fig.width=7, fig.height=7,echo=TRUE, results='hide'>>=
ancova(logMetabolicRate ~ logBodyMass * Class, 
                data = anage_a_r, pch = 1)
@ 

\clearpage
What about longevity?

<<>>=
longev_b_r <- lm(logLongevity ~ logBodyMass * Class, data = anage_a_r)
summary(longev_b_r)
confint(longev_b_r)
@ 

In this case, lines are not parallel: the rate of change of longevity with
body mass is faster in reptiles than in birds (note the coefficient
``logBodyMass:Class[T.Reptilia]'').


How do things look?

<<results='hide'>>=

scatterplot(Maximum.longevity..yrs.~Body.mass..g. | Class, 
            log="xy", smooth=FALSE, 
            by.groups=TRUE, 
            data=anage_a_r)

@ 

<<echo=TRUE, results='hide'>>=
ancova(logLongevity ~ logBodyMass * Class, 
                data = anage_a_r, pch = 1)
@ 

Note: the tightness of the data around the lines in this model for
longevity is not nearly as good as for metabolic rate, which probably does
make biological sense.

If we where we to remove ``Class'' and refit a simpler model we would see
that the larger model is clearly better when using \Rfunction{anova} to
compare the two models:

<<>>=
longev_b_r_2 <- lm(logLongevity ~ logBodyMass,  data = anage_a_r)
anova(longev_b_r_2, longev_b_r)
@ 




\subsection{More examples}

Section 12.7 of Dalgaard's ``Introductory statistics with R'' contains a
beautiful and detailed example of ANCOVA, including transformation of
variables, using the \Robject{hellung} dataset included in \CRANpkg{ISwR}.



%% \subsection{Anova as lm, etc (zz: put this better)}
%% FIXME:

%% add examples of variables with three or more levels, and show what lm and
%% anova show

%% yes, show here. Also mentioned above So for ancova, use a factor with
%% three levels (maybe mammals, birds, reptiles)




\subsection{More variables}
We can extend the models to incorporate more variables, add interactions,
etc. Interactions can involve more than two variables, can involve
continuous and discrete variables, etc.

\subsection{Parameters, coefficients}
There is nothing conceptually new here. If you want the details, go to section \nameref{anovaaslm}
and make sure things make sense with these new models: how we interpret
coefficients and how many parameters we have.


\section{Covariate adjustment and a few comments about causal
  inference}\label{causal_covar}

Should we use all covariates available to us in our models? It depends on what we
want to do. If all we care is prediction, then maybe yes. If we care about
interpretation, probably not. In particular, variables that are effects of the
outcome variable (i.e., variables that are, causally, downstream from our ``y'')
are often variables we do not want to adjust for. Likewise for
variables that are downstream from both the ``y'' and other predictor variables.


We have been using language a bit to casually (I wrote ``casually'', not
``causally'' :-). But sometimes we will read our models as saying ``a unit increase
in variable X1 is associate with these many units change in Y''. At other times,
we will want (and even be able to say) ``a unit increase in variable X1 causes
these many units change in Y''. These differences in wording also emphasize the
possible difficulties in understanding the meaning of phrases like ``holding
variable X2 constant, a unit change in variable X1 causes whatever''.


That said, it is possible sometimes to use data, both experimental and
observational, to make causal claims. Yes, we wrote observational, too. And it is
actually good news that we can (sometimes, and under certain assumptions)
estimate the effects of causes, since many important questions that concern us
are inherently causal. For example ``will doing X minimize the risks of being
infected by the coronavirus?''. An many of these questions (or aspects of these
questions) are ones for which gathering experimental data is just not possible
(ethically, logistically, etc).



% If we have time, we will briefly touch upon
% these issues in class. Some specific issues we will mention are:
% \begin{itemize}
% \item Counterfactuals
% \item DAGs (directed acyclic graphs) for representing our assumptions. (We have
%   already done so in some examples).
% \item The do operator and the interventionist account of causality and how it
%   relates to DAGs.
% \end{itemize}

The literature on causal inference has grown quite a bit in the last 10
years. Some recommended readings, in approximate order of increasing difficulty
(of the reading itself, or of working through all of the material) are:

\begin{itemize}
\item Pearl and Mackenzie, 2018, ``The book of why''.
\item Rosenbaum, 2017, ``Observation and experiment. An introduction to causal inference'' (Rosenbaum does not use
  DAGs)
\item Pearl, Glymour, Jewell, 2016, ``Causal inference in statistics: a primer''
  [Beware with errata: avoid early printings of the book and make sure to look at
  the errata page anyway!]
\item Hern\'an and Robins. 2020. ``Causal inference: what if''. Available from \Burl{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}.
\item Morgan and Winship, ``Counterfactuals and causal inference: methods and
  principles for social research'' [Yes, basically all examples are from
  sociology and political science, but that should not be a problem.]
\end{itemize}

A separate PDF is available for this topic, \texttt{covars-interpr-causal.pdf},
and we will cover it at the end of this lesson.



\section{Interactions, summary}\label{interaction-summ}
The general pattern is always the same: the effect of one independent
variable (say, A) depends on the setting of the other independent variable
(say, B) with which it interacts. In other words, to say something about
how a change of variable A affects the outcome, you need to also know the
setting or value of variable B.

The three main types are:

\begin{description}
\item[Between factors] There is one level for each combination of the
  factors, as in the example in section \nameref{2way-int}.
\item[Betwen a factor and a continuous variable] What we saw in ANCOVA, in
  section \nameref{ancova}: a different slope for each group. (Parallel slopes
  is not an interaction).
\item[Between two continuous variables] We mentioned this in section
  \nameref{regr-int}: slope changes as we move the other variable which gives
  curved surfaces.
\end{description}
\clearpage


%% ZZ FIXME
%% Use qqPlot from car, and possibly other diagnostic tools from car?

\section{Diagnostics}\label{diagnostics}

\subsection{Model diagnostics: why, how}

Wait!!! Do our models make sense? These are models, so we can, and should,
check some of their basic assumptions. We cannot do justice to this
\textbf{very important topic}. 

In general, for linear models (ANOVAs, regressions, etc) we want to check:

\begin{itemize}
\item Constant variance (across groups or over the range of the independet
  variables). Often referred to as ``homoscedasticity'' (where
  ``heteroscedasticity'' is the opposite).
\item Linearity (for regression).
\item Approximate normality of residuals.
\item Possible highly influential points (i.e., do our results depend on
  one or two points that are driving the model one way or another?).
\item Possible outliers.
\end{itemize}


\textbf{Independence} is also a crucial assumption. But often, checking
independence is very difficult from the data themselves (or at least from
the data we have been using, anyway). For example, lack of independence
among data is the reason why the analysis with the AnAge data set are not
really correct.


Before looking at the plots, two concepts should be clear:
\begin{description}
\item[Fitted value] The predicted, or fitted value: if we have an equation
  like $Y = \alpha + \beta_1 X + \beta_2 Z$, then the fitted values are
  the $Y$ for the observed combinations of $X$ and $Z$ (with the values of
  $\alpha$ and $\beta$ returned from the model). It is just what the
  model predicts.

\item[Residual] Basically, the difference between the observed and the
  fitted value. There are different types of residuals (residuals,
  standardized and studentized being the most common).

\end{description}


\subsection{Diagnostics: an example with a designed experiment with
  factors} \label{diag-factors}


We will first use the fake data from a perfectly balanced experimental
design, the data used in section \nameref{ordermatter}. The diagnostic plots
from these kinds of designed experiments can look slightly different from
those for regression, which makes sense if you think about it. I will
recreate the data here. However, to make things more interesting, I will
create a very large outlier:

<<>>=
## Create the data 
set.seed(1)
sex <- factor(rep(c("Male", "Female"), c(20, 20)))
drug <- factor(rep(rep(c("A", "B"), c(10, 10)), 2))
y <- rep(c(10, 13, 12, 16), rep(10, 4))
y <- y + rnorm(length(y), sd = 1.5)
y.data <- data.frame(y, sex, drug)
y.data[1, 1] <- 25
## Fit the model
myAdditive <- lm(y ~ sex + drug, data = y.data)
myInteract <- lm(y ~ sex * drug, data = y.data)
## What are they saying?
summary(myAdditive)
summary(myInteract)
@ 

<<fig.width=8,fig.height=8,fig.show='hold', fig.cap='Model diagnostics for designed experiment, additive model '>>=
oldpar <- par(oma=c(0,0,3,0), mfrow=c(2,2))
plot(myAdditive)
par(oldpar)
@ 

<<fig.width=8,fig.height=8,fig.show='hold', fig.cap='Model diagnostics for designed experiment, interaction model '>>=
oldpar <- par(oma=c(0,0,3,0), mfrow=c(2,2))
plot(myInteract)
par(oldpar)
@ 


First, look at the plots. Try to see what they are about. See how there is
one point that stands out in several plots, look at the regularity of the
patterns.


The plot on the upper left is used to judge if the functional form of the
model makes sense, especially for regression models (not so much for
designed experiments with factors, but it is still helpful).  This figure
also helps spots systematic changes in variance (i.e., violations of
homoscedasticity). But for this, the bottom left plot is better which, in
this case, does not suggest anything serious, except for the outlier.

The upper right plot (a ``q-q plot'') is used to assess approximate
normality of the residuals: you want points to more or less lie along the
dotted line (with some allowance for deviations in the tails). Here the
``q-q plot'' is not perfect (even if we discount the outlier), even when
data did come from a normal; this is totally normal (remember, we are
sampling).

The bottom right differs in regression models and experiments with
factors. Here you are shown residuals vs.\ factor level combinations. (The
idea of leverage that makes a lot of sense in regression ---see below---
is not that useful here). Notice how this plot shows four vertical lines
in both models, the additive and the interaction one, but plots on the
upper left and bottom left differ in the apparent number of vertical
lines. Do you understand why?


An issue about syntax: if you want the plots to show, in the upper part of the
figure, the actual model fitted, you might need to increase the margins. That is
what I do with the \verb@par(oma = c(0, 0, 3, 0))@ (I actually do that inside the
call to also allow to plot several figures at once: \verb@par(oma=c(0,0,3,0), mfrow=c(2,2))@)


\clearpage
\subsection{Diagnostics: examples with some of the regression models}\label{diag-regr}

We will now use the two simple regression models we fitted in section
\nameref{regr}. Make the first one (\Robject{metab}) active and go to
``Models'', ``Graphs'', ``Basic diagnostic plots''. 

<<fig.width=8,fig.height=8,fig.show='hold', fig.cap='Model diagnostics for metabolic rate model without log transformation '>>=
oldpar <- par(oma=c(0,0,3,0), mfrow=c(2,2))
plot(metab)
par(oldpar)
@ 

%% Before looking at the plots, two concepts should be clear:
%% \begin{description}
%% \item[Fitted value] The predicted, or fitted value: if we have an equation
%%   like $Y = \alpha + \beta_1 X + \beta_2 Z$, then the fitted values are
%%   the $Y$ for the observed combinations of $X$ and $Z$ (with the values of
%%   $\alpha$ and $\beta$ returned from the model). It is just what the
%%   model predicts.

%% \item[Residual] Basically, the difference between the observed and the
%%   fitted value. There are different types of residuals (residuals,
%%   standardized and studentized being the most common).

%% \end{description}



Now you can see why with regression models we can use the plot on the
upper left to judge if the functional form of the model makes sense: if
the relationship is really linear as modeled, you should see no systematic
pattern here, but we see it (in this case, it suggests that out model is
predicting too small a metabolic rate at intermediate values, and the
opposite at large values, which suggests a curvilinear
relationship). Again, this figure also helps spots systematic changes in
variance (i.e., violations of homoscedasticity). But for this, the bottom
left plot is better and it does suggest that violations of
homoscedasticity are present (though, right now, with such strong evidence
of non-linearity, this is not a surprise).

What about the ``q-q plot'': things do not look great here, and this
distribution has several very large residuals, is heavy tailed, and also
somewhat skewed.

The bottom right can be hard to interpret: it shows two quantities
(residuals and leverage) that, together, are part of Cook's distance, that
measures the effect of individual points on the fitted coefficients
(values of Cook's distance larger than 1 usually indicate a possibly very
influential point, but any point with a widely outstanding Cook's distance
deserves a closer look).  The plot called ``Influence plots'' is very
similar to this one. However, I often find it simpler to look at plots
Cook's distance (function \Rfunction{cooks.distance}).



Now, repeat the above with the model that has taken logs:
<<fig.width=8,fig.height=8,fig.show='hold', fig.cap='Model diagnostics for metabolic rate model after log transformation '>>=
oldpar <- par(oma=c(0,0,3,0), mfrow=c(2,2))
plot(metablog)
par(oldpar)
@ 

and you will see that all diagnostics look much better.


You should also take a look at the diagnostics for some of the other
models we have fitted, specifically \Robject{longev\_b\_r} and
\Robject{metab\_b\_r} (or \Robject{metab\_b\_r\_2}) (section
\nameref{ancova_rept}). The metabolism one are OK, but the longevity model is
not fully satisfactory (small problems in all diagnostic plots, and one
potentially influential value). \Robject{mcyst2} (section \nameref{ancova})
and \Robject{mcyst} (section \nameref{multreg}), the two models we fitted to
the cystic fibrosis data, both look relatively decent, although there
could be some concerns about increases in variance with fitted values.
However, it is not easy to tell, because of the relatively few
points. This, of course, makes sense: if there are few points, we will
only be able to detect if a model is a bad one if it really is very bad.

\Robject{cholestanova}, from section \nameref{twoway}, looks relatively OK
(remember, this is an ANOVA, and there were six combinations of levels, and thus
the discrete values you observe in two of the plots). However, the
qqplot\footnote{``qqplot'': ``quantile-quantile plot''} of residuals is a little
bit ugly, but it is hard to tell from relatively so little data\footnote{Again,
  this is an interesting case, since these data are simulated from a normal
  distribution}. Finally, \Robject{AnovaModel.1}, from section \nameref{oneway},
looks just fine. Please look at all of these yourself to see them.

\clearpage
\subsection{Diagnostics: more examples with regression and ANCOVA models}\label{diag-more-reg}

<<f2d1z,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Non-constant variance in ANCOVA model'>>=
suppressWarnings(try(rm(e1, e2, tg1, tg2, drug, tumor.size, exposure)))
set.seed(33)
e1 <- runif(100, 1, 15)
e2 <- runif(250, 10, 20)
tg1 <- 0 + 0.3 * e1 + rnorm(100)
tg2 <- 2 + 0.2 * e2 
## tg2 <- tg2 + rnorm(25, sd = e2 * 0.1)
tg2 <- tg2 + rnorm(length(tg2), sd = e2 * 0.1)
tg2 <- tg2 + rnorm(length(tg2))
exposure <- c(e1, e2)
tumor.size <- c(tg1, tg2)
drug <- factor(c(rep("A", 100), rep("B", 250)))
tgd2 <- data.frame(tumor.size, exposure, drug)
rm(drug, tumor.size, exposure, e1, e2, tg1, tg2)
##ancova(tumor.size ~ exposure * drug, data = tgd2)
tgd2lm <- lm(tumor.size ~ exposure * drug, data = tgd2)
par(mfrow = c(2, 2), oma=c(0,0,3,0))
plot(tgd2lm)

## Would this tell you anything?
@
\clearpage

Some plots to try to understand  the patterns in the last residual plots. These are just some initial steps, the sort of thing I'd do if I found the above patterns; what could be happening? And here, the next plots clearly show what is happening. Can you tell what is going on?

<<f2d12ww,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Trying to make sense of those patterns, 1'>>=
## library(lattice)
## xyplot(tumor.size ~ exposure | drug, data = tgd2)
ancova(tumor.size ~ exposure * drug, data = tgd2)
@ 

<<f2d12wwb,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Trying to make sense of those patterns, 1b'>>=
library(lattice)
xyplot(tumor.size ~ exposure | drug, data = tgd2)
@ 


<<f2d12ww2,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Trying to make sense of those patterns, 2'>>=
boxplot(fitted(tgd2lm) ~ tgd2$drug)
@


\clearpage

\subsection{Diagnostics: more examples of non-constant variance (and other issues)}

The next example presents fairly common patterns:

<<f2d12z,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Another example with non-constant variance in a regression model.'>>=
suppressWarnings(try(rm(x1, y1)))

set.seed(3)
x1 <- runif(200, 1, 100)
y1 <- 4 + 3 * x1 + rnorm(200, sd = 20 * (x1^0.7))
x1y1 <- data.frame(x1, y1)
x1y1lm <- lm(y1 ~ x1, data = x1y1)
par(mfrow = c(2, 2), oma=c(0,0,3,0))
plot(x1y1lm)
rm(x1, y1, x1y1, x1y1lm)
@



% <<f2d12z,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='More non-constant variance and a few other issues'>>=
% suppressWarnings(try(rm(list = ls()))) 
% suppressWarnings(try(rm(X, U, Z, x1, y1)))

% set.seed(9)
% x1 <- runif(200, 1, 100)
% y1 <- 4 + 3 * x1 + rnorm(200, sd = 0.1 * (1/(x1^.9)))
% x1y1 <- data.frame(x1, y1)
% x1y1lm <- lm(y1 ~ x1, data = x1y1)
% par(mfrow = c(2, 2), oma=c(0,0,3,0))
% plot(x1y1lm)
% rm(x1, y1, x1y1, x1y1lm)
% @



The next is a contrived example, but one that shows things are clearly wrong (look at the bottom left).

<<f2d77z,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Yet another example of non-constant variance (plus other issues ---these is a contrived example)'>>=
suppressWarnings(try(rm(X, U, Z, x1, y1, x1y1lm, x1y1)))

set.seed(19)
x1 <- runif(200, 1, 100)
y1 <-  1e6 + 5 * x1 + rnorm(200, sd = (20/((x1/100)^1.1)))
x1y1 <- data.frame(x1, y1)
x1y1lm <- lm(y1 ~ x1, data = x1y1)
par(mfrow = c(2, 2), oma=c(0,0,3,0))
plot(x1y1lm)

rm(x1, y1, x1y1, x1y1lm)
@



\clearpage

\subsection{Diagnostics: a couple of examples from ANOVA models that are largely OK}


<<f2dg1az,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Diagnostic plots from an ANOVA model with interactions and unbalanced data'>>=
  
set.seed(13)
sex <- factor(rep(c("Male", "Female"), c(60, 60)))
smoking <- factor(rep(rep(c("No", "Yes"), c(30, 30)), 2))
hdl <- rep(c(1, 1.5, 5, 2), rep(30, 4))
hdl <- hdl + rnorm(length(hdl), sd = 1.5)
hdl1B <- data.frame(hdl, sex, smoking)[1:110,]
rm(hdl, sex, smoking)

m6B <- lm(hdl ~ smoking * sex, data = hdl1B)
par(mfrow = c(2, 2), oma=c(0,0,3,0))
plot(m6B)
@

\clearpage
In the next plot what we probably want to understand is why we have those three
large residuals. Those are leading to the pattern in the upper left diagnostic
figure.

<<f2d1v,echo=FALSE, fig.pos = '!h',fig.width=5.8, fig.height=5.8, fig.cap='', fig.lp="fig:", fig.cap='Diagnostic plots from another ANOVA model with interactions and unbalanced data'>>=

set.seed(38)
exercise <- factor(rep(c("Type_I", "Type_II"), c(60, 60)))
drug <- factor(rep(rep(c("A", "B"), c(30, 30)), 2))
dsdd <- rep(c(3, 2, 6, 4), rep(30, 4))
dsdd <- dsdd + rnorm(length(dsdd), sd = 2)
dsdd[c(1, 21, 13)] <- dsdd[c(1, 21, 13)] + 27
dsdd1B <- data.frame(dsdd, exercise, drug)[1:110,]
rm(dsdd, exercise, drug)

mdsB <- lm(dsdd ~ drug * exercise, data = dsdd1B)
par(mfrow = c(2, 2), oma = c(0, 0, 3, 0))
plot(mdsB)

@
\clearpage

\subsection{Diagnostics: more examples with designed experiments}\label{diag-more-anova}

Follow the text and the graphics. We will discuss this in class.

We will create a data set where the true model is one where there really is an
interaction. And we will fit both an additive model (i.e., a model without
interaction) and a model with interaction.


<<>>=
## See how diagnostics suggest missing interaction or
## at least suggest something is wrong.
set.seed(1)
sex <- factor(rep(c("Male", "Female"), c(20, 20)))
drug <- factor(rep(rep(c("A", "B"), c(10, 10)), 2))
y <- rep(c(8, 16, 10, 12), rep(10, 4))
y <- y + rnorm(length(y), sd = 1.5)
y.data1 <- data.frame(y, sex, drug)
rm(y, sex, drug)
with(y.data1, tapply(y, list(sex, drug), mean))

## Fit the model
myAdditive2 <- lm(y ~ sex + drug, data = y.data1)
myInteract2 <- lm(y ~ sex * drug, data = y.data1)
summary(myAdditive2)
summary(myInteract2)
@ 

We will now show diagnostic plots for the additive model and the model with interaction.



<<fig.cap='Additive model, myAdditive2,  when there is interaction'>>=
par(oma=c(0,0,3,0), mfrow = c(2, 3))
plot(myAdditive2, which = c(1:5)) ## look at first plot
@





<<fig.cap='Interaction model, myInteract2, when there is interaction'>>=
par(oma=c(0,0,3,0), mfrow = c(2, 3))
plot(myInteract2, which = c(1:5))
@

\clearpage


Now, we will create a data set where there is a point with a large value for
Cook's statistic. The data are perfectly balanced and the large Cook's distance
is, thus, the result of a large residual.

<<>>=
############
## Large cook's in anova
set.seed(1)
sex <- factor(rep(c("Male", "Female"), c(20, 20)))
drug <- factor(rep(rep(c("A", "B"), c(10, 10)), 2))
y <- rep(c(8, 12, 11, 15), rep(10, 4))
y <- y + rnorm(length(y), sd = 1.5)
y.data2 <- data.frame(y, sex, drug)
rm(y, sex, drug)
## create a large outlier

y.data2[1, 1] <- 30
with(y.data2, tapply(y, list(sex, drug), mean))

## ## Fit the model
myAdditive2b <- lm(y ~ sex + drug, data = y.data2)
## myInteract <- lm(y ~ sex * drug, data = y.data2)
## summary(myAdditive)
## summary(myInteract)
@ 

<<fig.cap='myAdditive2b: large Cook with balanced data'>>=
## ## diagnostics, all of them except 6th
## we actually have a large Cook's distance
par(oma=c(0,0,3,0), mfrow = c(2, 3))
plot(myAdditive2b, which = 1:5)
@

\clearpage


Now, remove the offending value (\textbf{BEWARE: this is not necessarily what you
should do in real life!! This is just for the sake of showing the diagnostic
plots}). Pay attention at the change in the plots (and we no longer have constant
leverage).

<<>>=
## model with and without first obs
summary(lm(y ~ sex + drug, data = y.data2))
summary(lm(y ~ sex + drug, data = y.data2[-1, ]))
@ 

<<fig.cap='myAdditive2b removing the large offending value'>>=
#### Diagnostics if we remove the offending value
par(oma=c(0,0,3,0), mfrow = c(2, 3))
plot(lm(y ~ sex + drug, data = y.data2[-1, ]), which = 1:5)
@


\clearpage

Now, create unbalance in the data by removing two observations (but not the one
with large residual). Notice how we get the plot of residuals vs.\ leverage and
we can clearly see the observation with the large Cook's distance.

<<>>=
## The model with two observations missing
## create unbalance
y.data3 <- y.data2[-c(35, 40), ]
with(y.data3, tapply(y, list(sex, drug), mean))
myAdditive3 <- lm(y ~ sex + drug, data = y.data3)
@ 

<<fig.cap='myAdditive3: missing two observations'>>=
par(oma=c(0,0,3,0), mfrow = c(2, 3))
plot(myAdditive3, which = 1:5) ## see Cook's
@ 
\clearpage

<<echo=TRUE,results='hide'>>=
## A few more (maybe technical) details. Statistics for influence,
## the ``hat'' matrix.
(hii2b <- lm.influence(myAdditive2b, do.coef = FALSE)$hat) ## constant
(hii3 <- lm.influence(myAdditive3, do.coef = FALSE)$hat) ##nope

diff(range(hii2b)) ## constant indeed
diff(range(hii3)) ## not constant
@ 





\subsection{Diagnostics: further issues}

For you to read and play around further:
\begin{itemize}
\item An extended qqplot is available from package \CRANpkg{car},
  \Rfunction{qqPlot} (and in R Commander under ``Residual quantile
  comparison plots'').
\item ``Component+Residual'' (or partial residual\footnote{These plots,
    for each variable, shows a plot of the independent variable, say $x$, on the
    horizontal axis and, on the ordinates, the ``partial residuals'' $=
    residual + \hat{\beta}x$.}) plots allow us to
  examine, in models with multiple regressors, deviations from linearity
  and could suggest the appropriate transformation. ``CERES plots'' are a
  variation of ``Component + Residual'' plots that work well even if
  relationships are strongly nonlinear. Both also available from
  \CRANpkg{car}.
\item Various diagnostics related to dfbetas allow us to identify
  influential observations in specific terms of the model.
\item Variance inflation factors help us detect possible problems caused
  by collinearity (correlations between independent variables).
\item Added variable plots are particularly useful in multiple regression
  problems with multiple independent variables; they can help to identify
  influential points (which are easily masked with multiple variables) and
  can also help to try to find a good functional relationship (but
  Component+Residual are more useful here). Again, available from
  \CRANpkg{car}. 
\item A variety of numerical tests and diagnostics are also available
  (e.g., tests for nonlinearity or for homoscedasticity).
  
\item Package \CRANpkg{car} and the accompanying book by Fox and Weisberg ``An R
  companion to applied regression, 3rd ed'' contain excellent and detailed
  comments about those and other diagnostics, and examples of how to use the
  functions in the \CRANpkg{car} package. Read chapter 8 of that book. It will
  explain you how to use them as well as possible remedial measures. Actually,
  read the complete book. In the meantime, you can also take a look, for a
  summary, at the very nice sections 8.3 to 8.5 in Kabakoff's ``R in action.''

\item There is in CRAN a very interesting package, \CRANpkg{gvlma}
  (\Burl{https://cran.r-project.org/web/packages/gvlma/}), by Pe\~{n}a and Slate,
  that implements the methods in their paper ``Global Validation of Linear Model
  Assumptions'', \textit{J. American Statistical Association},
  101(473):341-354. This offers a global testing procedure that allows further
  examination of each of the key assumptions. Here, I have preferred to start by
  looking carefully at each one of the traditional diagnostic plots. But you will
  most likely want to take a look at this package.

\item What if diagnostics identify a problem? The usual procedures are making
  sure the model is right, and possibly transforming either the response or some
  of the predictors, and maybe using more complex models (e.g., for modeling the
  variance, etc). But \textbf{before} fitting a model, think about it carefully
  and what is an appropriate biological model of the phenomenon. That might
  dictate, for example, reasonable \textit{a priori} transformations of the
  response or predictors, or what the functional form should be. (For example,
  modeling metabolic rate as a function of body mass was a bad idea; there are
  many strong arguments to suggest a log-log relationship is the way to go).
  
\end{itemize}



\clearpage

\section{Variable and model selection}

\subsection{Why model selection?}


You say you want to select variables? Select them for what? Prediction?
Interpretation?  Variable selection is a touchy and delicate
subject. First, procedures based purely in statistical criteria might
select ``statistically important'' variables (under some suitable
definition of ``important''), but those need not be the most relevant from
a biological point of view, or causally, etc

And with regards to the statistical procedures, we will summarize it as
follows: please, please, please, distrust automated variable selection
procedures that rely on p-values or F-statistics of individual variables
(in all their variants, such as stepwise, etc). Careful model comparison,
for instance using something like \verb@ anova(model1, model2) @, as we
have seen (e.g., section \nameref{model-comp} or \nameref{ancova_rept}), might be
a good idea. Notice, again, that \verb@ anova(model1, model2) @ is all
about comparing models.


\subsection{Model selection using AIC and \Rfunction{step}}


If you really need automated or semiautomated procedures, then reasonable
strategies use model-comparison criteria such as AIC\footnote{AIC: Akaike
  Information Criterion} (e.g., with the \Rfunction{step} function) and even
better is then bootstrapping the whole process to get estimates of error,
predictive ability, etc. Even better, of course, is subject-matter guidance on
how to proceed and what are and are not candidates for deletion and in what
order. The book by Frank Harrell, ``Regression modeling strategies'' contains
great discussions of these topics. The actual data set used here is discussed,
also in the context of variable selection, by P.\ Dalgaard in chapter 11 of
``Introductory statistics with R''.

Of course, using model diagnostics (section \nameref{diagnostics}) with the
initial and final models is always a necessity.

Let's give some examples (look at them carefully, and compare with what we
did by hand, and with whether or not these are the models you would
use). We are using AIC ($AIC = -2 \log(\hat{L}) + 2\ k$, where $k$ is the
number of parameters, so we want to minimize AIC):

<<>>=
step(mcyst2, direction = "both")
@ 

<<>>=
step(mcyst, direction = "both")
@ 


<<>>=
## In metab, we drop the interaction
step(metab_b_r, direction = "both")
 
## But nothing can be dropped here
step(longev_b_r, direction = "both")
@ 

\subsection{Differences between model selection using AIC and model comparison
  using \Rfunction{anova} (and hypothesis testing using \Rfunction{Anova})}\label{aic-f-tests}


The major difference is that when we compare models using \Rfunction{anova}, or
examine each of the terms in a fitted model with, say, \Rfunction{Anova}, we are
conducting statistical hypothesis testing. In contrast, the AIC criterion is not
used to conduct hypothesis testing, but is a criterion related to predictive
performance; so when we carry out model selection using AIC we are trying to find
the best model where ``best'' is ``best from the point of view of prediction''.

A second difference is that when we use \Rfunction{anova} and \Rfunction{Anova}
to compare models or assess the significance of different variables, we generally
do not (should not) do this with tens or hundreds of models and variables. We are
testing some specific hypotheses and doing it ``manually''. In contrast, using
model selection with AIC (or similar criteria) the procedure will run
automatically and can potentially compare hundreds of models.


A third difference is that most automated procedures such as \Rfunction{step}
with AIC will add or remove a ``complete variable''. However, with
\Rfunction{Anova} (and \Rfunction{glht}, and others) you can test specific
hypotheses of interest to you, including hypotheses where, say, the average of
two levels of a factor are equal to the third, etc.


Finally, note that \Rfunction{anova} should only be used to compare nested models
(even if \Rfunction{Anova} and others you can test hypotheses of interest to
you). \Rfunction{step} will follow greedy comparison rules for moving from one
model to another, but if you insist you could look at the AIC of a large
collection of models (or all the possible models for a set of variables) and
compare between models even if they are not nested versions of each other. Again,
doing this using AIC might be sensible because the objective is prediction, not
hypothesis testing.

Simplifying the issues a little bit, hypothesis testing and model building using
tools such as \Rfunction{anova} and \Rfunction{Anova} are things you do when you
want to understand a phenomenon, whereas model selection using criteria such as
AIC is what you do when you want to build models with the best predictive performance.


As a consequence of the above, of course, sometimes using \Rfunction{step} is
something that would make no sense and you would not even consider, for example
in a clean, clear-cut experiment with two factors: you want to use an ANOVA, no
need for \Rfunction{step}. And, similarly, with thousands of possible variables,
running thousands of manually run model tests would not make sense if you are
trying to build a good predictive model; use a procedure that will try to find
the best model from the prediction point of view.



\section{Experimental design matters}

All of the data we have seen so far have been relatively
straightforward. Things aren't always this way. Suppose a situation such
as this: 


\begin{itemize}
\item 20 mice.
\item 10 assigned to drug A, 10 assigned to drug B.
\item Each mouse in one leg gets a corticoid ointment, on another leg gets
  a placebo ointment.
%% \item Ointment: it is thus nested within mouse.
\item What is the experimental unit?
\end{itemize}


There are, in fact, two experimental units: mouse and leg within mouse.
To compare drugs we use mice. To compare ointment: we should use leg
within mouse. Interactions? Can be studied, yes. How do we analyze this?
This example, nicely balanced, is a classical example of a split-plot
design (a type of ANOVA with multiple strata). But designs like this, and
others more general, or like this but unbalanced, or like this but with
additional covariates, etc, are nowadays analyzed using mixed-effects
models.


This also relates to questions we asked in Lesson 2 (see the section on
``Non-independent data''): What if we had
repeated measures on the same subjects over time? Or if we had some data
that came from brothers, cousins, etc?


And how would you go about designing an experiment from scratch? What
should you randomize over and what should you block over? Should you use a
factorial design? Matched pairs? Should each one of two technicians each
take care of half of the samples, randomly assigned to each, or should one
technician deal with all male sample and the other with all the female
samples? Should we start mice in each one of the four different diets in
different days of the week, or should we have similar sized groups of each
diet starting every day of the week? Do you give treatment A to all even
numbered samples and treatment B to all odd numbered ones, or do you
randomize order? Etc, etc. And, of course, how should we allocate sample
sizes to the different \textbf{levels of variation}?


Understanding what is the experimental unit is \textbf{absolutely
  crucial}. And sometimes things are complicated: talk to (collaborate
with) a statistician as soon as you can. Some high-profile mistakes in the
literature are derived from misunderstanding experimental design (a
somewhat amusing half-page comment about this by G.\ Churchill in
\textit{Science}, 2013, v.\ 343, p.\ 370). In fact, some mistakes made
during the experimental design phase just cannot be corrected
later\footnote{R.\ Fisher, one of the fathers of modern statistics, as
  well as modern quantitative and evolutionary genetics, is often quoted
  in this context because he said ``To consult the statistician after an
  experiment is finished is often merely to ask him to conduct a post
  mortem examination. He can perhaps say what the experiment died of.''}



\section{Dealing with ratios}\label{ratios}

We will only cover this in class if we have time. But many of you deal with this issue, so you might want to read it anyway \smiley{} . 

%% FIXME: 
% \red{FIXME: might want to add too examples like the one on pp.\ 364 and ff in
% Morgan and Winship ``Counterfactuals and causal inference'' (but for a RCD
% experiment on treatment assignment)}


In biology (as well as in other disciplines) it is common for researchers
to use ratios of variables to try to standardize/normalize a
variable. This procedure looks deceptively simple, but it is not. Here, we
will explore some of the problems\footnote{I thank Alba Concepcion, a
  former student of BM-1, for asking me questions that prompted me to
  write this section. She also provided the link to Curran-Everett's
  paper}. You can find more details\footnote{With discussion about errors
  in the X variable, an issue we have not covered here} in this commentary
from Curran-Everett:
\Burl{http://ajpadvan.physiology.org/cgi/doi/10.1152/advan.00053.2013}.

Before we start, though, note that this section should not really be
necessary after previous sections :-).




So that nothing is hidden, I will simulate the data here, but in these
notes I won't provide details about how/why the data have been simulated
that way (the code is below; look at it if you want). We will pretend
there is a response variable, called ``Y'', two groups (``g1'' and ``g2'')
and another variable, called ``Z''. Z might be some reporter protein, or
something that can be taken as a proxy for cell volume, etc.


<<>>=
set.seed(1) ## irrelevant, but so that we all 
            ## get the same numbers
n <- 20
sd <- 0.5
@ 



\subsection{A misleading case with parallel lines}
\label{mislead-ratios-1}


<<>>=
z1 <- runif(n, 1, 10)
t1 <- factor(rep(c("g1", "g2"), rep(n, 2)))
y1 <-  2 * z1 + 3 * as.numeric(t1) + rnorm(n, 0, sd)
data1 <- data.frame(Y = y1, Z = z1, Group = t1, Ratio = y1/z1)
@ 


%%, fig.cap="Parallel slopes but ratio differences?">>=
<<fig.height=6,fig.cap='Parallel slopes but ratio differences?'>>= 
par(mfrow = c(1, 2))
with(data1, plot(Ratio ~ Group))
with(data1, plot(Y ~ Z, col = c("red", "blue")[Group]))
abline(lm(Y ~ Z, subset(data1, Group == "g1")), col = "red")
abline(lm(Y ~ Z, subset(data1, Group == "g2")), col = "blue")
legend(x = 3, y = 20, legend = c("g1", "g2"), col = c("red", "blue"),
       pch = 1)

@ 

<<>>=
summary(lm(Y ~ Z * Group, data = data1))
t.test(Ratio ~ Group, data = data1)
## This is the equivalent to the t-test, except the
## t-test used by default by R does not assume equal variances
summary(aov(Ratio ~ Group, data = data1))
@ 

The lines are perfectly parallel, but the test for ratios says they
differ. Why? Because it is forcing a regression through the origin. Note
the the linear model does get the results right: there are no differences
in the rate of change of Y relative to Z, but the groups differ in
intercept. As you can see, the analysis with ratios is misleading.


\subsection{A misleading case where ratios differ}
\label{mislead-ratios2}

Generate  the data:
<<>>=
set.seed(123)
sd <- 0.1
z2 <- seq(from = 1, to = 3, length.out = n)
ya <- z2 + rnorm(n, 0, sd)
yb <- 0.5 * z2 + 1 + rnorm(n, 0, sd)
y <- c(ya, yb)
tf <- factor(rep(c("g1", "g2"), rep(n, 2)))
z <- rep(z2, 2)
data2 <- data.frame(Y = y, Z = z, Group = tf, Ratio = y/z)
@ 


The figure and analysis:
%% fig.cap="Different slopes and no ratio differences?">>=
<<fig.height=6,fig.cap='Different slopes and no ratio differences?'>>=
par(mfrow = c(1, 2))
with(data2, plot(Ratio ~ Group))
with(data2, plot(Y ~ Z, col = c("red", "blue")[Group]))
abline(lm(Y ~ Z, subset(data2, Group == "g1")), col = "red")
abline(lm(Y ~ Z, subset(data2, Group == "g2")), col = "blue")
legend(x = 1.5, y = 2.5, legend = c("g1", "g2"), col = c("red", "blue"),
       pch = 1)
@ 

<<>>=
summary(lm(Y ~ Z * Group, data = data2))
t.test(Ratio ~ Group, data = data2) ## or do an ANOVA, as above
@ 


The ratios do differ, if we do not force them through the origin: the rate
of change of Y relative to Z differs between the two groups. But just
comparing the ratios will not detect it. Again, a linear model does detect
it just fine: you see a strong interaction between slope and
group. And, again, the analysis of ratios is misleading.


Log-transforming the ratio, using non-parametric statistics, etc, won't
help; the problem is using the ratio.
<<>>=
t.test(log(Ratio) ~ Group, data = data2)
with(data2, plot(log(Ratio) ~ Group))
wilcox.test(Ratio ~ Group, data = data2)
@ 


\subsection{Diagnostics et al.\ and other warning signs}

If you look closely, the data show warning signals, such as possible
differences in variances of ratios. And if you have no idea about the
model, you might be able to compare several and use diagnostics, as
explained in section \nameref{diagnostics} to choose among models. But, in
general, ratios are not the way to. And using diagnostics might require a
decently large sample size. Summary: only use ratios if you really know
what you are doing and have good reasons to do it. Otherwise, use linear
models.

By the way: we have covered just two very simple examples. Things can of
course get a lot more complicated with non-linear relationships, etc.


\section{Additional reading}

Many books have been devoted to linear models, ANOVA, et al. And in R (not
necessarily through R Commander) there is a wide variety of procedures
implemented. To begin with, look at the great book by Fox and Weisberg ``An R
companion to applied regression'' (now in its third edition) and Faraway's
``Linear models with R'' (in its second edition). Take also a look at chapters 6
and 7 of Dalgaard's ``Introductory statistics with R'' and chapters 8 and 9 of
Kabakoff's ``R in action''. This should get you going. From here, you will
probably want to look at generalized linear models, mixed effects models,
nonlinear models, generalized additive models, and survival analysis, which are
some key extensions of linear models that you might want to use with your own
data.



\part*{Appendix}
\section{What if we did not recode training?}\label{nofactor}
Suppose we had not recoded training but we had fitted a linear model. This
would have happened:

<<>>=
lmMITnofactor <- lm(activ ~ training, data=dmit)
summary(lmMITnofactor)
Anova(lmMITnofactor, type="II")
@ 

See how the degrees of freedom make no sense.


\section{Does order always matter? Further examples and details}
\label{ordermatter-appendix}



% This is slightly more advanced material. Skip it on first reading. 


% If you want to continue reading, let's proceed with some details then. 

The following is an example. For the sake of the exposition, I will here simulate
the data, so everything is clear and in the open.


<<>>=
set.seed(1)
sex <- factor(rep(c("Male", "Female"), c(20, 20)))
drug <- factor(rep(rep(c("A", "B"), c(10, 10)), 2))
y <- rep(c(10, 13, 12, 16), rep(10, 4))
y <- y + rnorm(length(y), sd = 1.5)
y.data <- data.frame(y, sex, drug)
@ 

First, some basic stats about those data. Notice the perfect balance:

<<>>=
with(y.data, tapply(y, list(sex, drug), function(x) sum(!is.na(x))))
with(y.data, tapply(y, list(sex, drug), mean))
@ 

Just by eye, it seems the difference between sexes is around 2, and the
difference between drugs of about 4. And no, there is no interaction:

<<>>=
summary(lm(y ~ sex * drug, data = y.data))
@ 


Fit two models, simply changing the order (we assume no interaction, as
shown above).


<<>>=
m1 <- lm(y ~ sex + drug, data = y.data)
m2 <- lm(y ~ drug + sex, data = y.data)
@ 

And we also fit two small models, one only with sex, the other only with
drug:

<<>>=
msex <- lm(y ~ sex, data = y.data)
mdrug <- lm(y ~ drug, data = y.data)
@

Now, the output for the coefficients for m1 and m2 is the same (these are
always the coefficients as if entered last in the model):

<<>>=
summary(m1)
summary(m2)
@


So nothing new up to here. Now look at what happens if we get the
coefficients for the small models, those with only sex or only drug:
<<>>=
summary(msex)
summary(mdrug)
@

In both cases, the estimate is the same from the model with the two
factors, or with only a single factor. For example, the differences
between sexes are of about 2.2 (the coefficient that says "sexMale") and
the differences between drugs of about 3.8 (the coefficient that says
"drugB").  However, the standard error and, thus, the t value and the
p-value change.

Again, the key is to understand that even if the coefficient does not
change whether or not the other factor is included in the model (and it
does not change because there is complete balance here), the t statistic
and the p-value do change. Why? Because the other factor explains a large
part of variance, and thus makes the residual standard error much smaller
if we include it in the model.


And what about the ANOVA tables?
<<>>=
anova(m1)
anova(m2)
@

Order (when we include both factors, of course) does not change
anything. Why? Because the contributions of each factor do not depend at
all on the other (i.e., the Mean Squares of each factor does not depend on
the other). And since the F is the ratio of the Mean Squares of the factor
over the Mean Squares of the residuals (and this is whatever is left after
we have fitted everything), the order does not affect the F statistic or
the p-value.


Of course, an "Anova" (Type II tests) would show the same:

<<>>=
Anova(m1)
@ 

To understand this better, look at the anova tables for the models with
only one factor:

<<>>=
anova(msex)
anova(mdrug)
@ 


Notice how the Mean Sq for each factor is the same as in the previous
tables. So the Mean Squares for Sex do not depend on whether or not drug
is in the model. But the F statistic (and the p-value) do change a
lot. Why? Because what changes a lot are the Mean Sq.\ of the
residuals. And why is that? Because the other factor, the one we have not
included, does indeed explain a lot of variability, but in these two last
tables, since the other factor is not in the model, that variability is
included now in the error term.



So, to summarize: when there is balance, order does not change a thing if
we include both factors in the model. However, having or not the other
factor in the model can make a difference for the standard errors, the
residual standard errors, and thus the p-values.





%% I do not show this, to avoid further messing around, but could also have
%% done this, where there is no same number of cases in all cells, but this
%% also orthogonal:

%% <<>>=
%% set.seed(51)
%% sex <- factor(rep(c("Male", "Female"), c(17, 17)))
%% drug <- factor(rep(c("A", "B", "A", "B"), c(10, 7, 10, 7)))
%% z <- rep(c(10, 13, 12, 16), c(10, 10, 7, 7))
%% z <- z + rnorm(length(y), sd = 1.5)
%% z.data <- data.frame(z, sex, drug)

%% anova(lm(z ~ sex + drug, data = z.data))
%% anova(lm(z ~ drug + sex, data = z.data))
%% @ 


\section{Anova tables from \Rfunction{lm} et al.: understanding the
  coefficients and parameters}\label{anovaaslm}

We've said this: ANOVAs are a type of linear models. Thus, in R (and in
most statistical packages) you can get the output for an ANOVA by
different routes. Let us make sure we understand this, and can interpret
the output from using the different routes available to us. In fact, we
have jumped from using \Rfunction{aov} to \Rfunction{lm} several times by now.

This is also a good time to recap and make sure we understand ideas we
have used like ``more complex models'' and ``number of
parameters''. Before we see the examples with code below, make sure we can
understand how many parameters (and how many degrees of freedom are taken
by the model) and what they represent in models like:
\begin{itemize}
\item A simple linear regression like \verb@ y ~ x@.
\item A multiple linear regression like \verb@ y ~ x + z@.
\item A one-way ANOVA like the one for the training regimes, with three
  possible training regimes: \verb@y ~ ftraining@.
\item A two-way ANOVA with interactions, like the one about Drug (two
  levels) and Diet (three levels): \verb@y ~ Drug*Diet@.
\item A two-way ANOVA without interactions, like one we might fit to the
  Drug and Diet data: \verb@y ~ Drug + Diet@.
\end{itemize}



Note: the details about the exact numerical value of the coefficients can
be skipped in a first reading. What you definitely need to understand is
how many parameters we are estimating.


Let us now see a
simple example:

<<>>=
asAnova <- aov(activ ~ ftraining, data = dmit)
summary(asAnova)
@ 


Not let us fit the same model using \Rfunction{lm}. 

<<>>=
asLm <- lm(activ ~ ftraining, data = dmit)
anova(asLm)
@ 

The ANOVA table is the same. I've used \Rfunction{anova} but I could have
used \Rfunction{Anova}.

But what is this output?

<<>>=
asLm
summary(asLm)
@ 

We are being shown the fitted coefficients. They are expressed as
deviations with respect to a baseline level, in this case the first level:

<<>>=
means <- with(dmit, tapply(activ, ftraining, mean)) ## instead of "tapply", 
                                                    ## "by" or "aggregate" 
                                                    ## would 
                                                    ## also work here
means[1]
means[2] - means[1]
means[3] - means[1]
rm(means) ## let's remove it, so as not the leave
          ## garbage around
@ 

This is the default parameterization in R. But is is not the only one available.


Of course, the above applies to more than one factor, and to factors with
an arbitrary number of levels.

\subsection{Changing the reference in the one-way}

We can change the reference, and that will change what we are comparing against:

<<>>=
dmitb <- dmit
dmitb$ftraining2 <- factor(dmitb$ftraining, 
                           levels = c("Afternoon", "Lunch", "Morning"))
## check
with(dmitb, table(ftraining, ftraining2))
@ 

Now, rerun the analysis

<<>>=
asLm2 <- lm(activ ~ ftraining2, data = dmitb)
anova(asLm2) ## no difference, of course
summary(asLm2)
meansb <- with(dmitb, tapply(activ, ftraining2, mean))
meansb[1]
meansb[2] - meansb[1]
meansb[3] - meansb[1]
rm(meansb)
@ 



If we go back to \nameref{simpletwo} we can also do the same and change the
reference, and that changes what we are comparing. For instance, let us
recode A and run the model again in those data:

<<>>=
df1b <- df1
df1b$A <- factor(df1b$A, levels = c("a2", "a1"))
table(df1b$A, df1$A) ## double check
@ 

<<>>=
summary(lm(y ~ A + B, data = df1b))
@ 







\subsection{Coefficients with two-ways}

What about two-way models?  Lets us first play with another fake data set:

<<>>=
y <- c(1:9, 20, 21, 22)
X <- rep(rep(c("x1", "x2"), c(3, 3)), 2)
U <- rep(c("u1", "u2"), c(6, 6))
anova(lm(y ~ U * X)) ## so strong evidence of interaction
@ 

<<>>=
## The cell means
tapply(y, list(X, U), mean)
## The estimates
summary(lm(y ~ X * U))
## The intercept is the first cell mean.
## The right and bottom cell:
## the X2:U2 = 
21 - (2 + 3 + 6)
## The coefficient for x2 is the difference between the intercept and
## the first column of the second row:
5 - 2
## The coefficient for u2 is the difference between the intercept
## and the first row of the second column:
8 -2
@ 

So things here are easy: a saturated model with interactions has a
many parameters as cell means and once we figure out what is the
reference, we can see what each coefficient means.


What about additive models? This is slightly more complicated:


<<>>=
summary(mxu <- lm(y ~ X  + U))
model.matrix(mxu)
mean(y) ## 9
## What is the first cell? the overall mean with the effect of X:1 and
## U:1, which is the overall mean minus half the effects of X:2 and U:2

9 - 11/2 - 8/2 ## Where 11 and 8 are coming from the fitted model

fitted(mxu) ## Yes: fitted for first group are the intercept term


## But why the 11 and the 8 above?
## Again, look here:
(mxu <- tapply(y, list(X, U), mean))
## or here
aggregate(y ~ X * U, FUN = mean)

## And our model says: an overall mean, and row and column deviations. 
## Let's write it.
## Row effects, or effect of X:2 (effect of X:1 = - effect of X:2)
## is the average of the row differences at each column
## or average of (5 - 2) and (21 - 8) which is the effect of X:2
0.5 * ((5 - 2) + (21 - 8)) # = 8
## or, similarly
mean(mxu[2, ] - mxu[1, ])


## Similar for column effects, or the effect of U:2
## effect of U:2: 
0.5 * ((8 - 2) + (21 - 5)) # = 11
## or, similarly
mean(mxu[ , 2] - mxu[, 1])

## So the first cell is the first cell UNDER that additive model. You can't 
## just put the first cell mean in there.
@ 


What if we go to the data in section \nameref{simpletwo}? Again, in the
interaction case things are easiest:

<<>>=
(means <- with(df1, tapply(y, list(A, B), mean)))
@ 

<<>>=
summary(m2 <- lm(y ~ A * B, data = df1))
## each main effect
means[1, 2] - means[1, 1]
means[2, 1] - means[1, 1]
## interaction
means[2, 2] - ( means[1, 1] + (m2$coefficients[2] + m2$coefficients[3]))
@ 


What about additive model?

<<>>=
## Overall mean:
mean(df1$y)

## Note what are the estimates of the effects of A and B: the mean deviation
## of the second level from the first:
## A2 
mean(means[2, ] - means[1, ])
## B2
mean(means[, 2] - means[, 1])
## Intercept
mean(df1$y) - 
    0.5 * (mean(means[2, ] - means[1, ])) - 
    0.5 * mean(means[, 2] - means[, 1])
@ 


\subsection{Other contrasts}\label{othercontrasts_param}
We are using \texttt{contr.treatment}, the default in R. There are other
types. In particular, and when we want to estimate effects in the presence
of interactions, we probably want to use \texttt{contr.sum}. We will not
pursue this any further here.

To give you a quick taste, this might do:

(We will use \Rfunction{contr.Sum} from \CRANpkg{car}, since clearer labeling)

<<>>=
opt <- options(contrasts = c("contr.Sum", "contr.poly"))
m11 <- lm(y ~ A + B, data = df1)
anova(m11)
summary(m11)
@ 


<<>>=
(overallMean <- mean(df1$y))
mA <- with(df1, tapply(y, A, mean))
mA - overallMean

mB <- with(df1, tapply(y, B, mean))
mB - overallMean
@ 

Interaction now (note the estimates!)
<<>>=
m12 <- lm(y ~ A * B, data = df1)
anova(m12)
summary(m12)
@ 

% Eh, bad example: this is an old df1b, not from the last df1!
% What if we changed the reference?
% <<>>=
% summary(lm(y ~ A + B, data = df1b))
% @


Return contrasts to usual state
<<>>=
options(opt)
@ 


\subsection{Changing the reference in two-ways}
If we change the levels or references, of course the interpretation of the
coefficients must also change. This is nothing new relative to what we saw
for the one-way.



\subsection{Unbalanced case}
These examples have used balanced data. Things get trickier with
unbalanced data. The idea of this section is to get an intuitive
understanding of what the numbers mean, but then for real you'll do this
with a computer.

\subsection{Three way anovas, factors with more than two classes, etc}
There is nothing conceptually new, but the accounting gets more complicated.


%% Explaining parameters for a two-way. The interaction is simple, since we
%% just have each cell mean in contr.treatment. The additive is messier. A
%% numerical example
%% <<>>=
%% y <- c(1:9, 20, 21, 22)
%% X <- rep(rep(c("1", "2"), c(3, 3)), 2)
%% U <- rep(c("1", "2"), c(6, 6))
%% anova(lm(y ~ U * X))
%% tapply(y, list(X, U), mean)
%% summary(lm(y ~ X * U))
%% ## the X2:U2 = 21 - (2 + 3 + 6)
%% summary(lm(y ~ X  + U))
%% m1 <- lm(y ~ X + U)
%% model.matrix(m1)
%% fitted(m1)
%% ## fitted for first group are the intercept term
%% mean(y) ## 9

%% ## effect of X:2: 0.5 * ((5 -2) + (21 - 8)) = 8
%% ## effect of U:2: 0.5 * ((8 -2) + (21 - 5)) = 11

%% ## what is the first cell? the overall mean with the effect of X:1 and
%% ## U:1, which is the overall mean minus half the effects

%% 9 - 11/2 - 8/2

%% ## Yes, the first cell is the first cell UNDER that additive model. You can't 
%% ## just put the first cell mean in there.
%% @ 








\clearpage


\section{Type I, Type II, Type III: a few technical notes}\label{ssI-to-III}
(Skip this if you want. I leave it here in case you want to get back to
it. And yes, we have not mentioned Type III much (but see \ref{2way-int}).

% %% This is too complex, as the SS(A, B, AB) is the sum of the SS of the
% %% model.
% %% Of course, under a decomposition where sums add up, so Type I.
% I take this material from
% \Burl{http://md.psych.bio.uni-goettingen.de/mv/unit/lm_cat/lm_cat_unbal_ss_explained.html}: 

% \begin{verbatim}
% SS(AB | A, B) = SS(A, B, AB) - SS(A, B)
% SS(A | B, AB) = SS(A, B, AB) - SS(B, AB)
% SS(B | A, AB) = SS(A, B, AB) - SS(A, AB)
% SS(A | B) = SS(A, B) - SS(B)
% SS(B | A) = SS(A, B) - SS(A)
% \end{verbatim}

% (Where \texttt{SS(A, B)} is the model SS of a model with A and B).
% \paragraph Type I:

% \begin{verbatim}
% SS(A) for factor A. 
% SS(B | A) for factor B. 
% SS(AB | B, A) for interaction AB. 
% \end{verbatim}


% \paragraph Type II:

% \begin{verbatim}
% SS(A | B) for factor A. 
% SS(B | A) for factor B. 
% \end{verbatim}
% (We assume no significant interaction)

% \paragraph Type III:

% \begin{verbatim}
% SS(A | B, AB) for factor A. 
% SS(B | A, AB) for factor B. 
% \end{verbatim}

% Interaction need not be absent. But this does not mean these SS are
% interpretable. (And beware of the contrasts you use ---section
% \ref{othercontrasts}--- if you want to take a look at coefficients. Note
% also possible issues related to changes in results if you center the
% predictors in models with continuous predictors).


I take this from Nancy Reid's \Burl{http://www.utstat.utoronto.ca/reid/sta442f/2009/typeSS.pdf}:

``Let R(.) represent the residual sum of squares for a model, so for
example R(A,B,AB) is the residual sum of squares fitting the whole model,
R(A) is the residual sum of squares fitting just the main effect of A, and
R(1) is the residual sum of squares fitting just the mean.''

\textbf{Type I}

\begin{verbatim}
A:  SS(A) = R(1) - R(A) 
B:  SS(B|A) = R(A) - R(A, B)
AB: SS(AB|A, B) = R(A, B) - R(A, B, AB) 
\end{verbatim}

\vspace*{15pt}


\textbf{Type II}

\begin{verbatim}
A:  SS(A|B) = R(B) - R(A, B) 
B:  SS(B|A) = R(A) - R(A, B)
AB: SS(AB|A, B) = R(A, B) - R(A, B, AB) 
\end{verbatim}

We assume no significant interaction


\vspace*{15pt}


\textbf{Type III}

\begin{verbatim}
A:  SS(A|B, AB) = R(B, AB) - R(A, B, AB) 
B:  SS(B|A, AB) = R(A, AB) - R(A, B, AB)
AB: SS(AB|A, B) = R(A,B) - R(A, B, AB) 
\end{verbatim}

Interaction need not be absent. But this does not mean these SS are
interpretable. (And beware of the contrasts you use ---section
\ref{othercontrasts_param}--- if you want to take a look at coefficients. Note
also possible issues related to changes in results if you center the
predictors in models with continuous predictors). We will not use Type III
in this notes. Why? Read the paper by Bill Venables ``Exegeses on linear
models'', or Oyvind Langsrud, 2003, ``ANOVA for unbalanced data: Use Type
II instead of Type III sums of squares'', Statistics and Computing, Volume
13, Number 2, pp. 163-167, or the many debates about Type III SS, or chapter 5 of
Fox and Weisberg's ``An R companion to applied regression'', as well as Fox's
book on linear models (mentioned in Fox and Weisberg). There
are a few ways to obtain Type III in R, the easiest by using
\texttt{Anova(x, type = ``III'')}, and also by using \texttt{drop1} with
the right contrasts).

\vspace*{15pt}


And some numbers here:
<<>>=
m_diet <- lm(y ~ Diet, data = dcholest)
m_drug <- lm(y ~ Drug, data = dcholest)
m_diet_drug <- lm(y ~ Diet + Drug, data = dcholest)
m_int <- lm(y ~ Diet * Drug, data = dcholest)

summary(m_diet)
anova(m_diet)
anova(m_drug)
anova(m_diet_drug)
anova(m_diet, m_diet_drug)
## 124.1: RSS from model m_diet
## 91.8 : RSS from model m_diet_drub
## 32.3 = 124.1 - 91.8:
## SS(drug|diet) = R(m_diet) - R(m_diet_drug)
anova(m_drug, m_diet_drug)
## SS(diet|drug) = R(m_drug) - R(m_diet_drug)
## 75.5 = 167.3 - 91.8
Anova(m_diet_drug)

## But also the sum SS here for drug and diet
Anova(m_int)

@


% ## Finally, for Type III (SKIP this; left here to show it for completeness!!!)
% opt <- options(contrasts = c("contr.sum", "contr.poly"))
% mf <- lm(y ~ Diet * Drug, data = dcholest)
% drop1(mf, .~., test = "F")
% Anova(mf, type = "III")
% options(opt)
% Anova(m_int, type = "III") ## yes, it differs, as different contrasts

(You can also take a look at
\Burl{http://md.psych.bio.uni-goettingen.de/mv/unit/lm_cat/lm_cat_unbal_ss_explained.html}: 
that uses a slightly different notation.
)










\section{Session info and packages used}

This is the information about the version of R and packages used when
producing this document:
<<echo=FALSE,results='hide',error=FALSE>>=
options(width=60)
@ 

<<>>=
sessionInfo()
@ 



\end{document}

%% remember to use bibexport to keep just the minimal bib needed
%% bibexport -o extracted.bib OncoSimulR.aux
%% rm OncoSimulR.bib
%% mv extracted.bib OncoSimulR.bib
%% and then turn URL of packages into notes


%%% Local Variables:
%%% ispell-local-dictionary: "en_US"
%%% coding: iso-8859-15
%%% End:






%% Examples prediction and confidence intervals
%% x <- c(rep(1, 50), rep(5, 50)); x <- x + rnorm(100, sd = 0.1); y <- 3 * x + rnorm(100); ci.plot(lm(y ~ x)); mean(x)

% TeX-master: "Lesson-3"
